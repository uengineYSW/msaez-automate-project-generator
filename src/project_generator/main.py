import asyncio
import concurrent.futures
import threading
import json
import math
import copy
import os
from datetime import datetime
from dotenv import load_dotenv
load_dotenv()

from typing import List

from project_generator.utils import JobUtil, DecentralizedJobManager
from project_generator.systems.storage_system_factory import StorageSystemFactory
from project_generator.config import Config

# StorageSystem ë³„ì¹­ (í˜¸í™˜ì„±)
StorageSystem = StorageSystemFactory
from project_generator.run_healcheck_server import run_healcheck_server
from project_generator.simple_autoscaler import start_autoscaler
from project_generator.utils.logging_util import LoggingUtil

# Workflow imports
from project_generator.workflows.user_story.user_story_generator import UserStoryWorkflow
from project_generator.workflows.summarizer.requirements_summarizer import RequirementsSummarizerWorkflow
from project_generator.workflows.bounded_context.bounded_context_generator import BoundedContextWorkflow
from project_generator.workflows.sitemap.command_readmodel_extractor import create_command_readmodel_workflow
from project_generator.workflows.sitemap.sitemap_generator import create_sitemap_workflow
from project_generator.workflows.aggregate_draft.requirements_mapper import RequirementsMappingWorkflow
from project_generator.workflows.aggregate_draft.aggregate_draft_generator import AggregateDraftGenerator
from project_generator.utils.trace_markdown_util import TraceMarkdownUtil
from project_generator.workflows.aggregate_draft.preview_fields_generator import PreviewFieldsGenerator
from project_generator.workflows.aggregate_draft.ddl_fields_generator import DDLFieldsGenerator
from project_generator.workflows.aggregate_draft.traceability_generator import TraceabilityGenerator
from project_generator.workflows.aggregate_draft.ddl_extractor import DDLExtractor
from project_generator.workflows.aggregate_draft.standard_transformer import AggregateDraftStandardTransformer
from project_generator.workflows.requirements_validation.requirements_validator import RequirementsValidator

# ì „ì—­ job_manager ì¸ìŠ¤í„´ìŠ¤
_current_job_manager: DecentralizedJobManager = None


def _compute_intermediate_lengths(final_length: int, steps: int = 3) -> List[int]:
    """
    ìµœì¢… ìƒì„± ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ê°„ ê¸¸ì´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê³„ì‚°.
    ìŠ¤íŠ¸ë¦¬ë°ì´ ì–´ë ¤ìš´ ì›Œí¬í”Œë¡œìš°ì—ì„œ ì£¼ê¸°ì  ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ìš©ë„ë¡œ ì‚¬ìš©.
    """
    if final_length <= 0 or steps <= 0:
        return []

    lengths = set()
    for idx in range(1, steps + 1):
        length = max(1, min(final_length - 1, (final_length * idx) // (steps + 1)))
        lengths.add(length)

    intermediate = sorted(lengths)
    return intermediate


async def main():
    """ë©”ì¸ í•¨ìˆ˜ - Flask ì„œë²„, Job ëª¨ë‹ˆí„°ë§, ìë™ ìŠ¤ì¼€ì¼ëŸ¬ ë™ì‹œ ì‹œì‘"""
    
    flask_thread = None
    restart_count = 0
    
    while True:
        tasks = []
        job_manager = None
        
        try:
            # Storage ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            StorageSystemFactory.initialize()
            
            # Flask ì„œë²„ ì‹œì‘ (ì²« ì‹¤í–‰ì‹œì—ë§Œ)
            if flask_thread is None:
                flask_thread = threading.Thread(target=run_healcheck_server, daemon=True)
                flask_thread.start()
                flask_port = os.getenv('FLASK_PORT', '2025')
                flask_host = os.getenv('FLASK_HOST', 'localhost')
                LoggingUtil.info("main", f"Flask ì„œë²„ê°€ í¬íŠ¸ {flask_port}ì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
                LoggingUtil.info("main", f"í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸: http://{flask_host}:{flask_port}/ok")

            if restart_count > 0:
                LoggingUtil.info("main", f"ë©”ì¸ í•¨ìˆ˜ ì¬ì‹œì‘ ì¤‘... (ì¬ì‹œì‘ íšŸìˆ˜: {restart_count})")

            pod_id = Config.get_pod_id()
            job_manager = DecentralizedJobManager(pod_id, process_job_async)
            
            # ì „ì—­ job_manager ì„¤ì •
            global _current_job_manager
            _current_job_manager = job_manager
            
            # ê°ì‹œí•  namespace ëª©ë¡
            monitored_namespaces = ['user_story_generator', 'summarizer', 'bounded_context', 'command_readmodel_extractor', 'sitemap_generator', 'requirements_mapper', 'aggregate_draft_generator', 'preview_fields_generator', 'ddl_fields_generator', 'traceability_generator', 'standard_transformer', 'ddl_extractor', 'requirements_validator']
            
            if Config.is_local_run():
                tasks.append(asyncio.create_task(job_manager.start_job_monitoring(monitored_namespaces)))
                LoggingUtil.info("main", "ì‘ì—… ëª¨ë‹ˆí„°ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                tasks.append(asyncio.create_task(start_autoscaler()))
                tasks.append(asyncio.create_task(job_manager.start_job_monitoring(monitored_namespaces)))
                LoggingUtil.info("main", "ìë™ ìŠ¤ì¼€ì¼ëŸ¬ ë° ì‘ì—… ëª¨ë‹ˆí„°ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            
            # shutdown_event ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì¶”ê°€
            shutdown_monitor_task = asyncio.create_task(job_manager.shutdown_event.wait())
            tasks.append(shutdown_monitor_task)
            
            # íƒœìŠ¤í¬ë“¤ ì¤‘ í•˜ë‚˜ë¼ë„ ì™„ë£Œë˜ë©´ ì¢…ë£Œ (shutdown_event í¬í•¨)
            done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
            
            # shutdown_eventê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if shutdown_monitor_task in done:
                LoggingUtil.info("main", "Graceful shutdown ì‹ í˜¸ ìˆ˜ì‹ . ë©”ì¸ ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                
                # ë‚˜ë¨¸ì§€ ì‹¤í–‰ ì¤‘ì¸ íƒœìŠ¤í¬ë“¤ ì·¨ì†Œ
                for task in pending:
                    if not task.done():
                        LoggingUtil.debug("main", f"íƒœìŠ¤í¬ ì·¨ì†Œ ì¤‘: {task}")
                        task.cancel()
                        try:
                            await task
                        except asyncio.CancelledError:
                            LoggingUtil.debug("main", "íƒœìŠ¤í¬ê°€ ì •ìƒì ìœ¼ë¡œ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                        except Exception as cleanup_error:
                            LoggingUtil.exception("main", "íƒœìŠ¤í¬ ì •ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ", cleanup_error)
                
                LoggingUtil.info("main", "ë©”ì¸ í•¨ìˆ˜ ì •ìƒ ì¢…ë£Œ")
                break  # while ë£¨í”„ ì¢…ë£Œ
            
        except Exception as e:
            restart_count += 1
            LoggingUtil.exception("main", f"ë©”ì¸ í•¨ìˆ˜ì—ì„œ ì˜ˆì™¸ ë°œìƒ (ì¬ì‹œì‘ íšŸìˆ˜: {restart_count})", e)
            
            # ì‹¤í–‰ ì¤‘ì¸ íƒœìŠ¤í¬ë“¤ ì •ë¦¬
            for task in tasks:
                if not task.done():
                    LoggingUtil.debug("main", f"íƒœìŠ¤í¬ ì·¨ì†Œ ì¤‘: {task}")
                    task.cancel()
                    try:
                        await task
                    except asyncio.CancelledError:
                        LoggingUtil.debug("main", "íƒœìŠ¤í¬ê°€ ì •ìƒì ìœ¼ë¡œ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    except Exception as cleanup_error:
                        LoggingUtil.exception("main", "íƒœìŠ¤í¬ ì •ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ", cleanup_error)

            continue


async def process_summarizer_job(job_id: str, complete_job_func: callable):
    """Summarizer Job ì²˜ë¦¬ í•¨ìˆ˜"""
    error_occurred = None
    try:
        LoggingUtil.info("main", f"ğŸš€ Summarizer ì²˜ë¦¬ ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë”©
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            job_path = f'jobs/summarizer/{job_id}'
            job_data = await loop.run_in_executor(
                executor,
                lambda: StorageSystemFactory.instance().get_data(job_path)
            )
        
        if not job_data:
            LoggingUtil.warning("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        inputs = job_data.get("state", {}).get("inputs", {})
        if not inputs:
            LoggingUtil.warning("main", f"Job inputs ì—†ìŒ: {job_id}")
            return
        
        # SummarizerWorkflow ì‹¤í–‰
        workflow = RequirementsSummarizerWorkflow()
        result = await asyncio.to_thread(workflow.run, inputs)
        
        summaries = result.get('summarizedRequirements', [])
        LoggingUtil.info("main", f"âœ… ìš”ì•½ ì™„ë£Œ: {len(summaries)}ê°œ")
        
        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        output_path = f'jobs/summarizer/{job_id}/state/outputs'
        
        # 1) isCompleted ì œì™¸í•œ ë°ì´í„° ë¨¼ì € ì €ì¥
        result_without_completed = {k: v for k, v in result.items() if k != 'isCompleted'}
        await asyncio.to_thread(
            StorageSystemFactory.instance().set_data,
            output_path,
            result_without_completed
        )
        
        # 2) ì§§ì€ ëŒ€ê¸° í›„ isCompleted ì €ì¥
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            StorageSystemFactory.instance().update_data,
            output_path,
            {'isCompleted': True}
        )
        
        # requestedJob ì‚­ì œ
        req_path = f'requestedJobs/summarizer/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        error_occurred = e
        LoggingUtil.exception("main", f"Summarizer Job ì²˜ë¦¬ ì˜¤ë¥˜: {job_id}", e)
        
        # ì‹¤íŒ¨ ìƒíƒœ ì €ì¥
        try:
            error_output = {
                "summarizedRequirements": [],
                "isCompleted": False,
                "error": str(e),
                "logs": [{
                    "timestamp": datetime.now().isoformat(),
                    "message": f"ì˜¤ë¥˜: {str(e)}"
                }]
            }
            
            output_path = f'jobs/summarizer/{job_id}/state/outputs'
            await asyncio.to_thread(
                StorageSystemFactory.instance().set_data,
                output_path,
                error_output
            )
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        # ì˜ˆì™¸ ë°œìƒ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ complete_job_func í˜¸ì¶œ
        complete_job_func()

async def process_user_story_job(job_id: str, complete_job_func: callable):
    """UserStory Job ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        LoggingUtil.info("main", f"ğŸš€ UserStory ì²˜ë¦¬ ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë”© (user_story_generator namespace ì‚¬ìš©)
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            job_path = f'jobs/user_story_generator/{job_id}'
            job_data = await loop.run_in_executor(
                executor,
                lambda: StorageSystemFactory.instance().get_data(job_path)
            )
        
        if not job_data:
            LoggingUtil.warning("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        inputs = job_data.get("state", {}).get("inputs", {})
        if not inputs:
            LoggingUtil.warning("main", f"Job inputs ì—†ìŒ: {job_id}")
            return
        
        # UserStoryWorkflow ì‹¤í–‰
        workflow = UserStoryWorkflow()
        result = await asyncio.to_thread(workflow.run, inputs)
        
        # ê²°ê³¼ëŠ” ì´ë¯¸ camelCaseë¡œ ë³€í™˜ë˜ì–´ ìˆìŒ
        user_stories = result.get('userStories', [])
        actors = result.get('actors', [])
        business_rules = result.get('businessRules', [])
        LoggingUtil.info("main", f"âœ… ìƒì„± ì™„ë£Œ: Stories {len(user_stories)}, Actors {len(actors)}, Rules {len(business_rules)}")
        
        # ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥ (ë¹„ë™ê¸° ì²˜ë¦¬)
        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        output_path = f'jobs/user_story_generator/{job_id}/state/outputs'
        
        # 1) isCompleted ì œì™¸í•œ ë°ì´í„° ë¨¼ì € ì €ì¥
        result_without_completed = {k: v for k, v in result.items() if k != 'isCompleted'}
        await asyncio.to_thread(
            StorageSystemFactory.instance().set_data,
            output_path,
            result_without_completed
        )
        
        # 2) ì§§ì€ ëŒ€ê¸° í›„ isCompleted ì €ì¥ (ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥)
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            StorageSystemFactory.instance().update_data,
            output_path,
            {'isCompleted': True}
        )
        
        # requestedJob ì‚­ì œ
        req_path = f'requestedJobs/user_story_generator/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        LoggingUtil.exception("main", f"ì²˜ë¦¬ ì˜¤ë¥˜: {job_id}", e)
        
        # ì‹¤íŒ¨ ê¸°ë¡
        try:
            error_output = {
                'isFailed': True,
                'error': str(e),
                'progress': 0,
                'userStories': [],  # camelCase
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/user_story_generator/{job_id}/state/outputs'
            StorageSystemFactory.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        # ì˜ˆì™¸ ë°œìƒ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ complete_job_func í˜¸ì¶œ
        complete_job_func()

async def process_bounded_context_job(job_id: str, complete_job_func: callable):
    """Bounded Context ìƒì„± Job ì²˜ë¦¬"""
    
    try:
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/bounded_context/{job_id}'
        job_data = await asyncio.to_thread(
            StorageSystemFactory.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ (state.inputsì—ì„œ ê°€ì ¸ì˜´)
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        inputs = {
            'devisionAspect': inputs_data.get('devisionAspect', ''),
            'requirements': inputs_data.get('requirements', {}),
            'generateOption': inputs_data.get('generateOption', {}),
            'feedback': inputs_data.get('feedback'),
            'previousAspectModel': inputs_data.get('previousAspectModel')
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        workflow = BoundedContextWorkflow()
        result = await asyncio.to_thread(workflow.run, inputs)
        
        output_path = f'jobs/bounded_context/{job_id}/state/outputs'
        storage = StorageSystemFactory.instance()

        try:
            final_length = len(json.dumps(result, ensure_ascii=False))
        except Exception:
            final_length = 0

        intermediate_lengths = _compute_intermediate_lengths(final_length, steps=3)

        for idx, length in enumerate(intermediate_lengths):
            progress_value = max(1, min(95, int(((idx + 1) / (len(intermediate_lengths) + 1)) * 100)))
            update_payload = {
                'currentGeneratedLength': length,
                'progress': progress_value,
                'isCompleted': False
            }
            await storage.update_data_async(
                output_path,
                storage.sanitize_data_for_storage(update_payload)
            )
            await asyncio.sleep(1)

        result_with_length = copy.deepcopy(result)
        result_with_length['currentGeneratedLength'] = final_length

        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        # 1) isCompleted ì œì™¸í•œ ë°ì´í„° ë¨¼ì € ì €ì¥
        result_without_completed = {k: v for k, v in result_with_length.items() if k != 'isCompleted'}
        await asyncio.to_thread(
            storage.set_data,
            output_path,
            storage.sanitize_data_for_storage(result_without_completed)
        )
        
        # 2) ì§§ì€ ëŒ€ê¸° í›„ isCompleted ì €ì¥
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            storage.update_data,
            output_path,
            {'isCompleted': True}
        )
        
        # requestedJob ì‚­ì œ
        req_path = f'requestedJobs/bounded_context/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ BC ìƒì„± ì™„ë£Œ: {job_id}, BCs: {len(result.get('boundedContexts', []))}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        error_occurred = e
        LoggingUtil.exception("main", f"BC ìƒì„± ì˜¤ë¥˜: {job_id}", e)
        
        # ì‹¤íŒ¨ ê¸°ë¡
        try:
            error_output = {
                'isFailed': True,
                'error': str(e),
                'progress': 0,
                'thoughts': '',
                'boundedContexts': [],
                'relations': [],
                'explanations': [],
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/bounded_context/{job_id}/state/outputs'
            StorageSystemFactory.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        # ì˜ˆì™¸ ë°œìƒ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ complete_job_func í˜¸ì¶œ
        complete_job_func()

async def process_command_readmodel_job(job_id: str, complete_job_func: callable):
    """Command/ReadModel ì¶”ì¶œ Job ì²˜ë¦¬"""
    
    try:
        LoggingUtil.info("main", f"ğŸš€ Command/ReadModel ì¶”ì¶œ ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/command_readmodel_extractor/{job_id}'
        job_data = await asyncio.to_thread(
            StorageSystemFactory.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        inputs = {
            'job_id': job_id,
            'requirements': inputs_data.get('requirements', ''),
            'bounded_contexts': inputs_data.get('boundedContexts', []),
            'logs': [],
            'progress': 0,
            'is_completed': False,
            'is_failed': False,
            'error': '',
            'extracted_data': {}
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (recursion_limit ì¦ê°€)
        workflow = create_command_readmodel_workflow()
        result = await asyncio.to_thread(
            workflow.invoke, 
            inputs,
            {"recursion_limit": 50}
        )
        
        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        output_path = f'jobs/command_readmodel_extractor/{job_id}/state/outputs'
        
        # 1) isCompleted ì œì™¸í•œ ë°ì´í„° ë¨¼ì € ì €ì¥
        await asyncio.to_thread(
            StorageSystemFactory.instance().set_data,
            output_path,
            {
                'extractedData': result.get('extracted_data', {}),
                'logs': result.get('logs', []),
                'progress': result.get('progress', 0),
                'isFailed': result.get('is_failed', False),
                'error': result.get('error', '')
            }
        )
        
        # 2) ì§§ì€ ëŒ€ê¸° í›„ isCompleted ì €ì¥
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            StorageSystemFactory.instance().update_data,
            output_path,
            {'isCompleted': result.get('is_completed', False)}
        )
        
        # requestedJob ì‚­ì œ
        req_path = f'requestedJobs/command_readmodel_extractor/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ Command/ReadModel ì¶”ì¶œ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        error_occurred = e
        LoggingUtil.exception("main", f"Command/ReadModel ì¶”ì¶œ ì˜¤ë¥˜: {job_id}", e)
        
        # ì‹¤íŒ¨ ê¸°ë¡
        try:
            error_output = {
                'isFailed': True,
                'error': str(e),
                'progress': 0,
                'extractedData': {},
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/command_readmodel_extractor/{job_id}/state/outputs'
            StorageSystemFactory.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        complete_job_func()

async def process_sitemap_job(job_id: str, complete_job_func: callable):
    """SiteMap ìƒì„± Job ì²˜ë¦¬"""
    
    try:
        LoggingUtil.info("main", f"ğŸš€ SiteMap ìƒì„± ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/sitemap_generator/{job_id}'
        job_data = await asyncio.to_thread(
            StorageSystemFactory.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        inputs = {
            'job_id': job_id,
            'requirements': inputs_data.get('requirements', ''),
            'bounded_contexts': inputs_data.get('boundedContexts', []),
            'command_readmodel_data': inputs_data.get('commandReadModelData', {}),
            'existing_navigation': inputs_data.get('existingNavigation', []),
            'logs': [],
            'progress': 0,
            'is_completed': False,
            'is_failed': False,
            'error': '',
            'site_map': {}
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        workflow = create_sitemap_workflow()
        result = await asyncio.to_thread(
            workflow.invoke, 
            inputs,
            {"recursion_limit": 50}
        )
        
        output_path = f'jobs/sitemap_generator/{job_id}/state/outputs'
        storage = StorageSystemFactory.instance()

        try:
            final_length = len(json.dumps(result.get('site_map', {}), ensure_ascii=False))
        except Exception:
            final_length = 0

        intermediate_lengths = _compute_intermediate_lengths(final_length, steps=3)

        for idx, length in enumerate(intermediate_lengths):
            progress_value = max(1, min(95, int(((idx + 1) / (len(intermediate_lengths) + 1)) * 100)))
            update_payload = {
                'currentGeneratedLength': length,
                'progress': progress_value,
                'isCompleted': False
            }
            await storage.update_data_async(
                output_path,
                storage.sanitize_data_for_storage(update_payload)
            )
            await asyncio.sleep(1)

        final_output = {
            'siteMap': result.get('site_map', {}),
            'logs': result.get('logs', []),
            'progress': result.get('progress', 0),
            'isFailed': result.get('is_failed', False),
            'error': result.get('error', ''),
            'currentGeneratedLength': final_length
        }

        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        await asyncio.to_thread(
            storage.set_data,
            output_path,
            storage.sanitize_data_for_storage(final_output)
        )
        
        # ì§§ì€ ëŒ€ê¸° í›„ isCompleted ì €ì¥
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            storage.update_data,
            output_path,
            {'isCompleted': result.get('is_completed', False)}
        )
        
        # requestedJob ì‚­ì œ
        req_path = f'requestedJobs/sitemap_generator/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ SiteMap ìƒì„± ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        error_occurred = e
        LoggingUtil.exception("main", f"SiteMap ìƒì„± ì˜¤ë¥˜: {job_id}", e)
        
        # ì‹¤íŒ¨ ê¸°ë¡
        try:
            error_output = {
                'isFailed': True,
                'error': str(e),
                'progress': 0,
                'siteMap': {},
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/sitemap_generator/{job_id}/state/outputs'
            StorageSystemFactory.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        complete_job_func()

async def process_requirements_mapping_job(job_id: str, complete_job_func: callable):
    """Requirements Mapping Job ì²˜ë¦¬"""
    
    try:
        LoggingUtil.info("main", f"ğŸš€ Requirements Mapping ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/requirements_mapper/{job_id}'
        job_data = await asyncio.to_thread(
            StorageSystemFactory.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        inputs = {
            'bounded_context': inputs_data.get('boundedContext', {}),
            'requirement_chunk': inputs_data.get('requirementChunk', {}),
            'relevant_requirements': [],
            'progress': 0,
            'logs': [],
            'is_completed': False,
            'error': ''
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        workflow = RequirementsMappingWorkflow()
        result = workflow.run(inputs)
        
        # ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥
        bounded_context = inputs_data.get('boundedContext', {}) or {}
        bc_name = bounded_context.get('name', '')
        
        output = {
            'boundedContext': bc_name,
            'requirements': result.get('relevant_requirements', []),
            'progress': result.get('progress', 100),
            'logs': result.get('logs', [])
        }
        
        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        output_path = f'{job_path}/state/outputs'
        sanitized_output = StorageSystemFactory.instance().sanitize_data_for_storage(output)
        await asyncio.to_thread(
            StorageSystemFactory.instance().set_data,
            output_path,
            sanitized_output
        )
        
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            StorageSystemFactory.instance().update_data,
            output_path,
            {'isCompleted': result.get('is_completed', True)}
        )
        
        # ìš”ì²­ Job ì œê±°
        req_path = f'requestedJobs/requirements_mapper/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ Requirements Mapping ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        LoggingUtil.exception("main", f"Requirements Mapping ì˜¤ë¥˜: {job_id}", e)
        
        # ì‹¤íŒ¨ ê¸°ë¡
        try:
            error_output = {
                'isFailed': True,
                'error': str(e),
                'progress': 0,
                'requirements': [],
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/requirements_mapper/{job_id}/state/outputs'
            StorageSystemFactory.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        complete_job_func()

async def process_aggregate_draft_job(job_id: str, complete_job_func: callable):
    """Aggregate Draft Generation Job ì²˜ë¦¬"""
    
    try:
        LoggingUtil.info("main", f"ğŸš€ Aggregate Draft ìƒì„± ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/aggregate_draft_generator/{job_id}'
        job_data = await asyncio.to_thread(
            StorageSystemFactory.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        bounded_context = inputs_data.get('boundedContext', {})
        
        # traceMap ìƒì„± (í”„ë¡ íŠ¸ì—”ë“œì™€ ë™ì¼í•œ ë¡œì§)
        # boundedContext.requirements ë°°ì—´ì´ ìˆìœ¼ë©´ traceMap ìƒì„±
        if bounded_context.get('requirements') and isinstance(bounded_context['requirements'], list):
            relations = inputs_data.get('relations', [])
            explanations = inputs_data.get('explanations', [])
            analysis_result = inputs_data.get('analysisResult', {})
            events = analysis_result.get('events', []) if isinstance(analysis_result, dict) else []
            
            # ì›ë³¸ ìš”êµ¬ì‚¬í•­ êµ¬ì„± (traceMap ìƒì„± ì‹œ ì›ë³¸ ë¼ì¸ ê¸¸ì´ ê³„ì‚°ìš©)
            # inputs_dataì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê±°ë‚˜, requirements ë°°ì—´ì—ì„œ ì¶”ì¶œ
            original_requirements = inputs_data.get('originalRequirements', '')
            if not original_requirements:
                # requirements ë°°ì—´ì—ì„œ userStoryì™€ ddl ì¶”ì¶œ
                user_story_parts = []
                ddl_parts = []
                for req in bounded_context['requirements']:
                    req_type = req.get('type', '').lower()
                    req_text = req.get('text', '')
                    if req_type == 'userstory' and req_text:
                        user_story_parts.append(req_text)
                    elif req_type == 'ddl' and req_text:
                        ddl_parts.append(req_text)
            try:
                # í”„ë¡ íŠ¸ì—”ë“œì™€ ë™ì¼: ì›ë³¸ ìš”êµ¬ì‚¬í•­ì„ ì „ë‹¬í•˜ì§€ ì•ŠìŒ
                bc_description_with_mapping = TraceMarkdownUtil.get_description_with_mapping_index(
                    bounded_context,
                    relations,
                    explanations,
                    events
                )
                
                # traceMapì„ requirementsì— ì¶”ê°€
                if not isinstance(bounded_context.get('requirements'), dict):
                    # requirementsê°€ ë°°ì—´ì¸ ê²½ìš°, dictë¡œ ë³€í™˜
                    requirements_dict = {
                        'traceMap': bc_description_with_mapping['traceMap'],
                        'description': bc_description_with_mapping['markdown']
                    }
                    # ê¸°ì¡´ requirements ë°°ì—´ ì •ë³´ë„ ìœ ì§€
                    if bounded_context['requirements']:
                        requirements_dict['userStory'] = ''
                        requirements_dict['ddl'] = ''
                        requirements_dict['event'] = ''
                        # requirements ë°°ì—´ì„ íƒ€ì…ë³„ë¡œ ë¶„ë¥˜
                        for req in bounded_context['requirements']:
                            req_type = req.get('type', '').lower()
                            req_text = req.get('text', '')
                            if req_type == 'userstory' and req_text:
                                requirements_dict['userStory'] += req_text + '\n\n'
                            elif req_type == 'ddl' and req_text:
                                requirements_dict['ddl'] += req_text + '\n\n'
                            elif req_type == 'event' and req_text:
                                requirements_dict['event'] += req_text + '\n\n'
                    
                    bounded_context['requirements'] = requirements_dict
                else:
                    # requirementsê°€ ì´ë¯¸ dictì¸ ê²½ìš°
                    bounded_context['requirements']['traceMap'] = bc_description_with_mapping['traceMap']
                    if 'description' not in bounded_context['requirements']:
                        bounded_context['requirements']['description'] = bc_description_with_mapping['markdown']
                
                LoggingUtil.info("main", f"âœ… traceMap ìƒì„± ì™„ë£Œ: {len(bc_description_with_mapping['traceMap'])} lines")
            except Exception as e:
                LoggingUtil.warning("main", f"âš ï¸ traceMap ìƒì„± ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
                # traceMap ìƒì„± ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                if not isinstance(bounded_context.get('requirements'), dict):
                    bounded_context['requirements'] = {'traceMap': {}}
                elif 'traceMap' not in bounded_context['requirements']:
                    bounded_context['requirements']['traceMap'] = {}
        
        inputs = {
            'bounded_context': bounded_context,
            'description': inputs_data.get('description', ''),
            'accumulated_drafts': inputs_data.get('accumulatedDrafts', {}),
            'analysis_result': inputs_data.get('analysisResult', {})
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        generator = AggregateDraftGenerator()
        result = generator.run(inputs)
        
        # ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥
        # defaultOptionIndex: 1-based (LLM) â†’ 0-based (í”„ë¡ íŠ¸ì—”ë“œ)
        llm_default_index = result.get('default_option_index', 1)
        frontend_default_index = max(0, llm_default_index - 1)  # 1-based â†’ 0-based
        
        output = {
            'inference': result.get('inference', ''),
            'options': result.get('options', []),
            'defaultOptionIndex': frontend_default_index,
            'conclusions': result.get('conclusions', ''),
            'progress': result.get('progress', 100),
            'logs': result.get('logs', [])
        }
        
        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        output_path = f'{job_path}/state/outputs'
        sanitized_output = StorageSystemFactory.instance().sanitize_data_for_storage(output)
        await asyncio.to_thread(
            StorageSystemFactory.instance().set_data,
            output_path,
            sanitized_output
        )
        
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            StorageSystemFactory.instance().update_data,
            output_path,
            {'isCompleted': result.get('is_completed', True)}
        )
        
        # ìš”ì²­ Job ì œê±°
        req_path = f'requestedJobs/aggregate_draft_generator/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ Aggregate Draft ìƒì„± ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        LoggingUtil.exception("main", f"Aggregate Draft ìƒì„± ì˜¤ë¥˜: {job_id}", e)
        
        try:
            error_output = {
                'isFailed': True,
                'error': str(e),
                'progress': 0,
                'options': [],
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/aggregate_draft_generator/{job_id}/state/outputs'
            StorageSystemFactory.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        complete_job_func()


async def process_preview_fields_job(job_id: str, complete_job_func: callable):
    """Preview Fields Generation Job ì²˜ë¦¬"""
    
    try:
        LoggingUtil.info("main", f"ğŸš€ Preview Fields ìƒì„± ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/preview_fields_generator/{job_id}'
        job_data = await asyncio.to_thread(
            StorageSystemFactory.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        trace_map = inputs_data.get('traceMap', {})
        
        # traceMap ë³µì› (Firebaseê°€ ë°°ì—´ë¡œ ë³€í™˜í•œ ê²½ìš° ì²˜ë¦¬)
        if isinstance(trace_map, list):
            LoggingUtil.warning("main", f"âš ï¸ Preview Fields: traceMapì´ ë°°ì—´ í˜•íƒœì…ë‹ˆë‹¤! ë³µì› ì¤‘...")
            temp_generator = PreviewFieldsGenerator()
            trace_map = temp_generator._restore_trace_map(trace_map)
            LoggingUtil.info("main", f"âœ… Preview Fields: traceMap ë³µì› ì™„ë£Œ, keys={len(trace_map) if isinstance(trace_map, dict) else 0}")
        elif isinstance(trace_map, dict):
            LoggingUtil.info("main", f"âœ… Preview Fields: traceMap êµ¬ì¡° í™•ì¸ (dict), keys={len(trace_map)}")
        
        # í”„ë¡ íŠ¸ì—”ë“œ ì—ì´ì „íŠ¸ ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ descriptionë§Œ ì‚¬ìš©
        inputs = {
            'description': inputs_data.get('description', ''),
            'aggregateDrafts': inputs_data.get('aggregateDrafts', []),
            'generatorKey': inputs_data.get('generatorKey', 'default'),
            'traceMap': trace_map,
            'originalRequirements': inputs_data.get('originalRequirements', '')  # ì›ë³¸ ìš”êµ¬ì‚¬í•­ (userStory + ddl)
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        generator = PreviewFieldsGenerator()
        result = generator.run(inputs)
        
        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        output = {
            'inference': result.get('inference', ''),
            'aggregateFieldAssignments': result.get('aggregateFieldAssignments', []),
            'progress': result.get('progress', 100),
            'logs': result.get('logs', [])
        }
        
        output_path = f'{job_path}/state/outputs'
        sanitized_output = StorageSystemFactory.instance().sanitize_data_for_storage(output)
        await asyncio.to_thread(
            StorageSystemFactory.instance().set_data,
            output_path,
            sanitized_output
        )
        
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            StorageSystemFactory.instance().update_data,
            output_path,
            {'isCompleted': result.get('isCompleted', True)}
        )
        
        # ìš”ì²­ Job ì œê±°
        req_path = f'requestedJobs/preview_fields_generator/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ Preview Fields ìƒì„± ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        LoggingUtil.exception("main", f"Preview Fields ìƒì„± ì˜¤ë¥˜: {job_id}", e)
        
        try:
            error_output = {
                'isFailed': True,
                'isCompleted': True,
                'progress': 100,
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/preview_fields_generator/{job_id}/state/outputs'
            StorageSystemFactory.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        complete_job_func()


async def process_ddl_fields_job(job_id: str, complete_job_func: callable):
    """DDL Fields Assignment Job ì²˜ë¦¬"""
    
    try:
        LoggingUtil.info("main", f"ğŸš€ DDL Fields í• ë‹¹ ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/ddl_fields_generator/{job_id}'
        job_data = await asyncio.to_thread(
            StorageSystemFactory.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        input_data = {
            'description': inputs_data.get('description', ''),
            'aggregate_drafts': inputs_data.get('aggregateDrafts', []),
            'all_ddl_fields': inputs_data.get('allDdlFields', []),
            'generator_key': inputs_data.get('generatorKey', 'default')
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        generator = DDLFieldsGenerator()
        result = generator.generate(input_data)
        
        # ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥
        output = {
            'inference': result.get('inference', ''),
            'aggregateFieldAssignments': result.get('result', {}).get('aggregateFieldAssignments', []),
            'progress': 100,
            'logs': [{'timestamp': result.get('timestamp', ''), 'level': 'info', 'message': 'DDL fields assigned successfully'}]
        }
        
        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        output_path = f'{job_path}/state/outputs'
        sanitized_output = StorageSystemFactory.instance().sanitize_data_for_storage(output)
        await asyncio.to_thread(
            StorageSystemFactory.instance().set_data,
            output_path,
            sanitized_output
        )
        
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            StorageSystemFactory.instance().update_data,
            output_path,
            {'isCompleted': True}
        )
        
        # ìš”ì²­ Job ì œê±°
        req_path = f'requestedJobs/ddl_fields_generator/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ DDL Fields í• ë‹¹ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        LoggingUtil.exception("main", f"DDL Fields í• ë‹¹ ì˜¤ë¥˜: {job_id}", e)
        
        try:
            error_output = {
                'isFailed': True,
                'isCompleted': True,
                'progress': 100,
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/ddl_fields_generator/{job_id}/state/outputs'
            StorageSystemFactory.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        complete_job_func()


async def process_standard_transformation_job(job_id: str, complete_job_func: callable):
    """Standard Transformation Job ì²˜ë¦¬"""
    transformer = None  # ë³€ìˆ˜ ìŠ¤ì½”í”„ë¥¼ ìœ„í•´ í•¨ìˆ˜ ì‹œì‘ ë¶€ë¶„ì—ì„œ ì´ˆê¸°í™”
    try:
        LoggingUtil.info("main", f"ğŸš€ í‘œì¤€ ë³€í™˜ ì‹œì‘: {job_id}")

        job_path = f'jobs/standard_transformer/{job_id}'
        job_data = await asyncio.to_thread(
            StorageSystemFactory.instance().get_data,
            job_path
        )

        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return

        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})

        draft_options = inputs_data.get('draftOptions', [])
        bounded_context = inputs_data.get('boundedContext', {})
        transformation_session_id = inputs_data.get('transformationSessionId', None)
        user_id = inputs_data.get('userId', None)

        # Storage ì—…ë°ì´íŠ¸ ì½œë°± í•¨ìˆ˜ ì •ì˜
        output_path = f'{job_path}/state/outputs'
        storage = StorageSystemFactory.instance()
        
        # í‘œì¤€ ë³€í™˜ê¸° ì‹¤í–‰
        # transformationSessionIdê°€ ìˆìœ¼ë©´ ë””ë ‰í† ë¦¬ëª…ìœ¼ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ job_id ì‚¬ìš©
        result_dir_name = transformation_session_id if transformation_session_id else job_id
        transformer = AggregateDraftStandardTransformer(enable_rag=True, user_id=user_id)
        
        async def storage_update_callback(update_data: dict):
            """Storageì— ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ (Firebase/AceBase ê³µí†µ)"""
            try:
                sanitized_data = storage.sanitize_data_for_storage(update_data)
                await storage.update_data_async(output_path, sanitized_data)
            except Exception as e:
                LoggingUtil.warning("main", f"Storage ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ë™ê¸° í•¨ìˆ˜ë¡œ Storage ì—…ë°ì´íŠ¸ (transform ë‚´ë¶€ì—ì„œ í˜¸ì¶œ)
        def sync_storage_update(update_data: dict):
            """ë™ê¸° í•¨ìˆ˜ë¡œ Storage ì—…ë°ì´íŠ¸ (transform ë‚´ë¶€ì—ì„œ í˜¸ì¶œ)"""
            try:
                sanitized_data = storage.sanitize_data_for_storage(update_data)
                storage.update_data(output_path, sanitized_data)
            except Exception as e:
                LoggingUtil.warning("main", f"Storage ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        
        result = transformer.transform(
            draft_options, 
            bounded_context, 
            job_id=result_dir_name,
            firebase_update_callback=sync_storage_update,  # Storage ì—…ë°ì´íŠ¸ ì½œë°± (Firebase/AceBase ê³µí†µ)
            transformation_session_id=transformation_session_id  # ì„¸ì…˜ ID ì „ë‹¬
        )

        # errorê°€ Noneì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì´ë©´ ì œì™¸
        # transformedOptions ë˜ëŠ” transformed_options ë‘˜ ë‹¤ í™•ì¸ (í˜¸í™˜ì„±)
        transformed_options = result.get('transformedOptions') or result.get('transformed_options') or draft_options
        transformation_log = result.get('transformationLog') or result.get('transformation_log') or ''
        is_completed = result.get('isCompleted') if 'isCompleted' in result else result.get('is_completed', True)
        
        output = {
            'transformedOptions': transformed_options,
            'transformationLog': transformation_log,
            'progress': 100
        }
        
        # errorê°€ ì‹¤ì œë¡œ ìˆì„ ë•Œë§Œ ì¶”ê°€
        error = result.get('error')
        if error:
            output['error'] = error

        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        output_path = f'{job_path}/state/outputs'
        sanitized_output = StorageSystemFactory.instance().sanitize_data_for_storage(output)
        await asyncio.to_thread(
            StorageSystemFactory.instance().set_data,
            output_path,
            sanitized_output
        )
        
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            StorageSystemFactory.instance().update_data,
            output_path,
            {'isCompleted': is_completed}
        )

        req_path = f'requestedJobs/standard_transformer/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )

        LoggingUtil.info("main", f"ğŸ‰ í‘œì¤€ ë³€í™˜ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        LoggingUtil.exception("main", f"í‘œì¤€ ë³€í™˜ ì˜¤ë¥˜: {job_id}", e)
        
        # ì—ëŸ¬ ìƒíƒœ ì €ì¥
        try:
            job_path = f'jobs/standard_transformer/{job_id}'
            output_path = f'{job_path}/state/outputs'
            error_output = {
                'transformedOptions': inputs_data.get('draftOptions', []),  # ì›ë³¸ ë°˜í™˜
                'transformationLog': f'ë³€í™˜ ì‹¤íŒ¨: {str(e)}',
                'isCompleted': False,
                'progress': 0,
                'error': str(e)
            }
            sanitized_output = StorageSystemFactory.instance().sanitize_data_for_storage(error_output)
            await asyncio.to_thread(
                StorageSystemFactory.instance().set_data,
                output_path,
                sanitized_output
            )
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì—ëŸ¬ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {job_id}", save_error)
    finally:
        # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ ì‚¬ìš©ìë³„ ì„ì‹œ ë¬¸ì„œ ì •ë¦¬
        if transformer:
            try:
                if not transformation_session_id:
                    # transformation_session_idê°€ ì—†ìœ¼ë©´ ê° jobì´ ë…ë¦½ì ì´ë¯€ë¡œ ì¦‰ì‹œ ì •ë¦¬
                    transformer.cleanup_user_standards()
                else:
                    # transformation_session_idê°€ ìˆìœ¼ë©´ ê°™ì€ ì„¸ì…˜ì˜ ë‹¤ë¥¸ BCê°€ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸
                    # requestedJobs/standard_transformerì—ì„œ ê°™ì€ ì„¸ì…˜ì˜ ë‹¤ë¥¸ job í™•ì¸
                    requested_jobs = await asyncio.to_thread(
                                StorageSystemFactory.instance().get_children_data,
                        'requestedJobs/standard_transformer'
                    )
                    
                    # ê°™ì€ ì„¸ì…˜ì˜ ë‹¤ë¥¸ jobì´ ìˆëŠ”ì§€ í™•ì¸
                    has_other_session_jobs = False
                    if requested_jobs:
                        for other_job_id, other_job_data in requested_jobs.items():
                            if other_job_id == job_id:
                                continue  # í˜„ì¬ jobì€ ì œì™¸
                            
                            # ë‹¤ë¥¸ jobì˜ transformationSessionId í™•ì¸
                            other_job_path = f'jobs/standard_transformer/{other_job_id}'
                            other_job_data_full = await asyncio.to_thread(
                                StorageSystemFactory.instance().get_data,
                                other_job_path
                            )
                            
                            if other_job_data_full:
                                other_state = other_job_data_full.get('state', {})
                                other_inputs = other_state.get('inputs', {})
                                other_session_id = other_inputs.get('transformationSessionId', None)
                                
                                if other_session_id == transformation_session_id:
                                    has_other_session_jobs = True
                                    break
                    
                    # ê°™ì€ ì„¸ì…˜ì˜ ë‹¤ë¥¸ jobì´ ì—†ìœ¼ë©´ cleanup ìˆ˜í–‰
                    if not has_other_session_jobs:
                        LoggingUtil.info("main", f"ğŸ§¹ ì„¸ì…˜({transformation_session_id})ì˜ ëª¨ë“  BC ì²˜ë¦¬ ì™„ë£Œ, í‘œì¤€ ë¬¸ì„œ ì •ë¦¬ ì‹œì‘")
                        transformer.cleanup_user_standards()
                    else:
                        LoggingUtil.debug("main", f"â³ ì„¸ì…˜({transformation_session_id})ì˜ ë‹¤ë¥¸ BCê°€ ì•„ì§ ì²˜ë¦¬ ì¤‘, cleanup ëŒ€ê¸°")
            except Exception as cleanup_error:
                LoggingUtil.warning("main", f"ì‚¬ìš©ì í‘œì¤€ ë¬¸ì„œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {cleanup_error}")
        complete_job_func()


async def process_traceability_job(job_id: str, complete_job_func: callable):
    """Traceability Addition Job ì²˜ë¦¬"""
    try:
        LoggingUtil.info("main", f"ğŸš€ Traceability ì¶”ê°€ ì‹œì‘: {job_id}")

        job_path = f'jobs/traceability_generator/{job_id}'
        job_data = await asyncio.to_thread(
            StorageSystemFactory.instance().get_data,
            job_path
        )

        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return

        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})

        trace_map = inputs_data.get('traceMap', {})
        
        # traceMap ë³µì› (Firebaseê°€ ë°°ì—´ë¡œ ë³€í™˜í•œ ê²½ìš° ì²˜ë¦¬)
        if isinstance(trace_map, list):
            LoggingUtil.warning("main", f"âš ï¸ Traceability: traceMapì´ ë°°ì—´ í˜•íƒœì…ë‹ˆë‹¤! ë³µì› ì¤‘... (ë°°ì—´ ê¸¸ì´: {len(trace_map)})")
            # ì›ë³¸ ë°°ì—´ì—ì„œ í‚¤ ìƒ˜í”Œ í™•ì¸ (ë³µì› ì „) - ì „ì²´ í™•ì¸
            original_keys = []
            for item in trace_map:  # ì „ì²´ í™•ì¸
                if isinstance(item, dict) and 'key' in item:
                    try:
                        key = int(item['key'])
                        original_keys.append(key)
                    except (ValueError, TypeError):
                        pass
            if original_keys:
                original_odd = sorted([k for k in original_keys if k % 2 == 1])[:20]
                original_even = sorted([k for k in original_keys if k % 2 == 0])[:20]
                odd_count = len([k for k in original_keys if k % 2 == 1])
                even_count = len([k for k in original_keys if k % 2 == 0])
                LoggingUtil.info("main", f"ğŸ“‹ ì›ë³¸ ë°°ì—´ í‚¤ ë¶„ì„ - ì´ í‚¤ ìˆ˜: {len(original_keys)}, "
                    f"í™€ìˆ˜ í‚¤ ìˆ˜: {odd_count}, ì§ìˆ˜ í‚¤ ìˆ˜: {even_count}, "
                    f"í™€ìˆ˜ ìƒ˜í”Œ: {original_odd[:10]}, ì§ìˆ˜ ìƒ˜í”Œ: {original_even[:10]}")
            else:
                LoggingUtil.warning("main", f"âš ï¸ ì›ë³¸ ë°°ì—´ì—ì„œ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ë°°ì—´ êµ¬ì¡° í™•ì¸ í•„ìš”")
                if trace_map and len(trace_map) > 0:
                    # ë°°ì—´ êµ¬ì¡° ìƒì„¸ ë¶„ì„
                    first_item = trace_map[0]
                    LoggingUtil.info("main", f"ğŸ” ë°°ì—´ ì²« ë²ˆì§¸ í•­ëª© íƒ€ì…: {type(first_item)}, "
                        f"ë‚´ìš©: {str(first_item)[:200] if first_item else 'None'}")
                    if isinstance(first_item, dict):
                        LoggingUtil.info("main", f"ğŸ” ì²« ë²ˆì§¸ í•­ëª©ì˜ í‚¤ë“¤: {list(first_item.keys()) if first_item else []}")
                    # ì—¬ëŸ¬ í•­ëª© ìƒ˜í”Œ í™•ì¸
                    sample_items = []
                    for i, item in enumerate(trace_map[:5]):
                        if isinstance(item, dict):
                            sample_items.append(f"í•­ëª©{i}: keys={list(item.keys())}")
                        else:
                            sample_items.append(f"í•­ëª©{i}: type={type(item).__name__}")
                    if sample_items:
                        LoggingUtil.info("main", f"ğŸ” ë°°ì—´ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ): {'; '.join(sample_items)}")
            
            temp_generator = TraceabilityGenerator()
            trace_map = temp_generator._restore_trace_map(trace_map)
            if isinstance(trace_map, dict):
                # ë³µì›ëœ í‚¤ ìƒ˜í”Œ í™•ì¸ (í™€ìˆ˜/ì§ìˆ˜ ëª¨ë‘ í™•ì¸)
                # í‚¤ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬ (ë¬¸ìì—´ê³¼ ì •ìˆ˜ í˜¼í•© ì •ë ¬ ë°©ì§€)
                numeric_keys = []
                for k in trace_map.keys():
                    try:
                        if isinstance(k, int):
                            numeric_keys.append(k)
                        elif isinstance(k, str) and k.isdigit():
                            numeric_keys.append(int(k))
                    except (ValueError, TypeError):
                        pass
                sample_keys = sorted(numeric_keys)[:20]
                odd_keys = [k for k in sample_keys if k % 2 == 1]
                even_keys = [k for k in sample_keys if k % 2 == 0]
                LoggingUtil.info("main", f"âœ… Traceability: traceMap ë³µì› ì™„ë£Œ, keys={len(trace_map)}, "
                    f"ìƒ˜í”Œ í‚¤ (ì§ìˆ˜): {even_keys[:10]}, ìƒ˜í”Œ í‚¤ (í™€ìˆ˜): {odd_keys[:10]}")
            else:
                LoggingUtil.warning("main", f"âš ï¸ Traceability: traceMap ë³µì› ì‹¤íŒ¨, íƒ€ì…={type(trace_map)}")
        elif isinstance(trace_map, dict):
            # dictì¸ ê²½ìš°ë„ í‚¤ ìƒ˜í”Œ í™•ì¸
            # í‚¤ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬ (ë¬¸ìì—´ê³¼ ì •ìˆ˜ í˜¼í•© ì •ë ¬ ë°©ì§€)
            numeric_keys = []
            for k in trace_map.keys():
                try:
                    if isinstance(k, int):
                        numeric_keys.append(k)
                    elif isinstance(k, str) and k.isdigit():
                        numeric_keys.append(int(k))
                except (ValueError, TypeError):
                    pass
            sample_keys = sorted(numeric_keys)[:20]
            odd_keys = [k for k in sample_keys if k % 2 == 1]
            even_keys = [k for k in sample_keys if k % 2 == 0]
            LoggingUtil.info("main", f"âœ… Traceability: traceMap êµ¬ì¡° í™•ì¸ (dict), keys={len(trace_map)}, "
                f"ìƒ˜í”Œ í‚¤ (ì§ìˆ˜): {even_keys[:10]}, ìƒ˜í”Œ í‚¤ (í™€ìˆ˜): {odd_keys[:10]}")

        input_data = {
            'generatedDraftOptions': inputs_data.get('generatedDraftOptions', []),
            'boundedContextName': inputs_data.get('boundedContextName', ''),
            'description': inputs_data.get('description', ''),
            'functionalRequirements': inputs_data.get('functionalRequirements', ''),
            'traceMap': trace_map,
        }

        generator = TraceabilityGenerator()
        result = generator.generate(input_data)

        output = {
            'inference': result.get('inference', ''),
            'draftTraceMap': result.get('draftTraceMap', {}),
            'progress': 100,
            'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'info', 'message': 'Traceability mapping completed'}]
        }

        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        output_path = f'{job_path}/state/outputs'
        sanitized_output = StorageSystemFactory.instance().sanitize_data_for_storage(output)
        await asyncio.to_thread(
            StorageSystemFactory.instance().set_data,
            output_path,
            sanitized_output
        )
        
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            StorageSystemFactory.instance().update_data,
            output_path,
            {'isCompleted': True}
        )

        req_path = f'requestedJobs/traceability_generator/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )

        LoggingUtil.info("main", f"ğŸ‰ Traceability ì¶”ê°€ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    except Exception as e:
        LoggingUtil.exception("main", f"Traceability ì¶”ê°€ ì˜¤ë¥˜: {job_id}", e)
        try:
            error_output = {
                'isFailed': True,
                'isCompleted': True,
                'progress': 100,
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/traceability_generator/{job_id}/state/outputs'
            StorageSystemFactory.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    finally:
        complete_job_func()


async def process_ddl_extractor_job(job_id: str, complete_job_func: callable):
    """DDL Extractor Job ì²˜ë¦¬"""
    try:
        LoggingUtil.info("main", f"ğŸš€ DDL í•„ë“œ ì¶”ì¶œ ì‹œì‘: {job_id}")

        job_path = f'jobs/ddl_extractor/{job_id}'
        job_data = await asyncio.to_thread(
            StorageSystemFactory.instance().get_data,
            job_path
        )

        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return

        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})

        input_data = {
            'ddlRequirements': inputs_data.get('ddlRequirements', []),
            'boundedContextName': inputs_data.get('boundedContextName', ''),
        }

        generator = DDLExtractor()
        result = generator.generate(input_data)

        output = {
            'inference': result.get('inference', ''),
            'ddlFieldRefs': result.get('ddlFieldRefs', []),
            'progress': 100,
            'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'info', 'message': 'DDL extraction completed'}]
        }

        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        output_path = f'{job_path}/state/outputs'
        sanitized_output = StorageSystemFactory.instance().sanitize_data_for_storage(output)
        await asyncio.to_thread(
            StorageSystemFactory.instance().set_data,
            output_path,
            sanitized_output
        )
        
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            StorageSystemFactory.instance().update_data,
            output_path,
            {'isCompleted': True}
        )

        req_path = f'requestedJobs/ddl_extractor/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )

        LoggingUtil.info("main", f"ğŸ‰ DDL í•„ë“œ ì¶”ì¶œ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    except Exception as e:
        LoggingUtil.exception("main", f"DDL ì¶”ì¶œ ì˜¤ë¥˜: {job_id}", e)
        try:
            error_output = {
                'isFailed': True,
                'isCompleted': True,
                'progress': 100,
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/ddl_extractor/{job_id}/state/outputs'
            StorageSystemFactory.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    finally:
        complete_job_func()


async def process_requirements_validator_job(job_id: str, complete_job_func: callable):
    """Requirements Validator Job ì²˜ë¦¬"""
    try:
        LoggingUtil.info("main", f"ğŸš€ ìš”êµ¬ì‚¬í•­ ê²€ì¦ ì‹œì‘: {job_id}")

        job_path = f'jobs/requirements_validator/{job_id}'
        job_data = await asyncio.to_thread(
            StorageSystemFactory.instance().get_data,
            job_path
        )

        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return

        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})

        input_data = {
            'requirements': inputs_data.get('requirements', {}),
            'previousChunkSummary': inputs_data.get('previousChunkSummary', {}),
            'currentChunkStartLine': inputs_data.get('currentChunkStartLine', 1),
        }

        generator = RequirementsValidator()
        result = generator.generate(input_data)

        output_path = f'{job_path}/state/outputs'
        storage = StorageSystemFactory.instance()

        content = result.get('content', {}) or {}
        final_length = 0
        try:
            final_length = len(json.dumps(content, ensure_ascii=False))
        except Exception:
            final_length = 0

        intermediate_lengths = _compute_intermediate_lengths(final_length, steps=3)

        for idx, length in enumerate(intermediate_lengths):
            progress_value = max(1, min(95, int(((idx + 1) / (len(intermediate_lengths) + 1)) * 100)))
            update_payload = {
                'currentGeneratedLength': length,
                'progress': progress_value,
                'isCompleted': False
            }
            await storage.update_data_async(
                output_path,
                storage.sanitize_data_for_storage(update_payload)
            )
            await asyncio.sleep(1)

        output = {
            'type': result.get('type', 'ANALYSIS_RESULT'),
            'content': result.get('content', {}),
            'progress': 100,
            'currentGeneratedLength': final_length,
            'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'info', 'message': 'Requirements validation completed'}]
        }

        # â˜… isCompletedë¥¼ ë§ˆì§€ë§‰ì— ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥
        # 1) isCompleted ì œì™¸í•œ ë°ì´í„° ë¨¼ì € ì €ì¥
        sanitized_output = StorageSystemFactory.instance().sanitize_data_for_storage(output)
        await asyncio.to_thread(
            StorageSystemFactory.instance().set_data,
            output_path,
            sanitized_output
        )
        
        # 2) ì§§ì€ ëŒ€ê¸° í›„ isCompleted ì €ì¥ (ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥)
        await asyncio.sleep(0.1)
        await asyncio.to_thread(
            StorageSystemFactory.instance().update_data,
            output_path,
            {'isCompleted': True}
        )

        req_path = f'requestedJobs/requirements_validator/{job_id}'
        await asyncio.to_thread(
            StorageSystemFactory.instance().delete_data,
            req_path
        )

        LoggingUtil.info("main", f"ğŸ‰ ìš”êµ¬ì‚¬í•­ ê²€ì¦ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    except Exception as e:
        LoggingUtil.exception("main", f"ìš”êµ¬ì‚¬í•­ ê²€ì¦ ì˜¤ë¥˜: {job_id}", e)
        try:
            error_output = {
                'isFailed': True,
                'isCompleted': True,
                'progress': 100,
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/requirements_validator/{job_id}/state/outputs'
            StorageSystemFactory.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    finally:
        complete_job_func()

async def process_job_async(job_id: str, complete_job_func: callable):
    """ë¹„ë™ê¸° Job ì²˜ë¦¬ í•¨ìˆ˜ (Job ID prefixë¡œ ë¼ìš°íŒ…)"""
    
    try:
        LoggingUtil.debug("main", f"Job ì‹œì‘: {job_id}")
        if not JobUtil.is_valid_job_id(job_id):
            LoggingUtil.warning("main", f"Job ì²˜ë¦¬ ì˜¤ë¥˜: {job_id}, ìœ íš¨í•˜ì§€ ì•ŠìŒ")
            return
        
        # Job íƒ€ì…ë³„ ë¼ìš°íŒ… (ê° í•¨ìˆ˜ì—ì„œ finally ë¸”ë¡ìœ¼ë¡œ complete_job_func í˜¸ì¶œ)
        if job_id.startswith("usgen-"):
            await process_user_story_job(job_id, complete_job_func)
        elif job_id.startswith("summ-"):
            await process_summarizer_job(job_id, complete_job_func)
        elif job_id.startswith("bcgen-"):
            await process_bounded_context_job(job_id, complete_job_func)
        elif job_id.startswith("cmrext-"):
            await process_command_readmodel_job(job_id, complete_job_func)
        elif job_id.startswith("smapgen-"):
            await process_sitemap_job(job_id, complete_job_func)
        elif job_id.startswith("reqmap-"):
            await process_requirements_mapping_job(job_id, complete_job_func)
        elif job_id.startswith("aggr-draft-"):
            await process_aggregate_draft_job(job_id, complete_job_func)
        elif job_id.startswith("preview-fields-"):
            await process_preview_fields_job(job_id, complete_job_func)
        elif job_id.startswith("ddl-fields-"):
            await process_ddl_fields_job(job_id, complete_job_func)
        elif job_id.startswith("trace-add-"):
            await process_traceability_job(job_id, complete_job_func)
        elif job_id.startswith("std-trans-"):
            await process_standard_transformation_job(job_id, complete_job_func)
        elif job_id.startswith("ddl-extract-"):
            await process_ddl_extractor_job(job_id, complete_job_func)
        elif job_id.startswith("req-valid-"):
            await process_requirements_validator_job(job_id, complete_job_func)
        else:
            LoggingUtil.warning("main", f"ì§€ì›í•˜ì§€ ì•ŠëŠ” Job íƒ€ì…: {job_id}")
            
    except asyncio.CancelledError:
        LoggingUtil.debug("main", f"Job {job_id} ì·¨ì†Œë¨")
        return
        
    except Exception as e:
        LoggingUtil.exception("main", f"Job ì²˜ë¦¬ ì˜¤ë¥˜: {job_id}", e)

if __name__ == "__main__":
    asyncio.run(main())