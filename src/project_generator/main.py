import asyncio
import concurrent.futures
import threading
import json
import math
import copy
from datetime import datetime
from dotenv import load_dotenv
load_dotenv()

from typing import List

from project_generator.utils import JobUtil, DecentralizedJobManager
from project_generator.systems.firebase_system import FirebaseSystem
from project_generator.config import Config
from project_generator.run_healcheck_server import run_healcheck_server
from project_generator.simple_autoscaler import start_autoscaler
from project_generator.utils.logging_util import LoggingUtil

# Workflow imports
from project_generator.workflows.user_story.user_story_generator import UserStoryWorkflow
from project_generator.workflows.summarizer.requirements_summarizer import RequirementsSummarizerWorkflow
from project_generator.workflows.bounded_context.bounded_context_generator import BoundedContextWorkflow
from project_generator.workflows.sitemap.command_readmodel_extractor import create_command_readmodel_workflow
from project_generator.workflows.sitemap.sitemap_generator import create_sitemap_workflow
from project_generator.workflows.aggregate_draft.requirements_mapper import RequirementsMappingWorkflow
from project_generator.workflows.aggregate_draft.aggregate_draft_generator import AggregateDraftGenerator
from project_generator.workflows.aggregate_draft.preview_fields_generator import PreviewFieldsGenerator
from project_generator.workflows.aggregate_draft.ddl_fields_generator import DDLFieldsGenerator
from project_generator.workflows.aggregate_draft.traceability_generator import TraceabilityGenerator
from project_generator.workflows.aggregate_draft.ddl_extractor import DDLExtractor
from project_generator.workflows.requirements_validation.requirements_validator import RequirementsValidator

# ì „ì—­ job_manager ì¸ìŠ¤í„´ìŠ¤
_current_job_manager: DecentralizedJobManager = None


def _compute_intermediate_lengths(final_length: int, steps: int = 3) -> List[int]:
    """
    ìµœì¢… ìƒì„± ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ê°„ ê¸¸ì´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê³„ì‚°.
    ìŠ¤íŠ¸ë¦¬ë°ì´ ì–´ë ¤ìš´ ì›Œí¬í”Œë¡œìš°ì—ì„œ ì£¼ê¸°ì  ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ìš©ë„ë¡œ ì‚¬ìš©.
    """
    if final_length <= 0 or steps <= 0:
        return []

    lengths = set()
    for idx in range(1, steps + 1):
        length = max(1, min(final_length - 1, (final_length * idx) // (steps + 1)))
        lengths.add(length)

    intermediate = sorted(lengths)
    return intermediate


async def main():
    """ë©”ì¸ í•¨ìˆ˜ - Flask ì„œë²„, Job ëª¨ë‹ˆí„°ë§, ìë™ ìŠ¤ì¼€ì¼ëŸ¬ ë™ì‹œ ì‹œì‘"""
    
    flask_thread = None
    restart_count = 0
    
    while True:
        tasks = []
        job_manager = None
        
        try:
            
            # Flask ì„œë²„ ì‹œì‘ (ì²« ì‹¤í–‰ì‹œì—ë§Œ)
            if flask_thread is None:
                flask_thread = threading.Thread(target=run_healcheck_server, daemon=True)
                flask_thread.start()
                LoggingUtil.info("main", "Flask ì„œë²„ê°€ í¬íŠ¸ 2024ì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
                LoggingUtil.info("main", "í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸: http://localhost:2024/ok")

            if restart_count > 0:
                LoggingUtil.info("main", f"ë©”ì¸ í•¨ìˆ˜ ì¬ì‹œì‘ ì¤‘... (ì¬ì‹œì‘ íšŸìˆ˜: {restart_count})")

            pod_id = Config.get_pod_id()
            job_manager = DecentralizedJobManager(pod_id, process_job_async)
            
            # ì „ì—­ job_manager ì„¤ì •
            global _current_job_manager
            _current_job_manager = job_manager
            
            # ê°ì‹œí•  namespace ëª©ë¡
            monitored_namespaces = ['user_story_generator', 'summarizer', 'bounded_context', 'command_readmodel_extractor', 'sitemap_generator', 'requirements_mapper', 'aggregate_draft_generator', 'preview_fields_generator', 'ddl_fields_generator', 'traceability_generator', 'ddl_extractor', 'requirements_validator']
            
            if Config.is_local_run():
                tasks.append(asyncio.create_task(job_manager.start_job_monitoring(monitored_namespaces)))
                LoggingUtil.info("main", "ì‘ì—… ëª¨ë‹ˆí„°ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                tasks.append(asyncio.create_task(start_autoscaler()))
                tasks.append(asyncio.create_task(job_manager.start_job_monitoring(monitored_namespaces)))
                LoggingUtil.info("main", "ìë™ ìŠ¤ì¼€ì¼ëŸ¬ ë° ì‘ì—… ëª¨ë‹ˆí„°ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            
            # shutdown_event ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì¶”ê°€
            shutdown_monitor_task = asyncio.create_task(job_manager.shutdown_event.wait())
            tasks.append(shutdown_monitor_task)
            
            # íƒœìŠ¤í¬ë“¤ ì¤‘ í•˜ë‚˜ë¼ë„ ì™„ë£Œë˜ë©´ ì¢…ë£Œ (shutdown_event í¬í•¨)
            done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
            
            # shutdown_eventê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if shutdown_monitor_task in done:
                LoggingUtil.info("main", "Graceful shutdown ì‹ í˜¸ ìˆ˜ì‹ . ë©”ì¸ ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                
                # ë‚˜ë¨¸ì§€ ì‹¤í–‰ ì¤‘ì¸ íƒœìŠ¤í¬ë“¤ ì·¨ì†Œ
                for task in pending:
                    if not task.done():
                        LoggingUtil.debug("main", f"íƒœìŠ¤í¬ ì·¨ì†Œ ì¤‘: {task}")
                        task.cancel()
                        try:
                            await task
                        except asyncio.CancelledError:
                            LoggingUtil.debug("main", "íƒœìŠ¤í¬ê°€ ì •ìƒì ìœ¼ë¡œ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                        except Exception as cleanup_error:
                            LoggingUtil.exception("main", "íƒœìŠ¤í¬ ì •ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ", cleanup_error)
                
                LoggingUtil.info("main", "ë©”ì¸ í•¨ìˆ˜ ì •ìƒ ì¢…ë£Œ")
                break  # while ë£¨í”„ ì¢…ë£Œ
            
        except Exception as e:
            restart_count += 1
            LoggingUtil.exception("main", f"ë©”ì¸ í•¨ìˆ˜ì—ì„œ ì˜ˆì™¸ ë°œìƒ (ì¬ì‹œì‘ íšŸìˆ˜: {restart_count})", e)
            
            # ì‹¤í–‰ ì¤‘ì¸ íƒœìŠ¤í¬ë“¤ ì •ë¦¬
            for task in tasks:
                if not task.done():
                    LoggingUtil.debug("main", f"íƒœìŠ¤í¬ ì·¨ì†Œ ì¤‘: {task}")
                    task.cancel()
                    try:
                        await task
                    except asyncio.CancelledError:
                        LoggingUtil.debug("main", "íƒœìŠ¤í¬ê°€ ì •ìƒì ìœ¼ë¡œ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    except Exception as cleanup_error:
                        LoggingUtil.exception("main", "íƒœìŠ¤í¬ ì •ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ", cleanup_error)

            continue


async def process_summarizer_job(job_id: str, complete_job_func: callable):
    """Summarizer Job ì²˜ë¦¬ í•¨ìˆ˜"""
    error_occurred = None
    try:
        LoggingUtil.info("main", f"ğŸš€ Summarizer ì²˜ë¦¬ ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë”©
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            job_path = f'jobs/summarizer/{job_id}'
            job_data = await loop.run_in_executor(
                executor,
                lambda: FirebaseSystem.instance().get_data(job_path)
            )
        
        if not job_data:
            LoggingUtil.warning("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        inputs = job_data.get("state", {}).get("inputs", {})
        if not inputs:
            LoggingUtil.warning("main", f"Job inputs ì—†ìŒ: {job_id}")
            return
        
        # SummarizerWorkflow ì‹¤í–‰
        workflow = RequirementsSummarizerWorkflow()
        result = await asyncio.to_thread(workflow.run, inputs)
        
        summaries = result.get('summarizedRequirements', [])
        LoggingUtil.info("main", f"âœ… ìš”ì•½ ì™„ë£Œ: {len(summaries)}ê°œ")
        
        # ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥
        output_path = f'jobs/summarizer/{job_id}/state/outputs'
        await asyncio.to_thread(
            FirebaseSystem.instance().set_data,
            output_path,
            result
        )
        
        # requestedJob ì‚­ì œ
        req_path = f'requestedJobs/summarizer/{job_id}'
        await asyncio.to_thread(
            FirebaseSystem.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        error_occurred = e
        LoggingUtil.exception("main", f"Summarizer Job ì²˜ë¦¬ ì˜¤ë¥˜: {job_id}", e)
        
        # ì‹¤íŒ¨ ìƒíƒœ ì €ì¥
        try:
            error_output = {
                "summarizedRequirements": [],
                "isCompleted": False,
                "error": str(e),
                "logs": [{
                    "timestamp": datetime.now().isoformat(),
                    "message": f"ì˜¤ë¥˜: {str(e)}"
                }]
            }
            
            output_path = f'jobs/summarizer/{job_id}/state/outputs'
            await asyncio.to_thread(
                FirebaseSystem.instance().set_data,
                output_path,
                error_output
            )
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        # ì˜ˆì™¸ ë°œìƒ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ complete_job_func í˜¸ì¶œ
        complete_job_func()

async def process_user_story_job(job_id: str, complete_job_func: callable):
    """UserStory Job ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        LoggingUtil.info("main", f"ğŸš€ UserStory ì²˜ë¦¬ ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë”© (user_story_generator namespace ì‚¬ìš©)
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            job_path = f'jobs/user_story_generator/{job_id}'
            job_data = await loop.run_in_executor(
                executor,
                lambda: FirebaseSystem.instance().get_data(job_path)
            )
        
        if not job_data:
            LoggingUtil.warning("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        inputs = job_data.get("state", {}).get("inputs", {})
        if not inputs:
            LoggingUtil.warning("main", f"Job inputs ì—†ìŒ: {job_id}")
            return
        
        # UserStoryWorkflow ì‹¤í–‰
        workflow = UserStoryWorkflow()
        result = await asyncio.to_thread(workflow.run, inputs)
        
        # ê²°ê³¼ëŠ” ì´ë¯¸ camelCaseë¡œ ë³€í™˜ë˜ì–´ ìˆìŒ
        user_stories = result.get('userStories', [])
        actors = result.get('actors', [])
        business_rules = result.get('businessRules', [])
        LoggingUtil.info("main", f"âœ… ìƒì„± ì™„ë£Œ: Stories {len(user_stories)}, Actors {len(actors)}, Rules {len(business_rules)}")
        
        # ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥ (ë¹„ë™ê¸° ì²˜ë¦¬)
        output_path = f'jobs/user_story_generator/{job_id}/state/outputs'
        await asyncio.to_thread(
            FirebaseSystem.instance().set_data,
            output_path,
            result
        )
        
        # requestedJob ì‚­ì œ
        req_path = f'requestedJobs/user_story_generator/{job_id}'
        await asyncio.to_thread(
            FirebaseSystem.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        LoggingUtil.exception("main", f"ì²˜ë¦¬ ì˜¤ë¥˜: {job_id}", e)
        
        # ì‹¤íŒ¨ ê¸°ë¡
        try:
            error_output = {
                'isFailed': True,
                'error': str(e),
                'progress': 0,
                'userStories': [],  # camelCase
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/user_story_generator/{job_id}/state/outputs'
            FirebaseSystem.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        # ì˜ˆì™¸ ë°œìƒ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ complete_job_func í˜¸ì¶œ
        complete_job_func()

async def process_bounded_context_job(job_id: str, complete_job_func: callable):
    """Bounded Context ìƒì„± Job ì²˜ë¦¬"""
    
    try:
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/bounded_context/{job_id}'
        job_data = await asyncio.to_thread(
            FirebaseSystem.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ (state.inputsì—ì„œ ê°€ì ¸ì˜´)
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        inputs = {
            'devisionAspect': inputs_data.get('devisionAspect', ''),
            'requirements': inputs_data.get('requirements', {}),
            'generateOption': inputs_data.get('generateOption', {}),
            'feedback': inputs_data.get('feedback'),
            'previousAspectModel': inputs_data.get('previousAspectModel')
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        workflow = BoundedContextWorkflow()
        result = await asyncio.to_thread(workflow.run, inputs)
        
        output_path = f'jobs/bounded_context/{job_id}/state/outputs'
        firebase = FirebaseSystem.instance()

        try:
            final_length = len(json.dumps(result, ensure_ascii=False))
        except Exception:
            final_length = 0

        intermediate_lengths = _compute_intermediate_lengths(final_length, steps=3)

        for idx, length in enumerate(intermediate_lengths):
            progress_value = max(1, min(95, int(((idx + 1) / (len(intermediate_lengths) + 1)) * 100)))
            update_payload = {
                'currentGeneratedLength': length,
                'progress': progress_value,
                'isCompleted': False
            }
            await firebase.update_data_async(
                output_path,
                firebase.sanitize_data_for_firebase(update_payload)
            )
            await asyncio.sleep(1)

        result_with_length = copy.deepcopy(result)
        result_with_length['currentGeneratedLength'] = final_length

        await asyncio.to_thread(
            firebase.set_data,
            output_path,
            firebase.sanitize_data_for_firebase(result_with_length)
        )
        
        # requestedJob ì‚­ì œ
        req_path = f'requestedJobs/bounded_context/{job_id}'
        await asyncio.to_thread(
            FirebaseSystem.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ BC ìƒì„± ì™„ë£Œ: {job_id}, BCs: {len(result.get('boundedContexts', []))}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        error_occurred = e
        LoggingUtil.exception("main", f"BC ìƒì„± ì˜¤ë¥˜: {job_id}", e)
        
        # ì‹¤íŒ¨ ê¸°ë¡
        try:
            error_output = {
                'isFailed': True,
                'error': str(e),
                'progress': 0,
                'thoughts': '',
                'boundedContexts': [],
                'relations': [],
                'explanations': [],
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/bounded_context/{job_id}/state/outputs'
            FirebaseSystem.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        # ì˜ˆì™¸ ë°œìƒ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ complete_job_func í˜¸ì¶œ
        complete_job_func()

async def process_command_readmodel_job(job_id: str, complete_job_func: callable):
    """Command/ReadModel ì¶”ì¶œ Job ì²˜ë¦¬"""
    
    try:
        LoggingUtil.info("main", f"ğŸš€ Command/ReadModel ì¶”ì¶œ ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/command_readmodel_extractor/{job_id}'
        job_data = await asyncio.to_thread(
            FirebaseSystem.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        inputs = {
            'job_id': job_id,
            'requirements': inputs_data.get('requirements', ''),
            'bounded_contexts': inputs_data.get('boundedContexts', []),
            'logs': [],
            'progress': 0,
            'is_completed': False,
            'is_failed': False,
            'error': '',
            'extracted_data': {}
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (recursion_limit ì¦ê°€)
        workflow = create_command_readmodel_workflow()
        result = await asyncio.to_thread(
            workflow.invoke, 
            inputs,
            {"recursion_limit": 50}
        )
        
        # ê²°ê³¼ ì €ì¥
        output_path = f'jobs/command_readmodel_extractor/{job_id}/state/outputs'
        await asyncio.to_thread(
            FirebaseSystem.instance().set_data,
            output_path,
            {
                'extractedData': result.get('extracted_data', {}),
                'logs': result.get('logs', []),
                'progress': result.get('progress', 0),
                'isCompleted': result.get('is_completed', False),
                'isFailed': result.get('is_failed', False),
                'error': result.get('error', '')
            }
        )
        
        # requestedJob ì‚­ì œ
        req_path = f'requestedJobs/command_readmodel_extractor/{job_id}'
        await asyncio.to_thread(
            FirebaseSystem.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ Command/ReadModel ì¶”ì¶œ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        error_occurred = e
        LoggingUtil.exception("main", f"Command/ReadModel ì¶”ì¶œ ì˜¤ë¥˜: {job_id}", e)
        
        # ì‹¤íŒ¨ ê¸°ë¡
        try:
            error_output = {
                'isFailed': True,
                'error': str(e),
                'progress': 0,
                'extractedData': {},
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/command_readmodel_extractor/{job_id}/state/outputs'
            FirebaseSystem.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        complete_job_func()

async def process_sitemap_job(job_id: str, complete_job_func: callable):
    """SiteMap ìƒì„± Job ì²˜ë¦¬"""
    
    try:
        LoggingUtil.info("main", f"ğŸš€ SiteMap ìƒì„± ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/sitemap_generator/{job_id}'
        job_data = await asyncio.to_thread(
            FirebaseSystem.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        inputs = {
            'job_id': job_id,
            'requirements': inputs_data.get('requirements', ''),
            'bounded_contexts': inputs_data.get('boundedContexts', []),
            'command_readmodel_data': inputs_data.get('commandReadModelData', {}),
            'existing_navigation': inputs_data.get('existingNavigation', []),
            'logs': [],
            'progress': 0,
            'is_completed': False,
            'is_failed': False,
            'error': '',
            'site_map': {}
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        workflow = create_sitemap_workflow()
        result = await asyncio.to_thread(
            workflow.invoke, 
            inputs,
            {"recursion_limit": 50}
        )
        
        output_path = f'jobs/sitemap_generator/{job_id}/state/outputs'
        firebase = FirebaseSystem.instance()

        try:
            final_length = len(json.dumps(result.get('site_map', {}), ensure_ascii=False))
        except Exception:
            final_length = 0

        intermediate_lengths = _compute_intermediate_lengths(final_length, steps=3)

        for idx, length in enumerate(intermediate_lengths):
            progress_value = max(1, min(95, int(((idx + 1) / (len(intermediate_lengths) + 1)) * 100)))
            update_payload = {
                'currentGeneratedLength': length,
                'progress': progress_value,
                'isCompleted': False
            }
            await firebase.update_data_async(
                output_path,
                firebase.sanitize_data_for_firebase(update_payload)
            )
            await asyncio.sleep(1)

        final_output = {
            'siteMap': result.get('site_map', {}),
            'logs': result.get('logs', []),
            'progress': result.get('progress', 0),
            'isCompleted': result.get('is_completed', False),
            'isFailed': result.get('is_failed', False),
            'error': result.get('error', ''),
            'currentGeneratedLength': final_length
        }

        await asyncio.to_thread(
            firebase.set_data,
            output_path,
            firebase.sanitize_data_for_firebase(final_output)
        )
        
        # requestedJob ì‚­ì œ
        req_path = f'requestedJobs/sitemap_generator/{job_id}'
        await asyncio.to_thread(
            FirebaseSystem.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ SiteMap ìƒì„± ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        error_occurred = e
        LoggingUtil.exception("main", f"SiteMap ìƒì„± ì˜¤ë¥˜: {job_id}", e)
        
        # ì‹¤íŒ¨ ê¸°ë¡
        try:
            error_output = {
                'isFailed': True,
                'error': str(e),
                'progress': 0,
                'siteMap': {},
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/sitemap_generator/{job_id}/state/outputs'
            FirebaseSystem.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        complete_job_func()

async def process_requirements_mapping_job(job_id: str, complete_job_func: callable):
    """Requirements Mapping Job ì²˜ë¦¬"""
    
    try:
        LoggingUtil.info("main", f"ğŸš€ Requirements Mapping ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/requirements_mapper/{job_id}'
        job_data = await asyncio.to_thread(
            FirebaseSystem.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        inputs = {
            'bounded_context': inputs_data.get('boundedContext', {}),
            'requirement_chunk': inputs_data.get('requirementChunk', {}),
            'relevant_requirements': [],
            'progress': 0,
            'logs': [],
            'is_completed': False,
            'error': ''
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        workflow = RequirementsMappingWorkflow()
        result = workflow.run(inputs)
        
        # ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥
        bounded_context = inputs_data.get('boundedContext', {}) or {}
        bc_name = bounded_context.get('name', '')
        
        output = {
            'boundedContext': bc_name,
            'requirements': result.get('relevant_requirements', []),
            'isCompleted': result.get('is_completed', True),
            'progress': result.get('progress', 100),
            'logs': result.get('logs', [])
        }
        
        output_path = f'{job_path}/state/outputs'
        # Firebaseì— ì €ì¥í•˜ê¸° ì „ì— ë°ì´í„° ì •ì œ
        sanitized_output = FirebaseSystem.instance().sanitize_data_for_firebase(output)
        await asyncio.to_thread(
            FirebaseSystem.instance().set_data,
            output_path,
            sanitized_output
        )
        
        # ìš”ì²­ Job ì œê±°
        req_path = f'requestedJobs/requirements_mapper/{job_id}'
        await asyncio.to_thread(
            FirebaseSystem.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ Requirements Mapping ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        LoggingUtil.exception("main", f"Requirements Mapping ì˜¤ë¥˜: {job_id}", e)
        
        # ì‹¤íŒ¨ ê¸°ë¡
        try:
            error_output = {
                'isFailed': True,
                'error': str(e),
                'progress': 0,
                'requirements': [],
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/requirements_mapper/{job_id}/state/outputs'
            FirebaseSystem.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        complete_job_func()

async def process_aggregate_draft_job(job_id: str, complete_job_func: callable):
    """Aggregate Draft Generation Job ì²˜ë¦¬"""
    
    try:
        LoggingUtil.info("main", f"ğŸš€ Aggregate Draft ìƒì„± ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/aggregate_draft_generator/{job_id}'
        job_data = await asyncio.to_thread(
            FirebaseSystem.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        inputs = {
            'bounded_context': inputs_data.get('boundedContext', {}),
            'description': inputs_data.get('description', ''),
            'accumulated_drafts': inputs_data.get('accumulatedDrafts', {}),
            'analysis_result': inputs_data.get('analysisResult', {})
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        generator = AggregateDraftGenerator()
        result = generator.run(inputs)
        
        # ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥
        output = {
            'inference': result.get('inference', ''),
            'options': result.get('options', []),
            'defaultOptionIndex': result.get('default_option_index', 1),
            'conclusions': result.get('conclusions', ''),
            'isCompleted': result.get('is_completed', True),
            'progress': result.get('progress', 100),
            'logs': result.get('logs', [])
        }
        
        output_path = f'{job_path}/state/outputs'
        sanitized_output = FirebaseSystem.instance().sanitize_data_for_firebase(output)
        await asyncio.to_thread(
            FirebaseSystem.instance().set_data,
            output_path,
            sanitized_output
        )
        
        # ìš”ì²­ Job ì œê±°
        req_path = f'requestedJobs/aggregate_draft_generator/{job_id}'
        await asyncio.to_thread(
            FirebaseSystem.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ Aggregate Draft ìƒì„± ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        LoggingUtil.exception("main", f"Aggregate Draft ìƒì„± ì˜¤ë¥˜: {job_id}", e)
        
        try:
            error_output = {
                'isFailed': True,
                'error': str(e),
                'progress': 0,
                'options': [],
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/aggregate_draft_generator/{job_id}/state/outputs'
            FirebaseSystem.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        complete_job_func()


async def process_preview_fields_job(job_id: str, complete_job_func: callable):
    """Preview Fields Generation Job ì²˜ë¦¬"""
    
    try:
        LoggingUtil.info("main", f"ğŸš€ Preview Fields ìƒì„± ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/preview_fields_generator/{job_id}'
        job_data = await asyncio.to_thread(
            FirebaseSystem.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        inputs = {
            'description': inputs_data.get('description', ''),
            'aggregateDrafts': inputs_data.get('aggregateDrafts', []),
            'generatorKey': inputs_data.get('generatorKey', 'default'),
            'traceMap': inputs_data.get('traceMap', {})
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        generator = PreviewFieldsGenerator()
        result = generator.run(inputs)
        
        # ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥
        output = {
            'inference': result.get('inference', ''),
            'aggregateFieldAssignments': result.get('aggregateFieldAssignments', []),
            'isCompleted': result.get('isCompleted', True),
            'progress': result.get('progress', 100),
            'logs': result.get('logs', [])
        }
        
        output_path = f'{job_path}/state/outputs'
        sanitized_output = FirebaseSystem.instance().sanitize_data_for_firebase(output)
        await asyncio.to_thread(
            FirebaseSystem.instance().set_data,
            output_path,
            sanitized_output
        )
        
        # ìš”ì²­ Job ì œê±°
        req_path = f'requestedJobs/preview_fields_generator/{job_id}'
        await asyncio.to_thread(
            FirebaseSystem.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ Preview Fields ìƒì„± ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        LoggingUtil.exception("main", f"Preview Fields ìƒì„± ì˜¤ë¥˜: {job_id}", e)
        
        try:
            error_output = {
                'isFailed': True,
                'isCompleted': True,
                'progress': 100,
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/preview_fields_generator/{job_id}/state/outputs'
            FirebaseSystem.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        complete_job_func()


async def process_ddl_fields_job(job_id: str, complete_job_func: callable):
    """DDL Fields Assignment Job ì²˜ë¦¬"""
    
    try:
        LoggingUtil.info("main", f"ğŸš€ DDL Fields í• ë‹¹ ì‹œì‘: {job_id}")
        
        # Job ë°ì´í„° ë¡œë“œ
        job_path = f'jobs/ddl_fields_generator/{job_id}'
        job_data = await asyncio.to_thread(
            FirebaseSystem.instance().get_data,
            job_path
        )
        
        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return
        
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})
        
        input_data = {
            'description': inputs_data.get('description', ''),
            'aggregate_drafts': inputs_data.get('aggregateDrafts', []),
            'all_ddl_fields': inputs_data.get('allDdlFields', []),
            'generator_key': inputs_data.get('generatorKey', 'default')
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        generator = DDLFieldsGenerator()
        result = generator.generate(input_data)
        
        # ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥
        output = {
            'inference': result.get('inference', ''),
            'aggregateFieldAssignments': result.get('result', {}).get('aggregateFieldAssignments', []),
            'isCompleted': True,
            'progress': 100,
            'logs': [{'timestamp': result.get('timestamp', ''), 'level': 'info', 'message': 'DDL fields assigned successfully'}]
        }
        
        output_path = f'{job_path}/state/outputs'
        sanitized_output = FirebaseSystem.instance().sanitize_data_for_firebase(output)
        await asyncio.to_thread(
            FirebaseSystem.instance().set_data,
            output_path,
            sanitized_output
        )
        
        # ìš”ì²­ Job ì œê±°
        req_path = f'requestedJobs/ddl_fields_generator/{job_id}'
        await asyncio.to_thread(
            FirebaseSystem.instance().delete_data,
            req_path
        )
        
        LoggingUtil.info("main", f"ğŸ‰ DDL Fields í• ë‹¹ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
    except Exception as e:
        LoggingUtil.exception("main", f"DDL Fields í• ë‹¹ ì˜¤ë¥˜: {job_id}", e)
        
        try:
            error_output = {
                'isFailed': True,
                'isCompleted': True,
                'progress': 100,
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/ddl_fields_generator/{job_id}/state/outputs'
            FirebaseSystem.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    
    finally:
        complete_job_func()


async def process_traceability_job(job_id: str, complete_job_func: callable):
    """Traceability Addition Job ì²˜ë¦¬"""
    try:
        LoggingUtil.info("main", f"ğŸš€ Traceability ì¶”ê°€ ì‹œì‘: {job_id}")

        job_path = f'jobs/traceability_generator/{job_id}'
        job_data = await asyncio.to_thread(
            FirebaseSystem.instance().get_data,
            job_path
        )

        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return

        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})

        input_data = {
            'generatedDraftOptions': inputs_data.get('generatedDraftOptions', []),
            'boundedContextName': inputs_data.get('boundedContextName', ''),
            'description': inputs_data.get('description', ''),
            'functionalRequirements': inputs_data.get('functionalRequirements', ''),
            'traceMap': inputs_data.get('traceMap', {}),
        }

        generator = TraceabilityGenerator()
        result = generator.generate(input_data)

        output = {
            'inference': result.get('inference', ''),
            'draftTraceMap': result.get('draftTraceMap', {}),
            'isCompleted': True,
            'progress': 100,
            'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'info', 'message': 'Traceability mapping completed'}]
        }

        output_path = f'{job_path}/state/outputs'
        sanitized_output = FirebaseSystem.instance().sanitize_data_for_firebase(output)
        await asyncio.to_thread(
            FirebaseSystem.instance().set_data,
            output_path,
            sanitized_output
        )

        req_path = f'requestedJobs/traceability_generator/{job_id}'
        await asyncio.to_thread(
            FirebaseSystem.instance().delete_data,
            req_path
        )

        LoggingUtil.info("main", f"ğŸ‰ Traceability ì¶”ê°€ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    except Exception as e:
        LoggingUtil.exception("main", f"Traceability ì¶”ê°€ ì˜¤ë¥˜: {job_id}", e)
        try:
            error_output = {
                'isFailed': True,
                'isCompleted': True,
                'progress': 100,
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/traceability_generator/{job_id}/state/outputs'
            FirebaseSystem.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    finally:
        complete_job_func()


async def process_ddl_extractor_job(job_id: str, complete_job_func: callable):
    """DDL Extractor Job ì²˜ë¦¬"""
    try:
        LoggingUtil.info("main", f"ğŸš€ DDL í•„ë“œ ì¶”ì¶œ ì‹œì‘: {job_id}")

        job_path = f'jobs/ddl_extractor/{job_id}'
        job_data = await asyncio.to_thread(
            FirebaseSystem.instance().get_data,
            job_path
        )

        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return

        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})

        input_data = {
            'ddlRequirements': inputs_data.get('ddlRequirements', []),
            'boundedContextName': inputs_data.get('boundedContextName', ''),
        }

        generator = DDLExtractor()
        result = generator.generate(input_data)

        output = {
            'inference': result.get('inference', ''),
            'ddlFieldRefs': result.get('ddlFieldRefs', []),
            'isCompleted': True,
            'progress': 100,
            'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'info', 'message': 'DDL extraction completed'}]
        }

        output_path = f'{job_path}/state/outputs'
        sanitized_output = FirebaseSystem.instance().sanitize_data_for_firebase(output)
        await asyncio.to_thread(
            FirebaseSystem.instance().set_data,
            output_path,
            sanitized_output
        )

        req_path = f'requestedJobs/ddl_extractor/{job_id}'
        await asyncio.to_thread(
            FirebaseSystem.instance().delete_data,
            req_path
        )

        LoggingUtil.info("main", f"ğŸ‰ DDL í•„ë“œ ì¶”ì¶œ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    except Exception as e:
        LoggingUtil.exception("main", f"DDL ì¶”ì¶œ ì˜¤ë¥˜: {job_id}", e)
        try:
            error_output = {
                'isFailed': True,
                'isCompleted': True,
                'progress': 100,
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/ddl_extractor/{job_id}/state/outputs'
            FirebaseSystem.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    finally:
        complete_job_func()


async def process_requirements_validator_job(job_id: str, complete_job_func: callable):
    """Requirements Validator Job ì²˜ë¦¬"""
    try:
        LoggingUtil.info("main", f"ğŸš€ ìš”êµ¬ì‚¬í•­ ê²€ì¦ ì‹œì‘: {job_id}")

        job_path = f'jobs/requirements_validator/{job_id}'
        job_data = await asyncio.to_thread(
            FirebaseSystem.instance().get_data,
            job_path
        )

        if not job_data:
            LoggingUtil.error("main", f"Job ë°ì´í„° ì—†ìŒ: {job_id}")
            return

        state = job_data.get('state', {})
        inputs_data = state.get('inputs', {})

        input_data = {
            'requirements': inputs_data.get('requirements', {}),
            'previousChunkSummary': inputs_data.get('previousChunkSummary', {}),
            'currentChunkStartLine': inputs_data.get('currentChunkStartLine', 1),
        }

        generator = RequirementsValidator()
        result = generator.generate(input_data)

        output_path = f'{job_path}/state/outputs'
        firebase = FirebaseSystem.instance()

        content = result.get('content', {}) or {}
        final_length = 0
        try:
            final_length = len(json.dumps(content, ensure_ascii=False))
        except Exception:
            final_length = 0

        intermediate_lengths = _compute_intermediate_lengths(final_length, steps=3)

        for idx, length in enumerate(intermediate_lengths):
            progress_value = max(1, min(95, int(((idx + 1) / (len(intermediate_lengths) + 1)) * 100)))
            update_payload = {
                'currentGeneratedLength': length,
                'progress': progress_value,
                'isCompleted': False
            }
            await firebase.update_data_async(
                output_path,
                firebase.sanitize_data_for_firebase(update_payload)
            )
            await asyncio.sleep(1)

        output = {
            'type': result.get('type', 'ANALYSIS_RESULT'),
            'content': result.get('content', {}),
            'isCompleted': True,
            'progress': 100,
            'currentGeneratedLength': final_length,
            'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'info', 'message': 'Requirements validation completed'}]
        }

        sanitized_output = FirebaseSystem.instance().sanitize_data_for_firebase(output)
        await asyncio.to_thread(
            FirebaseSystem.instance().set_data,
            output_path,
            sanitized_output
        )

        req_path = f'requestedJobs/requirements_validator/{job_id}'
        await asyncio.to_thread(
            FirebaseSystem.instance().delete_data,
            req_path
        )

        LoggingUtil.info("main", f"ğŸ‰ ìš”êµ¬ì‚¬í•­ ê²€ì¦ ì™„ë£Œ: {job_id}")
        LoggingUtil.info("main", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    except Exception as e:
        LoggingUtil.exception("main", f"ìš”êµ¬ì‚¬í•­ ê²€ì¦ ì˜¤ë¥˜: {job_id}", e)
        try:
            error_output = {
                'isFailed': True,
                'isCompleted': True,
                'progress': 100,
                'logs': [{'timestamp': datetime.now().isoformat(), 'level': 'error', 'message': str(e)}]
            }
            output_path = f'jobs/requirements_validator/{job_id}/state/outputs'
            FirebaseSystem.instance().set_data(output_path, error_output)
        except Exception as save_error:
            LoggingUtil.exception("main", f"ì‹¤íŒ¨ ì €ì¥ ì˜¤ë¥˜: {job_id}", save_error)
    finally:
        complete_job_func()

async def process_job_async(job_id: str, complete_job_func: callable):
    """ë¹„ë™ê¸° Job ì²˜ë¦¬ í•¨ìˆ˜ (Job ID prefixë¡œ ë¼ìš°íŒ…)"""
    
    try:
        LoggingUtil.debug("main", f"Job ì‹œì‘: {job_id}")
        if not JobUtil.is_valid_job_id(job_id):
            LoggingUtil.warning("main", f"Job ì²˜ë¦¬ ì˜¤ë¥˜: {job_id}, ìœ íš¨í•˜ì§€ ì•ŠìŒ")
            return
        
        # Job íƒ€ì…ë³„ ë¼ìš°íŒ… (ê° í•¨ìˆ˜ì—ì„œ finally ë¸”ë¡ìœ¼ë¡œ complete_job_func í˜¸ì¶œ)
        if job_id.startswith("usgen-"):
            await process_user_story_job(job_id, complete_job_func)
        elif job_id.startswith("summ-"):
            await process_summarizer_job(job_id, complete_job_func)
        elif job_id.startswith("bcgen-"):
            await process_bounded_context_job(job_id, complete_job_func)
        elif job_id.startswith("cmrext-"):
            await process_command_readmodel_job(job_id, complete_job_func)
        elif job_id.startswith("smapgen-"):
            await process_sitemap_job(job_id, complete_job_func)
        elif job_id.startswith("reqmap-"):
            await process_requirements_mapping_job(job_id, complete_job_func)
        elif job_id.startswith("aggr-draft-"):
            await process_aggregate_draft_job(job_id, complete_job_func)
        elif job_id.startswith("preview-fields-"):
            await process_preview_fields_job(job_id, complete_job_func)
        elif job_id.startswith("ddl-fields-"):
            await process_ddl_fields_job(job_id, complete_job_func)
        elif job_id.startswith("trace-add-"):
            await process_traceability_job(job_id, complete_job_func)
        elif job_id.startswith("ddl-extract-"):
            await process_ddl_extractor_job(job_id, complete_job_func)
        elif job_id.startswith("req-valid-"):
            await process_requirements_validator_job(job_id, complete_job_func)
        else:
            LoggingUtil.warning("main", f"ì§€ì›í•˜ì§€ ì•ŠëŠ” Job íƒ€ì…: {job_id}")
            
    except asyncio.CancelledError:
        LoggingUtil.debug("main", f"Job {job_id} ì·¨ì†Œë¨")
        return
        
    except Exception as e:
        LoggingUtil.exception("main", f"Job ì²˜ë¦¬ ì˜¤ë¥˜: {job_id}", e)

if __name__ == "__main__":
    asyncio.run(main())