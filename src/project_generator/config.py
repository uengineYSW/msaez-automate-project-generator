import os

class Config:
    @staticmethod
    def get_requested_job_root_path() -> str:
        return f"requestedJobs/{Config.get_namespace()}"
            
    @staticmethod
    def get_requested_job_path(job_id: str) -> str:
        return f"{Config.get_requested_job_root_path()}/{job_id}"


    @staticmethod
    def get_job_root_path() -> str:
        return f"jobs/{Config.get_namespace()}"

    @staticmethod
    def get_job_path(job_id: str) -> str:
        return f"{Config.get_job_root_path()}/{job_id}"


    @staticmethod
    def get_job_state_root_path() -> str:
        return f"jobStates/{Config.get_namespace()}"
    
    @staticmethod
    def get_job_state_path(job_id: str) -> str:
        return f"{Config.get_job_state_root_path()}/{job_id}"


    @staticmethod
    def get_namespace() -> str:
        return os.getenv('NAMESPACE')

    @staticmethod
    def get_pod_id() -> str:
        return os.getenv('POD_ID')


    @staticmethod
    def is_local_run() -> bool:
        return os.getenv('IS_LOCAL_RUN') == 'true'
    

    @staticmethod
    def autoscaler_namespace() -> str:
        return os.getenv('AUTO_SCALE_NAMESPACE', 'default')
    
    @staticmethod
    def autoscaler_deployment_name() -> str:
        return os.getenv('AUTO_SCALE_DEPLOYMENT_NAME', 'project-generator')
    
    @staticmethod
    def autoscaler_service_name() -> str:
        return os.getenv('AUTO_SCALE_SERVICE_NAME', 'project-generator-service')

    @staticmethod
    def autoscaler_min_replicas() -> int:
        return int(os.getenv('AUTO_SCALE_MIN_REPLICAS', '1'))

    @staticmethod
    def autoscaler_max_replicas() -> int:
        return int(os.getenv('AUTO_SCALE_MAX_REPLICAS', '3'))
    
    @staticmethod
    def autoscaler_target_jobs_per_pod() -> int:
        return int(os.getenv('AUTO_SCALE_TARGET_JOBS_PER_POD', '1'))
    
    @staticmethod
    def max_concurrent_jobs() -> int:
        """단일 인스턴스에서 동시에 처리할 수 있는 최대 작업 수"""
        return int(os.getenv('MAX_CONCURRENT_JOBS', '3'))
    
    @staticmethod
    def job_polling_interval() -> float:
        """작업 모니터링 폴링 간격 (초)"""
        return float(os.getenv('JOB_POLLING_INTERVAL', '2.0'))

    @staticmethod
    def get_log_level() -> str:
        """환경별 로그 레벨 반환 (DEBUG, INFO, WARNING, ERROR)"""
        if Config.is_local_run():
            return os.getenv('LOG_LEVEL', 'DEBUG')  # 로컬에서는 DEBUG 기본
        else:
            return os.getenv('LOG_LEVEL', 'INFO')   # Pod에서는 INFO 기본
    

    @staticmethod
    def get_ai_model() -> str:
        return os.getenv('AI_MODEL')
    
    @staticmethod
    def get_ai_model_vendor() -> str:
        return Config.get_ai_model().split(':')[0]
    
    @staticmethod
    def get_ai_model_name() -> str:
        return Config.get_ai_model().split(':')[1]
    
    @staticmethod
    def get_ai_model_max_input_limit() -> int:
        return int(os.getenv('AI_MODEL_MAX_INPUT_LIMIT'))
    
    @staticmethod
    def get_ai_model_max_batch_size() -> int:
        return int(os.getenv('AI_MODEL_MAX_BATCH_SIZE'))
    

    @staticmethod
    def get_ai_model_light() -> str:
        return os.getenv('AI_MODEL_LIGHT')
    
    @staticmethod
    def get_ai_model_light_vendor() -> str:
        return Config.get_ai_model_light().split(':')[0]
    
    @staticmethod
    def get_ai_model_light_name() -> str:
        return Config.get_ai_model_light().split(':')[1]

    @staticmethod
    def get_ai_model_light_max_input_limit() -> int:
        return int(os.getenv('AI_MODEL_LIGHT_MAX_INPUT_LIMIT'))
    
    @staticmethod
    def get_ai_model_light_max_batch_size() -> int:
        return int(os.getenv('AI_MODEL_LIGHT_MAX_BATCH_SIZE'))
    
    # Knowledge Base 경로 설정
    from pathlib import Path
    # __file__ = backend-generators/src/project_generator/config.py
    # parent.parent.parent = backend-generators/
    _project_root = Path(__file__).parent.parent.parent
    
    # 공유 스토리지 경로 (Kubernetes PersistentVolume 등)
    # 설정되지 않으면 기본 경로 사용
    _shared_storage = os.getenv('SHARED_STORAGE_PATH')
    if _shared_storage:
        _base_path = Path(_shared_storage)
    else:
        _base_path = _project_root
    
    # RAG 설정
    # VECTORSTORE_PATH는 SHARED_STORAGE_PATH가 있으면 그 경로를 사용, 없으면 기본 경로 사용
    _vectorstore_env = os.getenv('VECTORSTORE_PATH')
    if _vectorstore_env:
        VECTORSTORE_PATH = _vectorstore_env
    elif _shared_storage:
        # 배포 환경: PVC 경로 사용
        VECTORSTORE_PATH = str(_base_path / 'knowledge_base' / 'vectorstore')
    else:
        # 로컬 환경: 프로젝트 루트 기준 상대 경로
        VECTORSTORE_PATH = str(_project_root / 'knowledge_base' / 'vectorstore')
    
    EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')
    
    # 표준 변환 시스템 설정
    # 임계값 0.8: 행별 청킹 + 핵심 키워드만 포함으로 유사도 향상
    # 필수 매핑은 전체 표준 원본을 직접 읽어서 global mapping 구성 (유사도 검색과 무관)
    # 각 행이 독립적으로 임베딩되고 검색 키워드(한글명, 영문명)만 포함
    # OpenAI 임베딩: "주문" vs "주문 Order" → 90%+ 예상
    STANDARD_TRANSFORMER_SCORE_THRESHOLD = float(os.getenv('STANDARD_TRANSFORMER_SCORE_THRESHOLD', '0.8'))
    
    COMPANY_STANDARDS_PATH = _base_path / 'knowledge_base' / 'company_standards'
    
    # LLM 설정 (OpenAI 실제 모델)
    DEFAULT_LLM_MODEL = os.getenv('DEFAULT_LLM_MODEL', 'gpt-4o')  # OpenAI 최신 모델
    DEFAULT_LLM_TEMPERATURE = float(os.getenv('DEFAULT_LLM_TEMPERATURE', '0.2'))  # Frontend와 동일