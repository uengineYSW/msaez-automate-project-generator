"""
RAG Retriever - ê³µí†µ RAG ê²€ìƒ‰ ëª¨ë“ˆ
ëª¨ë“  ì›Œí¬í”Œë¡œìš°ì—ì„œ ìž¬ì‚¬ìš© ê°€ëŠ¥í•œ RAG ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
"""
from typing import List, Dict, Optional
from pathlib import Path
import json
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    # ìƒˆë¡œìš´ íŒ¨í‚¤ì§€ë¡œ import ì‹œë„ (deprecation warning í•´ê²°)
    try:
        from langchain_chroma import Chroma
        from langchain_openai import OpenAIEmbeddings
    except ImportError:
        # fallback: ê¸°ì¡´ íŒ¨í‚¤ì§€ ì‚¬ìš©
        from langchain.vectorstores import Chroma
        from langchain.embeddings import OpenAIEmbeddings
    # DocumentëŠ” langchain_coreì—ì„œ import
    try:
        from langchain_core.documents import Document
    except ImportError:
        from langchain.schema import Document
    HAS_CHROMA = True
except ImportError:
    HAS_CHROMA = False
    print("âš ï¸  chromadb not installed. RAG features will be disabled.")

from src.project_generator.config import Config

# ê¸°ë³¸ ìœ ì‚¬ë„ ìž„ê³„ê°’ (0.0~1.0)
# ìžì—°ì–´ + ë„ë©”ì¸ í…ìŠ¤íŠ¸ì—ì„œ ì½”ì‚¬ì¸ ê¸°ë°˜ìœ¼ë¡œ 0.3~0.4 ì´í•˜ë¥¼ ì»·ìœ¼ë¡œ ì“°ëŠ” ê²½ìš°ê°€ ë§ŽìŒ
# 0.7ì€ ê±°ì˜ "ê±°ì˜ ê°™ì€ ë¬¸ìž¥ ìˆ˜ì¤€"ì´ë¼ ë„ˆë¬´ ë†’ìŒ
DEFAULT_SIM_THRESHOLD = 0.3


class RAGRetriever:
    """
    RAG ê²€ìƒ‰ ê³µí†µ í´ëž˜ìŠ¤
    
    Knowledge Baseì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ AI í”„ë¡¬í”„íŠ¸ì— ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€
    """
    
    def __init__(self, vectorstore_path: Optional[str] = None):
        """
        Args:
            vectorstore_path: Vector Store ê²½ë¡œ (Noneì´ë©´ Configì—ì„œ ê°€ì ¸ì˜´)
        """
        self.vectorstore_path = vectorstore_path or Config.VECTORSTORE_PATH
        self.vectorstore = None
        self._initialized = False
        
        if HAS_CHROMA:
            self._initialize_vectorstore()
    
    def _initialize_vectorstore(self):
        """Vector Store ì´ˆê¸°í™”"""
        try:
            if Path(self.vectorstore_path).exists():
                self.vectorstore = Chroma(
                    persist_directory=str(self.vectorstore_path),
                    embedding_function=OpenAIEmbeddings(
                        model=Config.EMBEDDING_MODEL
                    )
                )
                self._initialized = True
                print(f"âœ… Vector Store loaded from {self.vectorstore_path}")
            else:
                # Vector Storeê°€ ì—†ìœ¼ë©´ ìƒì„±
                Path(self.vectorstore_path).mkdir(parents=True, exist_ok=True)
                self.vectorstore = Chroma(
                    persist_directory=str(self.vectorstore_path),
                    embedding_function=OpenAIEmbeddings(
                        model=Config.EMBEDDING_MODEL
                    )
                )
                self._initialized = True
                print(f"âœ… Vector Store created at {self.vectorstore_path}")
        except Exception as e:
            print(f"âš ï¸  Failed to initialize Vector Store: {e}")
            print("   RAG features will work with fallback mode.")
    
    def clear_vectorstore(self) -> bool:
        """
        Vector Storeì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ì‚­ì œ (ì»¬ë ‰ì…˜ í´ë¦¬ì–´)
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not self._initialized or not self.vectorstore:
            print("âš ï¸  Vector Store not initialized. Cannot clear.")
            return False
        
        try:
            # ChromaDB ì»¬ë ‰ì…˜ ì‚­ì œ
            self.vectorstore.delete_collection()
            print(f"ðŸ—‘ï¸  Vector Store cleared: {self.vectorstore_path}")
            
            # ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ìœ¼ë¡œ ìž¬ì´ˆê¸°í™”
            self.vectorstore = Chroma(
                persist_directory=str(self.vectorstore_path),
                embedding_function=OpenAIEmbeddings(
                    model=Config.EMBEDDING_MODEL
                )
            )
            self._initialized = True
            print(f"âœ… Vector Store reinitialized at {self.vectorstore_path}")
            return True
        except Exception as e:
            print(f"âš ï¸  Failed to clear Vector Store: {e}")
            return False
    
    def add_documents(self, documents: List[Document], check_duplicates: bool = True) -> bool:
        """
        Vector Storeì— ë¬¸ì„œë¥¼ ë™ì ìœ¼ë¡œ ì¶”ê°€
        
        Args:
            documents: ì¶”ê°€í•  Document ë¦¬ìŠ¤íŠ¸
            check_duplicates: ì¤‘ë³µ ì²´í¬ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not self._initialized or not self.vectorstore:
            print("âš ï¸  Vector Store not initialized. Cannot add documents.")
            return False
        
        try:
            if check_duplicates:
                # ì¤‘ë³µ ì²´í¬: source + sheet + has_draft_context ì¡°í•©ìœ¼ë¡œ ê³ ìœ  í‚¤ ìƒì„±
                # ChromaDBì—ì„œ ê¸°ì¡´ ë¬¸ì„œ í™•ì¸
                documents_to_add = []
                skipped_count = 0
                
                for doc in documents:
                    metadata = doc.metadata
                    source = metadata.get("source", "")
                    sheet = metadata.get("sheet", "")
                    has_draft_context = metadata.get("has_draft_context", False)
                    
                    # ê³ ìœ  ID ìƒì„±: source + sheet + has_draft_context
                    # ê°™ì€ source+sheetì— ëŒ€í•´ ì´ˆì•ˆ ì •ë³´ í¬í•¨ ë²„ì „ì€ ë³„ë„ ë¬¸ì„œë¡œ ì·¨ê¸‰
                    unique_id = f"{source}::{sheet}::{has_draft_context}"
                    metadata["_unique_id"] = unique_id
                    
                    # ê¸°ì¡´ ë¬¸ì„œ í™•ì¸: ChromaDBì˜ get ë©”ì„œë“œë¡œ ë©”íƒ€ë°ì´í„° í•„í„°ë§
                    try:
                        # ChromaDBì—ì„œ ê°™ì€ source, sheet, has_draft_contextë¥¼ ê°€ì§„ ë¬¸ì„œ ê²€ìƒ‰
                        existing_docs = self.vectorstore.get(
                            where={
                                "source": source,
                                "sheet": sheet,
                                "has_draft_context": has_draft_context
                            }
                        )
                        
                        if existing_docs and len(existing_docs.get("ids", [])) > 0:
                            # ì´ë¯¸ ì¡´ìž¬í•˜ëŠ” ë¬¸ì„œëŠ” ìŠ¤í‚µ
                            skipped_count += 1
                            continue
                    except Exception as e:
                        # í•„í„° ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ ì¶”ê°€ (ì•ˆì „í•œ ë°©ì‹)
                        # ChromaDB ë²„ì „ì— ë”°ë¼ get ë©”ì„œë“œê°€ ë‹¤ë¥¼ ìˆ˜ ìžˆìŒ
                        pass
                    
                    documents_to_add.append(doc)
                
                if documents_to_add:
                    self.vectorstore.add_documents(documents_to_add)
                    if skipped_count > 0:
                        print(f"âœ… Added {len(documents_to_add)}/{len(documents)} documents to Vector Store ({skipped_count} duplicates skipped)")
                    else:
                        print(f"âœ… Added {len(documents_to_add)} documents to Vector Store")
                else:
                    print(f"âš ï¸  All {len(documents)} documents are duplicates, skipping...")
            else:
                self.vectorstore.add_documents(documents)
                print(f"âœ… Added {len(documents)} documents to Vector Store")
            
            return True
        except Exception as e:
            print(f"âš ï¸  Failed to add documents to Vector Store: {e}")
            return False
    
    def search_ddd_patterns(self, query: str, k: int = 10) -> List[Dict]:
        """
        DDD íŒ¨í„´ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self._initialized or not self.vectorstore:
            return self._fallback_search_ddd_patterns(query, k)
        
        try:
            results = self.vectorstore.similarity_search(
                query,
                k=k,
                filter={"type": "ddd_pattern"}
            )
            return [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in results
            ]
        except Exception as e:
            print(f"âš ï¸  DDD pattern search failed: {e}")
            return self._fallback_search_ddd_patterns(query, k)
    
    def search_project_templates(self, query: str, k: int = 5) -> List[Dict]:
        """
        ìœ ì‚¬ í”„ë¡œì íŠ¸ ì‚¬ë¡€ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self._initialized or not self.vectorstore:
            return self._fallback_search_project_templates(query, k)
        
        try:
            results = self.vectorstore.similarity_search(
                query,
                k=k,
                filter={"type": "project_template"}
            )
            return [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in results
            ]
        except Exception as e:
            print(f"âš ï¸  Project template search failed: {e}")
            return self._fallback_search_project_templates(query, k)
    
    def search_vocabulary(self, query: str, k: int = 20) -> List[Dict]:
        """
        ë„ë©”ì¸ ìš©ì–´ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self._initialized or not self.vectorstore:
            return self._fallback_search_vocabulary(query, k)
        
        try:
            results = self.vectorstore.similarity_search(
                query,
                k=k,
                filter={"type": "vocabulary"}
            )
            return [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in results
            ]
        except Exception as e:
            print(f"âš ï¸  Vocabulary search failed: {e}")
            return self._fallback_search_vocabulary(query, k)
    
    def search_ui_patterns(self, query: str, k: int = 10) -> List[Dict]:
        """
        UI íŒ¨í„´ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self._initialized or not self.vectorstore:
            return self._fallback_search_ui_patterns(query, k)
        
        try:
            results = self.vectorstore.similarity_search(
                query,
                k=k,
                filter={"type": "ui_pattern"}
            )
            return [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in results
            ]
        except Exception as e:
            print(f"âš ï¸  UI pattern search failed: {e}")
            return self._fallback_search_ui_patterns(query, k)
    
    # Fallback methods (Vector Storeê°€ ì—†ì„ ë•Œ JSON íŒŒì¼ì—ì„œ ì§ì ‘ ê²€ìƒ‰)
    
    def _fallback_search_ddd_patterns(self, query: str, k: int) -> List[Dict]:
        """DDD íŒ¨í„´ Fallback ê²€ìƒ‰ (JSON íŒŒì¼ì—ì„œ ì§ì ‘)"""
        try:
            pattern_files = list(Config.DOMAIN_PATTERNS_PATH.glob("*.json"))
            results = []
            
            for file_path in pattern_files:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    results.append({
                        "content": json.dumps(data, ensure_ascii=False),
                        "metadata": {
                            "source": str(file_path),
                            "type": "ddd_pattern"
                        }
                    })
            
            return results[:k]
        except Exception as e:
            print(f"âš ï¸  Fallback DDD search failed: {e}")
            return []
    
    def _fallback_search_project_templates(self, query: str, k: int) -> List[Dict]:
        """í”„ë¡œì íŠ¸ í…œí”Œë¦¿ Fallback ê²€ìƒ‰"""
        try:
            template_files = list(Config.PROJECT_TEMPLATES_PATH.glob("*.json"))
            results = []
            
            for file_path in template_files:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    results.append({
                        "content": json.dumps(data, ensure_ascii=False),
                        "metadata": {
                            "source": str(file_path),
                            "type": "project_template"
                        }
                    })
            
            return results[:k]
        except Exception as e:
            print(f"âš ï¸  Fallback project search failed: {e}")
            return []
    
    def _fallback_search_vocabulary(self, query: str, k: int) -> List[Dict]:
        """ìš©ì–´ Fallback ê²€ìƒ‰"""
        try:
            vocab_files = list(Config.VOCABULARY_PATH.glob("*.json"))
            results = []
            
            for file_path in vocab_files:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    results.append({
                        "content": json.dumps(data, ensure_ascii=False),
                        "metadata": {
                            "source": str(file_path),
                            "type": "vocabulary"
                        }
                    })
            
            return results[:k]
        except Exception as e:
            print(f"âš ï¸  Fallback vocabulary search failed: {e}")
            return []
    
    def _fallback_search_ui_patterns(self, query: str, k: int) -> List[Dict]:
        """UI íŒ¨í„´ Fallback ê²€ìƒ‰"""
        try:
            ui_files = list(Config.UI_PATTERNS_PATH.glob("*.json"))
            results = []
            
            for file_path in ui_files:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    results.append({
                        "content": json.dumps(data, ensure_ascii=False),
                        "metadata": {
                            "source": str(file_path),
                            "type": "ui_pattern"
                        }
                    })
            
            return results[:k]
        except Exception as e:
            print(f"âš ï¸  Fallback UI search failed: {e}")
            return []
    
    def search_company_standards(self, query: str, k: int = 5, score_threshold: float = DEFAULT_SIM_THRESHOLD) -> List[Dict]:
        """
        íšŒì‚¬ í‘œì¤€ ê²€ìƒ‰ (ë°ì´í„°ë² ì´ìŠ¤, API, ìš©ì–´ ë“± ëª¨ë“  í‘œì¤€)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            score_threshold: ìœ ì‚¬ë„ ì ìˆ˜ ìž„ê³„ê°’ (0.0~1.0, ê¸°ë³¸ê°’ 0.3)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ í¬í•¨)
        """
        if not self._initialized or not self.vectorstore:
            return self._fallback_search_company_standards(query, k)
        
        try:
            # similarity_search_with_score ì‚¬ìš©í•˜ì—¬ ì ìˆ˜ í¬í•¨
            # í•„í„° ì‚¬ìš© ì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ try-exceptë¡œ ê°ì‹¸ê¸°
            try:
                results_with_scores = self.vectorstore.similarity_search_with_score(
                    query,
                    k=k * 3,  # í•„í„°ë§ì„ ìœ„í•´ ë” ë§Žì´ ê°€ì ¸ì˜´
                    filter={"type": {"$in": ["database_standard", "api_standard", "terminology_standard"]}}
                )
            except Exception as filter_error:
                # í•„í„° ì˜¤ë¥˜ ì‹œ í•„í„° ì—†ì´ ê²€ìƒ‰ í›„ ìˆ˜ë™ í•„í„°ë§
                # ChromaDB ë™ì‹œì„± ë¬¸ì œ("Failed to get segments")ëŠ” ì¼ì‹œì ì´ë¯€ë¡œ ì¡°ìš©ížˆ ì²˜ë¦¬
                error_msg = str(filter_error)
                if "Failed to get segments" not in error_msg:
                    print(f"âš ï¸  Search failed with filter: {filter_error}")
                all_results = self.vectorstore.similarity_search_with_score(
                    query,
                    k=k * 5  # ë” ë§Žì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
                )
                # ìˆ˜ë™ í•„í„°ë§
                results_with_scores = []
                for doc, score in all_results:
                    doc_type = doc.metadata.get("type", "")
                    if doc_type in ["database_standard", "api_standard", "terminology_standard"]:
                        results_with_scores.append((doc, score))
            # ì ìˆ˜ í•„í„°ë§
            # ChromaDBì˜ similarity_search_with_scoreëŠ” ê±°ë¦¬(distance)ë¥¼ ë°˜í™˜
            # ChromaDBëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì½”ì‚¬ì¸ ê±°ë¦¬(cosine distance)ë¥¼ ì‚¬ìš©
            # 
            # ê±°ë¦¬ ë²”ìœ„ëŠ” ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìžˆìŒ:
            # - ì •ê·œí™”ëœ ë²¡í„°: 0~1 ë²”ìœ„ (distance = 1 - cosine_similarity)
            # - ì¼ë°˜ ì½”ì‚¬ì¸ ê±°ë¦¬: 0~2 ë²”ìœ„ (distance = 1 - cos(Î¸), cos(Î¸) = -1~1)
            # 
            # ì‹¤ì œ ê±°ë¦¬ ê°’ì˜ ë²”ìœ„ë¥¼ ë™ì ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ë³€í™˜
            filtered_results = []
            all_scores = []  # ë””ë²„ê¹…ìš©
            
            # ë¨¼ì € ëª¨ë“  ê±°ë¦¬ ê°’ì„ ìˆ˜ì§‘í•˜ì—¬ ë²”ìœ„ í™•ì¸
            distances = [abs(float(score_value)) for _, score_value in results_with_scores]
            if distances:
                dist_min, dist_max = min(distances), max(distances)
                # ê±°ë¦¬ ë²”ìœ„ì— ë”°ë¼ ë³€í™˜ ë°©ì‹ ê²°ì •
                # ëŒ€ë¶€ë¶„ì˜ ê±°ë¦¬ê°€ 1.0ì„ ë„˜ìœ¼ë©´ 0~2 ë²”ìœ„ë¡œ ê°€ì •, ì•„ë‹ˆë©´ 0~1 ë²”ìœ„ë¡œ ê°€ì •
                if dist_max > 1.0:
                    # 0~2 ë²”ìœ„: similarity = 1 - (distance / 2)
                    distance_range = 2.0
                else:
                    # 0~1 ë²”ìœ„: similarity = 1 - distance
                    distance_range = 1.0
            else:
                # ê¸°ë³¸ê°’: 0~2 ë²”ìœ„ë¡œ ê°€ì • (ì•ˆì „í•œ ì„ íƒ)
                distance_range = 2.0
            
            for doc, score_value in results_with_scores:
                # ì›ë³¸ ê°’ì„ í™•ì¸
                raw_score = float(score_value)
                distance = abs(raw_score)
                
                # ê±°ë¦¬ ë²”ìœ„ì— ë”°ë¼ ìœ ì‚¬ë„ ë³€í™˜
                if distance_range == 2.0:
                    # 0~2 ë²”ìœ„: similarity = 1 - (distance / 2)
                    similarity = max(0.0, 1.0 - (distance / 2.0))
                else:
                    # 0~1 ë²”ìœ„: similarity = 1 - distance
                    similarity = max(0.0, 1.0 - distance)
                
                all_scores.append((raw_score, distance, similarity))
                
                # ì ìˆ˜ í•„í„°ë§: ìœ ì‚¬ë„ê°€ ìž„ê³„ê°’ ì´ìƒì¸ ê²ƒë§Œ í¬í•¨
                if similarity >= score_threshold:
                    filtered_results.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "score": similarity,
                        "distance": distance,
                        "raw_score": raw_score  # ì›ë³¸ ê°’ë„ ì €ìž¥
                    })
                # ìƒìœ„ kê°œë¥¼ ê°€ì ¸ì˜¤ë˜, ìž„ê³„ê°’ ì´ìƒì¸ ê²ƒë§Œ í¬í•¨
                # kê°œë¥¼ ì±„ìš°ì§€ ëª»í•´ë„ ìž„ê³„ê°’ ì´ìƒì¸ ê²ƒë“¤ì€ ëª¨ë‘ í¬í•¨
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ë¶€í„°)
            filtered_results.sort(key=lambda x: x["score"], reverse=True)
            
            # ë””ë²„ê¹…: ì „ì²´ ì ìˆ˜ ë¶„í¬ ì¶œë ¥ (ì²˜ìŒ 10ê°œ)
            if all_scores:
                print(f"  [DEBUG] ê²€ìƒ‰ ê²°ê³¼ ì ìˆ˜ ë¶„í¬ (ì²˜ìŒ 10ê°œ, ì´ {len(all_scores)}ê°œ):")
                for i, (raw, dist, sim) in enumerate(all_scores[:10]):
                    print(f"    [{i+1}] ì›ë³¸ê°’: {raw:.6f}, ê±°ë¦¬: {dist:.6f}, ìœ ì‚¬ë„: {sim:.6f}")
                print(f"  [DEBUG] í•„í„°ë§ í›„ ê²°ê³¼: {len(filtered_results)}/{len(all_scores)}")
                if len(all_scores) > 0:
                    raw_min, raw_max = min(s[0] for s in all_scores), max(s[0] for s in all_scores)
                    dist_min, dist_max = min(s[1] for s in all_scores), max(s[1] for s in all_scores)
                    sim_min, sim_max = min(s[2] for s in all_scores), max(s[2] for s in all_scores)
                    print(f"  [DEBUG] ì›ë³¸ê°’ ë²”ìœ„: {raw_min:.6f} ~ {raw_max:.6f}")
                    print(f"  [DEBUG] ê±°ë¦¬ ë²”ìœ„: {dist_min:.6f} ~ {dist_max:.6f}")
                    print(f"  [DEBUG] ìœ ì‚¬ë„ ë²”ìœ„: {sim_min:.6f} ~ {sim_max:.6f}")
                    
                    # í•„í„°ë§ ì „í›„ ë¹„êµ
                    print(f"  [DEBUG] í•„í„°ë§ ì „ ê²°ê³¼: {len(results_with_scores)}ê°œ")
                    print(f"  [DEBUG] í•„í„°ë§ í›„ ê²°ê³¼: {len(filtered_results)}ê°œ (ìž„ê³„ê°’: {score_threshold:.3f} ì´ìƒ)")
                    
                    # ë§Œì•½ ìœ ì‚¬ë„ê°€ ëª¨ë‘ 0ì´ë©´ ê²½ê³  ë° ê±°ë¦¬ ë²”ìœ„ ë¶„ì„
                    if sim_max == 0.0:
                        print(f"  [WARNING] âš ï¸  ëª¨ë“  ìœ ì‚¬ë„ê°€ 0ìž…ë‹ˆë‹¤! ê±°ë¦¬ ë²”ìœ„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                        print(f"  [WARNING] ê±°ë¦¬ ë²”ìœ„: {dist_min:.6f} ~ {dist_max:.6f}")
                        # ì‹¤ì œ ë³€í™˜ ë¡œì§ê³¼ ì¼ì¹˜í•˜ë„ë¡ distance_range ì‚¬ìš©
                        if distance_range == 2.0:
                            sim_est = max(0.0, 1.0 - (dist_max / 2.0))
                        else:
                            sim_est = max(0.0, 1.0 - dist_max)
                        print(f"  [WARNING] ê±°ë¦¬ {dist_max:.3f}ëŠ” ìœ ì‚¬ë„ {sim_est:.3f}ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.")
                        print(f"  [WARNING] ìž„ê³„ê°’ {score_threshold:.3f}ë³´ë‹¤ ë‚®ì•„ì„œ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ìµœì¢…ì ìœ¼ë¡œ ìƒìœ„ kê°œë§Œ ë°˜í™˜ (ì¼ê´€ì„± ìœ ì§€)
            return filtered_results[:k]
        except Exception as e:
            # similarity_search_with_scoreê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° ê¸°ë³¸ ê²€ìƒ‰ ì‚¬ìš© (ì ìˆ˜ í•„í„°ë§ ì—†ìŒ)
            print(f"âš ï¸  similarity_search_with_score ì‹¤íŒ¨, ê¸°ë³¸ ê²€ìƒ‰ ì‚¬ìš©: {e}")
            try:
                results = self.vectorstore.similarity_search(
                    query,
                    k=k,
                    filter={"type": {"$in": ["database_standard", "api_standard", "terminology_standard"]}}
                )
                return [
                    {
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "score": None  # ì ìˆ˜ ì—†ìŒ (í•„í„°ë§ ì•ˆ í•¨)
                    }
                    for doc in results
                ]
            except Exception as e2:
                # í•„í„°ê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° í•„í„° ì—†ì´ ê²€ìƒ‰
                try:
                    results = self.vectorstore.similarity_search(query, k=k)
                    return [
                        {
                            "content": doc.page_content,
                            "metadata": doc.metadata,
                            "score": None
                        }
                        for doc in results
                        if doc.metadata.get("type") in ["database_standard", "api_standard", "terminology_standard"]
                    ]
                except Exception as e3:
                    print(f"âš ï¸  Company standards search failed: {e3}")
                    return self._fallback_search_company_standards(query, k)
    
    def search_api_standards(self, query: str, k: int = 5, score_threshold: float = DEFAULT_SIM_THRESHOLD) -> List[Dict]:
        """
        API í‘œì¤€ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            score_threshold: ìœ ì‚¬ë„ ì ìˆ˜ ìž„ê³„ê°’ (0.0~1.0, ê¸°ë³¸ê°’ 0.3)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ í¬í•¨)
        """
        if not self._initialized or not self.vectorstore:
            return self._fallback_search_api_standards(query, k)
        
        try:
            # similarity_search_with_score ì‚¬ìš©í•˜ì—¬ ì ìˆ˜ í¬í•¨
            # í•„í„° ì‚¬ìš© ì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ try-exceptë¡œ ê°ì‹¸ê¸°
            try:
                results_with_scores = self.vectorstore.similarity_search_with_score(
                    query,
                    k=k * 3,
                    filter={"type": "api_standard"}
                )
            except Exception as filter_error:
                # í•„í„° ì˜¤ë¥˜ ì‹œ í•„í„° ì—†ì´ ê²€ìƒ‰ í›„ ìˆ˜ë™ í•„í„°ë§
                # ChromaDB ë™ì‹œì„± ë¬¸ì œ("Failed to get segments")ëŠ” ì¼ì‹œì ì´ë¯€ë¡œ ì¡°ìš©ížˆ ì²˜ë¦¬
                error_msg = str(filter_error)
                if "Failed to get segments" not in error_msg:
                    print(f"âš ï¸  Search failed with filter: {filter_error}")
                all_results = self.vectorstore.similarity_search_with_score(
                    query,
                    k=k * 5  # ë” ë§Žì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
                )
                # ìˆ˜ë™ í•„í„°ë§
                results_with_scores = []
                for doc, score in all_results:
                    doc_type = doc.metadata.get("type", "")
                    if doc_type == "api_standard":
                        results_with_scores.append((doc, score))
            # ì ìˆ˜ í•„í„°ë§ (ì½”ì‚¬ì¸ ê±°ë¦¬ ê¸°ë°˜)
            # ê±°ë¦¬ ë²”ìœ„ë¥¼ ë™ì ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ë³€í™˜
            filtered_results = []
            
            # ë¨¼ì € ëª¨ë“  ê±°ë¦¬ ê°’ì„ ìˆ˜ì§‘í•˜ì—¬ ë²”ìœ„ í™•ì¸
            distances = [abs(float(score_value)) for _, score_value in results_with_scores]
            if distances:
                dist_max = max(distances)
                distance_range = 2.0 if dist_max > 1.0 else 1.0
            else:
                distance_range = 2.0  # ê¸°ë³¸ê°’
            
            for doc, score_value in results_with_scores:
                raw_score = float(score_value)
                distance = abs(raw_score)
                
                # ê±°ë¦¬ ë²”ìœ„ì— ë”°ë¼ ìœ ì‚¬ë„ ë³€í™˜
                if distance_range == 2.0:
                    similarity = max(0.0, 1.0 - (distance / 2.0))
                else:
                    similarity = max(0.0, 1.0 - distance)
                
                if similarity >= score_threshold:
                    filtered_results.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "score": similarity,
                        "distance": distance,
                        "raw_score": raw_score  # ì›ë³¸ ê°’ë„ ì €ìž¥
                    })
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ë¶€í„°)
            filtered_results.sort(key=lambda x: x["score"], reverse=True)
            
            # ìµœì¢…ì ìœ¼ë¡œ ìƒìœ„ kê°œë§Œ ë°˜í™˜ (ì¼ê´€ì„± ìœ ì§€)
            return filtered_results[:k]
        except Exception as e:
            # fallback: ê¸°ë³¸ ê²€ìƒ‰ ì‚¬ìš©
            try:
                results = self.vectorstore.similarity_search(
                    query,
                    k=k,
                    filter={"type": "api_standard"}
                )
                return [
                    {
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "score": None
                    }
                    for doc in results
                ]
            except Exception as e2:
                print(f"âš ï¸  API standards search failed: {e2}")
                return self._fallback_search_api_standards(query, k)
    
    def search_terminology_standards(self, query: str, k: int = 5, score_threshold: float = DEFAULT_SIM_THRESHOLD) -> List[Dict]:
        """
        ìš©ì–´ í‘œì¤€ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            score_threshold: ìœ ì‚¬ë„ ì ìˆ˜ ìž„ê³„ê°’ (0.0~1.0, ê¸°ë³¸ê°’ 0.3)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ í¬í•¨)
        """
        if not self._initialized or not self.vectorstore:
            return self._fallback_search_terminology_standards(query, k)
        
        try:
            # similarity_search_with_score ì‚¬ìš©í•˜ì—¬ ì ìˆ˜ í¬í•¨
            # í•„í„° ì‚¬ìš© ì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ try-exceptë¡œ ê°ì‹¸ê¸°
            try:
                results_with_scores = self.vectorstore.similarity_search_with_score(
                    query,
                    k=k * 3,
                    filter={"type": "terminology_standard"}
                )
            except Exception as filter_error:
                # í•„í„° ì˜¤ë¥˜ ì‹œ í•„í„° ì—†ì´ ê²€ìƒ‰ í›„ ìˆ˜ë™ í•„í„°ë§
                # ChromaDB ë™ì‹œì„± ë¬¸ì œ("Failed to get segments")ëŠ” ì¼ì‹œì ì´ë¯€ë¡œ ì¡°ìš©ížˆ ì²˜ë¦¬
                error_msg = str(filter_error)
                if "Failed to get segments" not in error_msg:
                    print(f"âš ï¸  Search failed with filter: {filter_error}")
                all_results = self.vectorstore.similarity_search_with_score(
                    query,
                    k=k * 5  # ë” ë§Žì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
                )
                # ìˆ˜ë™ í•„í„°ë§
                results_with_scores = []
                for doc, score in all_results:
                    doc_type = doc.metadata.get("type", "")
                    if doc_type == "terminology_standard":
                        results_with_scores.append((doc, score))
            # ì ìˆ˜ í•„í„°ë§ (ì½”ì‚¬ì¸ ê±°ë¦¬ ê¸°ë°˜)
            # ê±°ë¦¬ ë²”ìœ„ë¥¼ ë™ì ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ë³€í™˜
            filtered_results = []
            
            # ë¨¼ì € ëª¨ë“  ê±°ë¦¬ ê°’ì„ ìˆ˜ì§‘í•˜ì—¬ ë²”ìœ„ í™•ì¸
            distances = [abs(float(score_value)) for _, score_value in results_with_scores]
            if distances:
                dist_max = max(distances)
                distance_range = 2.0 if dist_max > 1.0 else 1.0
            else:
                distance_range = 2.0  # ê¸°ë³¸ê°’
            
            for doc, score_value in results_with_scores:
                distance = abs(float(score_value))
                
                # ê±°ë¦¬ ë²”ìœ„ì— ë”°ë¼ ìœ ì‚¬ë„ ë³€í™˜
                if distance_range == 2.0:
                    similarity = max(0.0, 1.0 - (distance / 2.0))
                else:
                    similarity = max(0.0, 1.0 - distance)
                if similarity >= score_threshold:
                    filtered_results.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "score": similarity,
                        "distance": distance,
                        "raw_score": float(score_value)  # ì›ë³¸ ê°’ë„ ì €ìž¥
                    })
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ë¶€í„°)
            filtered_results.sort(key=lambda x: x["score"], reverse=True)
            
            # ìµœì¢…ì ìœ¼ë¡œ ìƒìœ„ kê°œë§Œ ë°˜í™˜ (ì¼ê´€ì„± ìœ ì§€)
            return filtered_results[:k]
        except Exception as e:
            # fallback: ê¸°ë³¸ ê²€ìƒ‰ ì‚¬ìš©
            try:
                results = self.vectorstore.similarity_search(
                    query,
                    k=k,
                    filter={"type": "terminology_standard"}
                )
                return [
                    {
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "score": None
                    }
                    for doc in results
                ]
            except Exception as e2:
                print(f"âš ï¸  Terminology standards search failed: {e2}")
                return self._fallback_search_terminology_standards(query, k)
    
    def _fallback_search_company_standards(self, query: str, k: int) -> List[Dict]:
        """íšŒì‚¬ í‘œì¤€ Fallback ê²€ìƒ‰"""
        try:
            standards_path = Config.COMPANY_STANDARDS_PATH
            if not standards_path.exists():
                return []
            
            results = []
            # í‘œì¤€ ë¬¸ì„œ íŒŒì¼ ì°¾ê¸°
            for file_path in standards_path.rglob('*'):
                if file_path.is_file() and file_path.suffix.lower() in ['.xlsx', '.xls', '.pptx', '.txt', '.md']:
                    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë§¤ì¹­ (ì‹¤ì œë¡œëŠ” Vector Storeê°€ í•„ìš”)
                    results.append({
                        "content": f"Standard document: {file_path.name}",
                        "metadata": {
                            "source": str(file_path),
                            "type": "database_standard"
                        }
                    })
            
            return results[:k]
        except Exception as e:
            print(f"âš ï¸  Fallback company standards search failed: {e}")
            return []
    
    def _fallback_search_api_standards(self, query: str, k: int) -> List[Dict]:
        """API í‘œì¤€ Fallback ê²€ìƒ‰"""
        return self._fallback_search_company_standards(query, k)
    
    def _fallback_search_terminology_standards(self, query: str, k: int) -> List[Dict]:
        """ìš©ì–´ í‘œì¤€ Fallback ê²€ìƒ‰"""
        return self._fallback_search_company_standards(query, k)

