"""
Aggregate Draft Standard Transformer
ìƒì„±ëœ Aggregate ì´ˆì•ˆì„ íšŒì‚¬ í‘œì¤€ì— ë§ê²Œ ë³€í™˜
RAG ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ í‘œì¤€ë§Œ ë™ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ì ìš©
"""
from typing import Dict, List, Optional, TypedDict
from pathlib import Path
import json
import re
import os
from datetime import datetime
import pandas as pd
import tempfile
import shutil
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
try:
    from langchain_core.documents import Document
except ImportError:
    from langchain.schema import Document

from project_generator.utils.logging_util import LoggingUtil
from src.project_generator.workflows.common.rag_retriever import RAGRetriever
from src.project_generator.workflows.common.standard_rag_service import (
    StandardRAGService, StandardQuery, StandardSearchResult
)
from src.project_generator.workflows.common.standard_loader import StandardLoader
from src.project_generator.config import Config


# ============================================================================
# Standard Mapping Context (Terminology/Standard Mapping ë ˆì´ì–´)
# ============================================================================

class TableNameMapping(TypedDict):
    """í…Œì´ë¸”ëª… ë§¤í•‘"""
    entity_to_table: Dict[str, str]  # í•œê¸€/ì˜ë¬¸ ë„ë©”ì¸ ëª… -> í…Œì´ë¸”ëª… (ì˜ˆ: "ì£¼ë¬¸ ë§ˆìŠ¤í„°" -> "T_ODR_M")
    table_standards: Dict[str, str]  # ëª¨ë“  í‘œì¤€ ë§¤í•‘ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
    column_standards: Dict[str, str]  # ëª¨ë“  í‘œì¤€ ë§¤í•‘ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
    domain_to_tables: Dict[str, List[str]]  # ë„ë©”ì¸ ì½”ë“œ -> í…Œì´ë¸”ëª… ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: "ODR" -> ["T_ODR_M", "T_ODR_D"])


class DomainCodeMapping(TypedDict):
    """ë„ë©”ì¸ ì½”ë“œ ë§¤í•‘"""
    name_to_domain: Dict[str, str]  # ë„ë©”ì¸ ëª…ì¹­(í•œ/ì˜) -> ë„ë©”ì¸ ì½”ë“œ (ì˜ˆ: "ì£¼ë¬¸"/"Order" -> "ODR")
    table_to_domain: Dict[str, str]  # í…Œì´ë¸”ëª… -> ë„ë©”ì¸ ì½”ë“œ (ì˜ˆ: "T_ODR_M" -> "ODR")


class ColumnNameMapping(TypedDict):
    """ì»¬ëŸ¼ëª… ë§¤í•‘ (ë¡œê¹…ìš©)"""
    column_desc_by_table: Dict[str, Dict[str, str]]  # í…Œì´ë¸”ëª… -> {ì»¬ëŸ¼ëª… -> ì„¤ëª…} (ë¡œê¹…ìš©, ì‹¤ì œ ë³€í™˜ì—ëŠ” ì‚¬ìš© ì•ˆ í•¨)
    desc_to_columns: Dict[str, List[str]]  # ì„¤ëª… -> ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸ (ë¡œê¹…ìš©, ì‹¤ì œ ë³€í™˜ì—ëŠ” ì‚¬ìš© ì•ˆ í•¨)


class ApiPathMapping(TypedDict):
    """API ê²½ë¡œ ë§¤í•‘"""
    resource_abbrev: Dict[str, str]  # ê°œë…ëª… -> ë¦¬ì†ŒìŠ¤ ì•½ì–´ (ì˜ˆ: "Order" -> "odr")
    action_to_path: Dict[str, str]  # í–‰ìœ„/ê¸°ëŠ¥ -> API ê²½ë¡œ íŒ¨í„´ (ì˜ˆ: "ê²°ì œ ìš”ì²­" -> "/v1/pym/req")
    http_method_by_action: Dict[str, str]  # í–‰ìœ„ -> HTTP Method (ì˜ˆ: "ìƒì„±" -> "POST", "ì¡°íšŒ" -> "GET")


class StandardMappingContext(TypedDict):
    """í‘œì¤€ ë§¤í•‘ ì»¨í…ìŠ¤íŠ¸ (ì „ì²´ ë§¤í•‘ ì‚¬ì „)"""
    table: TableNameMapping
    domain: DomainCodeMapping
    column: ColumnNameMapping
    api: ApiPathMapping


class StandardTransformationState(TypedDict):
    """í‘œì¤€ ë³€í™˜ ìƒíƒœ"""
    # Input
    draft_options: List[Dict]  # ìƒì„±ëœ Aggregate ì´ˆì•ˆ ì˜µì…˜ë“¤
    bounded_context: Dict  # Bounded Context ì •ë³´
    
    # Working state
    extracted_names: List[str]  # ì¶”ì¶œëœ ì´ë¦„ë“¤
    queries: List[str]  # ìƒì„±ëœ ì¿¼ë¦¬ë“¤
    relevant_standards: List[Dict]  # ê²€ìƒ‰ëœ ê´€ë ¨ í‘œì¤€ ì²­í¬ë“¤
    
    # Output
    transformed_options: List[Dict]  # ë³€í™˜ëœ ì˜µì…˜ë“¤
    transformation_log: str  # ë³€í™˜ ë¡œê·¸
    is_completed: bool
    error: str


class AggregateDraftStandardTransformer:
    """
    Aggregate ì´ˆì•ˆ í‘œì¤€ ë³€í™˜ê¸°
    RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ í‘œì¤€ë§Œ ê²€ìƒ‰í•˜ê³  ì ìš©
    """
    
    # í´ë˜ìŠ¤ ë ˆë²¨ ë³€ìˆ˜: ì„¸ì…˜ë³„ ì¸ë±ì‹± ìƒíƒœ ì¶”ì 
    # ê°™ì€ transformation_session_idë¥¼ ê°€ì§„ BCë“¤ì€ ê°™ì€ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ê³µìœ 
    _indexed_sessions = set()  # {transformation_session_id}
    _base_standards_indexed = set()  # {transformation_session_id} - ê¸°ë³¸ í‘œì¤€ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œëœ ì„¸ì…˜
    _user_documents_downloaded = set()  # {user_id} - ì‚¬ìš©ìë³„ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
    _user_vectorstores_indexed = set()  # {(user_id, transformation_session_id)} - ì‚¬ìš©ìë³„ Vector Store ì¸ë±ì‹± ì™„ë£Œ
    _vectorstore_cleared_sessions = set()  # {(user_id, transformation_session_id)} - Vector Store í´ë¦¬ì–´ ì™„ë£Œëœ ì„¸ì…˜
    
    def __init__(self, enable_rag: bool = True, user_id: Optional[str] = None):
        """
        Args:
            enable_rag: RAG ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
            user_id: ì‚¬ìš©ì ID (Firebase Storageì—ì„œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ ë•Œ ì‚¬ìš©)
        """
        self.enable_rag = enable_rag
        self.user_id = user_id
        self.user_standards_path: Optional[Path] = None  # ì‚¬ìš©ìë³„ í‘œì¤€ ë¬¸ì„œ ê²½ë¡œ
        self.user_vectorstore_path: Optional[str] = None  # ì‚¬ìš©ìë³„ Vector Store ê²½ë¡œ
        
        # ì‚¬ìš©ì IDê°€ ìˆìœ¼ë©´ Firebase Storageì—ì„œ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ
        if user_id:
            self.user_standards_path = self._download_user_standards_from_firebase(user_id)
            # ì‚¬ìš©ìë³„ Vector Store ê²½ë¡œ ì„¤ì •
            if self.user_standards_path:
                self.user_vectorstore_path = str(self.user_standards_path / 'vectorstore')
        
        # ì‚¬ìš©ìë³„ Vector Store ê²½ë¡œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        vectorstore_path = self.user_vectorstore_path if self.user_vectorstore_path else None
        self.rag_retriever = RAGRetriever(vectorstore_path=vectorstore_path) if enable_rag else None
        # StandardRAGService ì´ˆê¸°í™” (ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ ì§€ì›)
        # ê¸°ë³¸ ì„ê³„ê°’: 0.7 (RAGëŠ” "ì°¸ê³ ìš© ì»¨í…ìŠ¤íŠ¸ ì œê³µìš©"ìœ¼ë¡œë§Œ ì‚¬ìš©, ë†’ì€ ìœ ì‚¬ë„ë§Œ ì±„íƒ)
        # í•„ìˆ˜ ë§¤í•‘ì€ ì „ì²´ í‘œì¤€ ì›ë³¸ì„ ì§ì ‘ ì½ì–´ì„œ global mapping êµ¬ì„± (ìœ ì‚¬ë„ ê²€ìƒ‰ê³¼ ë¬´ê´€)
        # í™˜ê²½ ë³€ìˆ˜ STANDARD_TRANSFORMER_SCORE_THRESHOLDë¡œ ì¡°ì • ê°€ëŠ¥
        score_threshold = Config.STANDARD_TRANSFORMER_SCORE_THRESHOLD
        self.standard_service = StandardRAGService(
            retriever=self.rag_retriever,
            score_threshold=score_threshold  # ê¸°ë³¸ê°’: 0.7
        ) if enable_rag and self.rag_retriever else None
        
        # StandardLoader ì´ˆê¸°í™” (ì „ì²´ í‘œì¤€ ì›ë³¸ ì½ê¸°ìš©)
        # LLM ë¹„í™œì„±í™”: ì¸ë±ì‹± ì‹œ ê°„ë‹¨í•œ ë²ˆì—­/í‚¤ì›Œë“œë§Œ ì‚¬ìš©
        self.standard_loader = StandardLoader(enable_llm=False) if enable_rag else None
        
        self.llm = ChatOpenAI(
            model="gpt-4.1-2025-04-14",
            temperature=0.2,
            top_p=1.0,
            frequency_penalty=0.0,
            presence_penalty=0.0,
            timeout=300,  # 5ë¶„ íƒ€ì„ì•„ì›ƒ (ì´ˆ ë‹¨ìœ„) - LLM ì‘ë‹µ ì§€ì—° ëŒ€ë¹„
            max_tokens=32768
        )
        
        # aggregate_draft_generatorì™€ ë™ì¼í•œ íŒ¨í„´ ì‚¬ìš©
        self.llm_structured = self.llm.with_structured_output(
            self._get_response_schema(),
            strict=True
        )
        
        # ğŸ”§ BC ê°„ ì°¸ì¡°ë¥¼ ìœ„í•œ ì „ì—­ aggregate ì´ë¦„ ë§¤í•‘ (original_name -> transformed_name)
        # ëª¨ë“  BCì˜ aggregate ë³€í™˜ ê²°ê³¼ë¥¼ ëˆ„ì í•˜ì—¬ ì €ì¥
        self._global_aggregate_name_mapping: Dict[str, str] = {}
    
    def _download_user_standards_from_firebase(self, user_id: str) -> Optional[Path]:
        """
        Firebase Storageì—ì„œ ì‚¬ìš©ìë³„ í‘œì¤€ ë¬¸ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ knowledge_base/company_standards/{user_id}/ ê²½ë¡œì— ì €ì¥
        
        í”„ë¡œì„¸ìŠ¤ê°€ ë™ì‘í•˜ëŠ” ë™ì•ˆ ì„ì‹œì ìœ¼ë¡œ ì €ì¥ë˜ë©°, í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ ì •ë¦¬ë©ë‹ˆë‹¤.
        ê°™ì€ ì‚¬ìš©ì IDì˜ ê²½ìš° í•œ ë²ˆë§Œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
            
        Returns:
            ë‹¤ìš´ë¡œë“œí•œ ë¬¸ì„œê°€ ì €ì¥ëœ ê²½ë¡œ (Path ê°ì²´), ì‹¤íŒ¨ ì‹œ None
        """
        # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
        if user_id in AggregateDraftStandardTransformer._user_documents_downloaded:
            user_standards_dir = Config.COMPANY_STANDARDS_PATH / user_id
            if user_standards_dir.exists() and any(user_standards_dir.iterdir()):
                LoggingUtil.info("StandardTransformer", f"â™»ï¸  ì‚¬ìš©ì({user_id}) í‘œì¤€ ë¬¸ì„œ ì¬ì‚¬ìš© (ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨)")
                return user_standards_dir
        
        try:
            from firebase_admin import storage as firebase_storage
            
            LoggingUtil.info("StandardTransformer", f"ğŸ“¥ Firebase Storageì—ì„œ ì‚¬ìš©ì({user_id}) í‘œì¤€ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
            
            # knowledge_base/company_standards/{user_id}/ ê²½ë¡œì— ì €ì¥
            user_standards_dir = Config.COMPANY_STANDARDS_PATH / user_id
            
            # ë””ë ‰í† ë¦¬ ìƒì„± ë° ê¶Œí•œ ì„¤ì • (non-root ì‚¬ìš©ìë¥¼ ìœ„í•œ ê¶Œí•œ ì„¤ì •)
            user_standards_dir.mkdir(parents=True, exist_ok=True)
            # ë””ë ‰í† ë¦¬ì™€ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë“¤ì— ì“°ê¸° ê¶Œí•œ ë¶€ì—¬
            try:
                os.chmod(user_standards_dir, 0o777)
                # ë¶€ëª¨ ë””ë ‰í† ë¦¬ë“¤ë„ ê¶Œí•œ ì„¤ì •
                parent = user_standards_dir.parent
                while parent.exists() and parent != Config.COMPANY_STANDARDS_PATH.parent:
                    os.chmod(parent, 0o777)
                    parent = parent.parent
                if Config.COMPANY_STANDARDS_PATH.exists():
                    os.chmod(Config.COMPANY_STANDARDS_PATH, 0o777)
            except (OSError, PermissionError) as perm_error:
                LoggingUtil.warning("StandardTransformer", f"âš ï¸  ë””ë ‰í† ë¦¬ ê¶Œí•œ ì„¤ì • ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {perm_error}")
            
            # Firebase Storageì—ì„œ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
            # bucket ì´ë¦„ì„ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
            storage_bucket_name = os.getenv('FIREBASE_STORAGE_BUCKET')
            if storage_bucket_name:
                # gs:// prefix ì œê±° (ìˆëŠ” ê²½ìš°)
                if storage_bucket_name.startswith('gs://'):
                    storage_bucket_name = storage_bucket_name[5:]
                bucket = firebase_storage.bucket(storage_bucket_name)
            else:
                bucket = firebase_storage.bucket()
            storage_path = f"standard-documents/{user_id}/"
            blobs = bucket.list_blobs(prefix=storage_path)
            
            downloaded_files = []
            for blob in blobs:
                # ë””ë ‰í† ë¦¬ëŠ” ê±´ë„ˆë›°ê¸°
                if blob.name.endswith('/'):
                    continue
                
                # íŒŒì¼ëª… ì¶”ì¶œ (standard-documents/{user_id}/standard-document.xlsx -> standard-document.xlsx)
                file_name = blob.name.split('/')[-1]
                
                # ì‚¬ìš©ìë³„ ë””ë ‰í† ë¦¬ì— íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                local_file_path = user_standards_dir / file_name
                blob.download_to_filename(str(local_file_path))
                # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì— ì“°ê¸° ê¶Œí•œ ë¶€ì—¬ (non-root ì‚¬ìš©ìë¥¼ ìœ„í•´)
                try:
                    os.chmod(local_file_path, 0o666)
                except (OSError, PermissionError):
                    pass  # ê¶Œí•œ ì„¤ì • ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                downloaded_files.append(file_name)
                LoggingUtil.info("StandardTransformer", f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_name}")
            
            if downloaded_files:
                LoggingUtil.info("StandardTransformer", f"ğŸ“ ì‚¬ìš©ì í‘œì¤€ ë¬¸ì„œ ê²½ë¡œ: {user_standards_dir}")
                # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í‘œì‹œ
                AggregateDraftStandardTransformer._user_documents_downloaded.add(user_id)
                return user_standards_dir
            else:
                LoggingUtil.warning("StandardTransformer", f"âš ï¸  ì‚¬ìš©ì({user_id})ì˜ í‘œì¤€ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # ë¹ˆ ë””ë ‰í† ë¦¬ ì •ë¦¬
                try:
                    user_standards_dir.rmdir()
                except (OSError, FileNotFoundError):
                    pass
                return None
                
        except ImportError:
            LoggingUtil.warning("StandardTransformer", "âš ï¸  firebase_admin.storageë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        except Exception as e:
            LoggingUtil.error("StandardTransformer", f"âŒ Firebase Storageì—ì„œ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup_user_standards(self):
        """
        ì‚¬ìš©ìë³„ í‘œì¤€ ë¬¸ì„œ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ í˜¸ì¶œí•˜ì—¬ ì„ì‹œ íŒŒì¼ë“¤ì„ ì‚­ì œ
        """
        if self.user_standards_path and self.user_standards_path.exists():
            try:
                # Vector Storeë„ í•¨ê»˜ ì‚­ì œ
                if self.user_vectorstore_path and Path(self.user_vectorstore_path).exists():
                    shutil.rmtree(self.user_vectorstore_path, ignore_errors=True)
                
                # ì‚¬ìš©ìë³„ ë¬¸ì„œ ë””ë ‰í† ë¦¬ ì‚­ì œ
                shutil.rmtree(self.user_standards_path, ignore_errors=True)
                LoggingUtil.info("StandardTransformer", f"ğŸ§¹ ì‚¬ìš©ì í‘œì¤€ ë¬¸ì„œ ì •ë¦¬ ì™„ë£Œ: {self.user_standards_path}")
            except Exception as e:
                LoggingUtil.warning("StandardTransformer", f"âš ï¸  ì‚¬ìš©ì í‘œì¤€ ë¬¸ì„œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def reprocess_all_bcs_with_complete_mapping(self, all_bc_results: List[Dict]) -> List[Dict]:
        """
        ëª¨ë“  BC ì²˜ë¦¬ ì™„ë£Œ í›„ ì „ì²´ì ìœ¼ë¡œ ë‹¤ì‹œ prefixë¥¼ ì²˜ë¦¬
        BC ê°„ ì°¸ì¡° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ëª¨ë“  BCì˜ aggregate ë§¤í•‘ì´ ì™„ë£Œëœ í›„ ì¬ì²˜ë¦¬
        
        Args:
            all_bc_results: ëª¨ë“  BCì˜ ë³€í™˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
                ê° í•­ëª©ì€ {
                    'boundedContext': str,
                    'transformedOptions': List[Dict],
                    ...
                } í˜•íƒœ
        
        Returns:
            ì¬ì²˜ë¦¬ëœ ëª¨ë“  BC ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        LoggingUtil.info("StandardTransformer", "ğŸ”„ ëª¨ë“  BC ì¬ì²˜ë¦¬ ì‹œì‘ (ì „ì²´ aggregate ë§¤í•‘ ì‚¬ìš©)")
        
        # 1. ì „ì—­ ë§¤í•‘ ì‚¬ìš© (original_name -> transformed_name)
        complete_aggregate_mapping = self._global_aggregate_name_mapping.copy()
        LoggingUtil.info("StandardTransformer", 
                       f"   [ì „ì²´ë§¤í•‘ìˆ˜ì§‘] complete_aggregate_mapping={complete_aggregate_mapping}")
        
        # 2. ê° BCì˜ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì²˜ë¦¬
        reprocessed_results = []
        for bc_result in all_bc_results:
            bounded_context = bc_result.get("boundedContext")
            transformed_options = bc_result.get("transformedOptions", [])
            
            LoggingUtil.info("StandardTransformer", 
                           f"   [ì¬ì²˜ë¦¬] BC: {bounded_context}, ì˜µì…˜ ìˆ˜: {len(transformed_options)}")
            
            # ê° ì˜µì…˜ì˜ Enum/VOë¥¼ ì „ì²´ ë§¤í•‘ìœ¼ë¡œ ë‹¤ì‹œ ì²˜ë¦¬
            reprocessed_options = []
            for option in transformed_options:
                structure = option.get("structure", [])
                
                for item in structure:
                    aggregate = item.get("aggregate", {})
                    agg_alias = aggregate.get("alias")
                    current_agg_name = aggregate.get("name")
                    
                    # Enum ì²˜ë¦¬
                    enumerations = item.get("enumerations", [])
                    for enum in enumerations:
                        enum_alias = enum.get("alias")
                        enum_name = enum.get("name")
                        
                        # ì „ì—­ ë§¤í•‘ì—ì„œ ì°¸ì¡°í•˜ëŠ” aggregate ì°¾ê¸°
                        # Enum ì´ë¦„ì´ ì›ë³¸ aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                        for original_agg_name, transformed_agg_name in complete_aggregate_mapping.items():
                            # ì˜ˆ: "OrderStatus"ëŠ” "Order"ë¡œ ì‹œì‘
                            if enum_name and original_agg_name and enum_name.startswith(original_agg_name):
                                # aggregate ì´ë¦„ì„ í¬í•¨í•˜ëŠ” ê²½ìš° prefix ì ìš©
                                suffix = enum_name[len(original_agg_name):]
                                import re
                                suffix_snake = re.sub(r'(?<!^)(?=[A-Z])', '_', suffix).lower()
                                new_enum_name = transformed_agg_name + "_" + suffix_snake if suffix_snake else transformed_agg_name + "_enum"
                                # ë³€í™˜ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì ìš© (ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ë³€í™˜ëœ ê²½ìš°ëŠ” ì œì™¸)
                                if enum_name != new_enum_name:
                                    enum["name"] = new_enum_name
                                    LoggingUtil.info("StandardTransformer", 
                                                   f"   [ì¬ì²˜ë¦¬Enum] '{enum_alias}': '{enum_name}' â†’ '{new_enum_name}' (ì°¸ì¡°: {original_agg_name} â†’ {transformed_agg_name})")
                                break
                    
                    # VO ì²˜ë¦¬
                    value_objects = item.get("valueObjects", [])
                    for vo in value_objects:
                        vo_alias = vo.get("alias")
                        vo_name = vo.get("name")
                        
                        # ì „ì—­ ë§¤í•‘ì—ì„œ ì°¸ì¡°í•˜ëŠ” aggregate ì°¾ê¸°
                        # VO ì´ë¦„ì´ ì›ë³¸ aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                        for original_agg_name, transformed_agg_name in complete_aggregate_mapping.items():
                            # ì˜ˆ: "CustomerReference"ëŠ” "Customer"ë¡œ ì‹œì‘
                            if vo_name and original_agg_name and vo_name.startswith(original_agg_name):
                                # aggregate ì´ë¦„ì„ í¬í•¨í•˜ëŠ” ê²½ìš° prefix ì ìš©
                                suffix = vo_name[len(original_agg_name):]
                                import re
                                suffix_snake = re.sub(r'(?<!^)(?=[A-Z])', '_', suffix).lower()
                                new_vo_name = transformed_agg_name + "_" + suffix_snake if suffix_snake else transformed_agg_name + "_vo"
                                # ë³€í™˜ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì ìš© (ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ë³€í™˜ëœ ê²½ìš°ëŠ” ì œì™¸)
                                if vo_name != new_vo_name:
                                    vo["name"] = new_vo_name
                                    LoggingUtil.info("StandardTransformer", 
                                                   f"   [ì¬ì²˜ë¦¬VO] '{vo_alias}': '{vo_name}' â†’ '{new_vo_name}' (ì°¸ì¡°: {original_agg_name} â†’ {transformed_agg_name})")
                                break
                
                reprocessed_options.append(option)
            
            reprocessed_results.append({
                **bc_result,
                "transformedOptions": reprocessed_options
            })
        
        LoggingUtil.info("StandardTransformer", "âœ… ëª¨ë“  BC ì¬ì²˜ë¦¬ ì™„ë£Œ")
        return reprocessed_results
    
    def transform(self, draft_options: List[Dict], bounded_context: Dict, job_id: Optional[str] = None, 
                  firebase_update_callback: Optional[callable] = None, transformation_session_id: Optional[str] = None) -> Dict:
        """
        Aggregate ì´ˆì•ˆì„ í‘œì¤€ì— ë§ê²Œ ë³€í™˜
        
        Args:
            draft_options: ìƒì„±ëœ Aggregate ì´ˆì•ˆ ì˜µì…˜ë“¤
            bounded_context: Bounded Context ì •ë³´
            job_id: Job ID (ê²°ê³¼ ì €ì¥ìš©, ì„ íƒì‚¬í•­)
            firebase_update_callback: Firebase ì—…ë°ì´íŠ¸ ì½œë°± í•¨ìˆ˜ (ì„ íƒì‚¬í•­)
            
        Returns:
            ë³€í™˜ëœ ì˜µì…˜ë“¤ê³¼ ë³€í™˜ ë¡œê·¸
        """
        def _update_job_progress(progress: int, stage: str, 
                                 bc_name: Optional[str] = None,
                                 agg_name: Optional[str] = None,
                                 property_type: Optional[str] = None,  # "enum", "vo", "field", "aggregate"
                                 chunk_info: Optional[str] = None,  # "ì²­í¬ 1/2" ë“±
                                 status: str = "processing",  # "processing", "completed", "error"
                                 error_message: Optional[str] = None):
            """Firebaseì— ìƒì„¸ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
            if firebase_update_callback:
                try:
                    # ìƒì„¸ ì •ë³´ êµ¬ì„±
                    detail_info = []
                    if bc_name:
                        detail_info.append(f"BC: {bc_name}")
                    if agg_name:
                        detail_info.append(f"Agg: {agg_name}")
                    if property_type:
                        property_label = {
                            "aggregate": "Aggregate",
                            "enum": "Enum",
                            "vo": "ValueObject",
                            "field": "Field"
                        }.get(property_type, property_type)
                        detail_info.append(f"{property_label}")
                    if chunk_info:
                        detail_info.append(chunk_info)
                    
                    detail_text = " > ".join(detail_info) if detail_info else stage
                    
                    # ìƒíƒœì— ë”°ë¥¸ ë©”ì‹œì§€ í¬ë§·
                    if status == "error":
                        message = f"âŒ ì˜¤ë¥˜: {detail_text}"
                        if error_message:
                            message += f" ({error_message})"
                    elif status == "completed":
                        message = f"âœ… ì™„ë£Œ: {detail_text}"
                    else:
                        message = f"ğŸ”„ ì²˜ë¦¬ ì¤‘: {detail_text}"
                    
                    firebase_update_callback({
                        'progress': progress,
                        'transformationLog': message,
                        'isCompleted': False,
                        'currentBC': bc_name,
                        'currentAgg': agg_name,
                        'currentPropertyType': property_type,
                        'chunkInfo': chunk_info,
                        'status': status,
                        'error': error_message if status == "error" else None
                    })
                except Exception as e:
                    LoggingUtil.warning("StandardTransformer", f"Firebase ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        
        try:
            LoggingUtil.info("StandardTransformer", "ğŸ”„ í‘œì¤€ ë³€í™˜ ì‹œì‘")
            
            # ì‚¬ìš©ìë³„ Vector Store ì¸ë±ì‹± ìƒíƒœ í™•ì¸
            # ê°™ì€ (user_id, transformation_session_id) ì¡°í•©ì— ëŒ€í•´ í•œ ë²ˆë§Œ ì¸ë±ì‹±
            user_vectorstore_key = (self.user_id, transformation_session_id) if self.user_id and transformation_session_id else None
            should_index_vectorstore = False
            should_clear_vectorstore = False
            
            if user_vectorstore_key:
                # ì‚¬ìš©ìë³„ Vector Storeê°€ ì•„ì§ ì¸ë±ì‹±ë˜ì§€ ì•Šì€ ê²½ìš°
                if user_vectorstore_key not in AggregateDraftStandardTransformer._user_vectorstores_indexed:
                    should_index_vectorstore = True
                    should_clear_vectorstore = True
                    AggregateDraftStandardTransformer._user_vectorstores_indexed.add(user_vectorstore_key)
                    LoggingUtil.info("StandardTransformer", f"ğŸ“ ì‚¬ìš©ìë³„ Vector Store ì¸ë±ì‹± í•„ìš”: user_id={self.user_id}, session={transformation_session_id}")
                else:
                    # ì´ë¯¸ ì¸ë±ì‹±ëœ ê²ƒìœ¼ë¡œ í‘œì‹œë˜ì–´ ìˆì§€ë§Œ, Vector Storeê°€ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦
                    if self.rag_retriever and (not self.rag_retriever._initialized or not self.rag_retriever.vectorstore):
                        # Vector Store ì´ˆê¸°í™” ì‹¤íŒ¨ ê°ì§€: ì¸ë±ì‹± ìƒíƒœ ì œê±°í•˜ê³  ì¬ì¸ë±ì‹±
                        AggregateDraftStandardTransformer._user_vectorstores_indexed.discard(user_vectorstore_key)
                        should_index_vectorstore = True
                        should_clear_vectorstore = True
                        LoggingUtil.warning("StandardTransformer", 
                                          f"âš ï¸  Vector Store ì´ˆê¸°í™” ì‹¤íŒ¨ ê°ì§€: ì¸ë±ì‹± ìƒíƒœ ì´ˆê¸°í™” ë° ì¬ì¸ë±ì‹± ì‹œë„ (user_id={self.user_id}, session={transformation_session_id})")
                    else:
                        LoggingUtil.info("StandardTransformer", f"â™»ï¸  ì‚¬ìš©ìë³„ Vector Store ì¬ì‚¬ìš©: user_id={self.user_id}, session={transformation_session_id}")
            elif transformation_session_id:
                # transformation_session_idë§Œ ìˆëŠ” ê²½ìš° (ê¸°ì¡´ ë¡œì§)
                if transformation_session_id not in AggregateDraftStandardTransformer._indexed_sessions:
                    should_clear_vectorstore = True
                    AggregateDraftStandardTransformer._indexed_sessions.add(transformation_session_id)
            else:
                # transformation_session_idê°€ ì—†ìœ¼ë©´ í•­ìƒ í´ë¦¬ì–´ (ê¸°ì¡´ ë™ì‘ ìœ ì§€)
                should_clear_vectorstore = True
            
            # ê°™ì€ ì„¸ì…˜ì—ì„œëŠ” Vector Storeë¥¼ í•œ ë²ˆë§Œ í´ë¦¬ì–´ (ì¤‘ë³µ í´ë¦¬ì–´ ë°©ì§€)
            vectorstore_clear_key = None
            if self.user_id and transformation_session_id:
                vectorstore_clear_key = (self.user_id, transformation_session_id)
            elif transformation_session_id:
                vectorstore_clear_key = (None, transformation_session_id)
            
            should_clear_now = should_clear_vectorstore
            if vectorstore_clear_key and vectorstore_clear_key in AggregateDraftStandardTransformer._vectorstore_cleared_sessions:
                should_clear_now = False  # ì´ë¯¸ í´ë¦¬ì–´ëœ ì„¸ì…˜ì´ë©´ ìŠ¤í‚µ
                LoggingUtil.info("StandardTransformer", 
                               f"â„¹ï¸  Vector StoreëŠ” ì´ë¯¸ í´ë¦¬ì–´ë¨ (ì„¸ì…˜: {transformation_session_id}). ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            
            if should_clear_now and self.rag_retriever:
                LoggingUtil.info("StandardTransformer", "ğŸ—‘ï¸  ê¸°ì¡´ Vector Store í´ë¦¬ì–´ ì¤‘...")
                try:
                    clear_success = self.rag_retriever.clear_vectorstore()
                    if clear_success and vectorstore_clear_key:
                        # í´ë¦¬ì–´ ì„±ê³µ ì‹œ ì„¸ì…˜ ê¸°ë¡
                        AggregateDraftStandardTransformer._vectorstore_cleared_sessions.add(vectorstore_clear_key)
                    elif not clear_success:
                        # Vector Store í´ë¦¬ì–´ ì‹¤íŒ¨ ì‹œ ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ì†ìƒ ê°€ëŠ¥ì„±
                        # RAGRetrieverì˜ ë³µêµ¬ ë©”ì„œë“œ ì‚¬ìš©
                        LoggingUtil.warning("StandardTransformer", 
                                          f"âš ï¸  Vector Store í´ë¦¬ì–´ ì‹¤íŒ¨: ë°ì´í„°ë² ì´ìŠ¤ ì†ìƒ ê°€ëŠ¥ì„±. ë³µêµ¬ ì‹œë„ ì¤‘...")
                        try:
                            # RAGRetrieverì˜ ë³µêµ¬ ë©”ì„œë“œ í˜¸ì¶œ
                            if hasattr(self.rag_retriever, '_repair_vectorstore'):
                                repair_success = self.rag_retriever._repair_vectorstore()
                                if repair_success:
                                    LoggingUtil.info("StandardTransformer", "âœ… Vector Store ë³µêµ¬ ë° ì¬ì´ˆê¸°í™” ì™„ë£Œ")
                                    if vectorstore_clear_key:
                                        AggregateDraftStandardTransformer._vectorstore_cleared_sessions.add(vectorstore_clear_key)
                                else:
                                    LoggingUtil.warning("StandardTransformer", 
                                                      f"âš ï¸  Vector Store ë³µêµ¬ ì‹¤íŒ¨: _initialized={self.rag_retriever._initialized}, vectorstore={self.rag_retriever.vectorstore is not None}")
                            else:
                                # êµ¬ë²„ì „ í˜¸í™˜ì„±: ìˆ˜ë™ ë³µêµ¬
                                import shutil
                                vectorstore_path = Path(self.rag_retriever.vectorstore_path) if hasattr(self.rag_retriever, 'vectorstore_path') else None
                                if vectorstore_path and vectorstore_path.exists():
                                    shutil.rmtree(vectorstore_path)
                                LoggingUtil.info("StandardTransformer", f"ğŸ—‘ï¸  ì†ìƒëœ Vector Store ë””ë ‰í† ë¦¬ ì‚­ì œ ì™„ë£Œ: {vectorstore_path}")
                                # RAGRetriever ì¬ì´ˆê¸°í™”
                                from src.project_generator.workflows.common.rag_retriever import RAGRetriever
                                self.rag_retriever = RAGRetriever(vectorstore_path=str(vectorstore_path))
                                if not self.rag_retriever._initialized or not self.rag_retriever.vectorstore:
                                    LoggingUtil.warning("StandardTransformer", 
                                                      f"âš ï¸  Vector Store ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: _initialized={self.rag_retriever._initialized}, vectorstore={self.rag_retriever.vectorstore is not None}")
                                else:
                                    LoggingUtil.info("StandardTransformer", "âœ… Vector Store ì¬ì´ˆê¸°í™” ì™„ë£Œ")
                                if vectorstore_clear_key:
                                    AggregateDraftStandardTransformer._vectorstore_cleared_sessions.add(vectorstore_clear_key)
                        except Exception as cleanup_e:
                            LoggingUtil.warning("StandardTransformer", f"âš ï¸  Vector Store ë³µêµ¬ ì‹¤íŒ¨: {cleanup_e}")
                except Exception as e:
                    LoggingUtil.warning("StandardTransformer", f"âš ï¸  Vector Store í´ë¦¬ì–´ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
            
            transformation_logs = []  # ì§„í–‰ ë‹¨ê³„ ë¡œê·¸ ìˆ˜ì§‘
            
            # ğŸ”„ êµ¬ì¡° ë³€ê²½: BC ë‹¨ìœ„ê°€ ì•„ë‹Œ structure(aggregate) ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            # ê° structureë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ë³µì¡ë„ ê°ì†Œ
            
            transformation_logs.append("í‘œì¤€ ë§¤í•‘ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
            _update_job_progress(0, "í‘œì¤€ ë§¤í•‘ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
            
            # 1. StandardMappingContext ìƒì„± (ì„ í–‰ ì²˜ë¦¬) - BC ì „ì²´ ê¸°ì¤€
            mapping_context: Optional[StandardMappingContext] = None
            mapping_context = self._build_global_standard_mapping_context(
                draft_options=draft_options,
                bounded_context=bounded_context,
                transformation_session_id=transformation_session_id,
                should_index_vectorstore=should_index_vectorstore
            )
            transformation_logs.append("ì„ ì²˜ë¦¬ ë§¤í•‘ ì ìš© ì¤‘...")
            _update_job_progress(20, "ì„ ì²˜ë¦¬ ë§¤í•‘ ì ìš© ì¤‘...")
            
            # 2. ì„ ì²˜ë¦¬ ì „ ì›ë³¸ ì €ì¥ (Enum/VO í›„ì²˜ë¦¬ ì‹œ ë§¤í•‘ ì •ë³´ ì¶”ì¶œìš©)
            import copy
            original_draft_options = copy.deepcopy(draft_options)
            
            # 3. Deterministic ë£° ì ìš© (ì„ í–‰ ì¹˜í™˜) - BC ì „ì²´ì— ì ìš©
            mapped_draft_options = draft_options
            if mapping_context:
                mapped_draft_options = self._apply_standard_mappings(draft_options, mapping_context)
            
            transformation_logs.append("structure ë‹¨ìœ„ ë³€í™˜ ì‹œì‘...")
            _update_job_progress(30, "structure ë‹¨ìœ„ ë³€í™˜ ì‹œì‘...")
            
            # 4. ê° optionì˜ structureë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
            transformed_options = []
            all_query_search_results = []  # ëª¨ë“  structureì˜ ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
            all_relevant_standards = []  # ëª¨ë“  structureì˜ ê²€ìƒ‰ëœ í‘œì¤€ ìˆ˜ì§‘
            seen_standard_keys = set()  # ì¤‘ë³µ ì œê±°ìš©
            
            for opt_idx, option in enumerate(mapped_draft_options):
                structure = option.get("structure", [])
                transformed_structure = []
                
                LoggingUtil.info("StandardTransformer", 
                               f"ğŸ“¦ Option {opt_idx + 1}/{len(mapped_draft_options)} ì²˜ë¦¬ ì‹œì‘: {len(structure)}ê°œ structure")
                
                # ê° structure(aggregate) ë‹¨ìœ„ë¡œ ì²˜ë¦¬
                for struct_idx, struct_item in enumerate(structure):
                    agg_name = struct_item.get("aggregate", {}).get("name", "Unknown")
                    LoggingUtil.info("StandardTransformer", 
                                   f"   ğŸ”„ Structure {struct_idx + 1}/{len(structure)} ì²˜ë¦¬ ì‹œì‘: {agg_name}")
                    # ë‹¨ì¼ structureë¡œ êµ¬ì„±ëœ ì„ì‹œ ì˜µì…˜ ìƒì„±
                    single_structure_option = {
                        "structure": [struct_item]
                    }
                    single_structure_options = [single_structure_option]
                    
                    # ì´ structureì—ì„œ ì´ë¦„ ì¶”ì¶œ
                    structure_names = self._extract_names_from_structure(
                        single_structure_options, bounded_context
                    )
                    
                    # ì„ ì²˜ë¦¬ë¡œ ë§¤í•‘ëœ ì´ë¦„ ì œì™¸
                    mapped_names = set()
                    if mapping_context:
                        for name in structure_names:
                            if name in mapping_context["table"]["entity_to_table"]:
                                mapped_names.add(name)
                    
                    filtered_names = [n for n in structure_names if n not in mapped_names]
                    
                    # ì¿¼ë¦¬ ìƒì„±
                    standard_queries = self._build_standard_queries(
                        filtered_names,
                        bounded_context,
                        single_structure_options
                    )
                    
                    # RAG ê²€ìƒ‰ (top-k=3)
                    relevant_standards = []
                    query_search_results = []
                    has_rag_search_results = False
                    if self.enable_rag and self.standard_service and len(standard_queries) > 0:
                        search_result = self._retrieve_relevant_standards_with_categories(
                            standard_queries,
                            k_per_query=3,  # top-k 3ê°œë¡œ ë³€ê²½
                            transformation_session_id=transformation_session_id
                        )
                        if isinstance(search_result, tuple):
                            relevant_standards, query_search_results = search_result
                        else:
                            relevant_standards = search_result
                            query_search_results = getattr(self, '_last_query_search_results', [])
                        
                        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì‹¤ì œë¡œ ìˆëŠ”ì§€ í™•ì¸
                        has_rag_search_results = len(relevant_standards) > 0 or len(query_search_results) > 0
                        
                        if not has_rag_search_results:
                            LoggingUtil.warning("StandardTransformer", 
                                              f"âš ï¸  RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {agg_name} - Vector Store ì¸ë±ì‹± ì‹¤íŒ¨ ë˜ëŠ” ê´€ë ¨ í‘œì¤€ ë¬¸ì„œ ì—†ìŒ. LLMì´ í‘œì¤€ ë¬¸ì„œ ì—†ì´ ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                            if firebase_update_callback:
                                try:
                                    bc_name_val = bounded_context.get("name", bounded_context.get("boundedContext", "Unknown"))
                                    _update_job_progress(0, f"âš ï¸ RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {agg_name} (í‘œì¤€ ë¬¸ì„œ ì—†ì´ ë³€í™˜ ì¤‘)",
                                                        bc_name=bc_name_val,
                                                        agg_name=agg_name,
                                                        property_type="aggregate",
                                                        status="processing",
                                                        error_message="RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - Vector Store ì¸ë±ì‹± ì‹¤íŒ¨ ê°€ëŠ¥")
                                except Exception as e:
                                    LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                        
                        all_query_search_results.extend(query_search_results)
                        
                        # relevant_standards ì¤‘ë³µ ì œê±°í•˜ì—¬ ìˆ˜ì§‘
                        for std in relevant_standards:
                            metadata = std.get("metadata", {})
                            source = metadata.get("source", "")
                            chunk_index = metadata.get("chunk_index", "")
                            std_key = f"{source}::{chunk_index}"
                            if std_key not in seen_standard_keys:
                                seen_standard_keys.add(std_key)
                                all_relevant_standards.append(std)
                    elif self.enable_rag and len(standard_queries) == 0:
                        LoggingUtil.warning("StandardTransformer", 
                                          f"âš ï¸  í‘œì¤€ ì¿¼ë¦¬ ì—†ìŒ: {agg_name} - ë³€í™˜í•  í•­ëª©ì´ ì—†ì–´ RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    elif not self.enable_rag:
                        LoggingUtil.info("StandardTransformer", 
                                       f"â„¹ï¸  RAG ë¹„í™œì„±í™”: {agg_name} - RAG ê²€ìƒ‰ ì—†ì´ LLM ë³€í™˜ ìˆ˜í–‰")
                    
                    # ë‹¨ì¼ structure LLM ë³€í™˜
                    try:
                        # original_structure_item ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
                        original_structure_item = None
                        try:
                            if (opt_idx < len(original_draft_options) and 
                                original_draft_options[opt_idx] and
                                "structure" in original_draft_options[opt_idx] and
                                struct_idx < len(original_draft_options[opt_idx]["structure"])):
                                original_structure_item = original_draft_options[opt_idx]["structure"][struct_idx]
                        except (IndexError, KeyError, TypeError) as e:
                            LoggingUtil.warning("StandardTransformer", 
                                              f"âš ï¸  ì›ë³¸ structure í•­ëª© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (opt_idx={opt_idx}, struct_idx={struct_idx}): {e}")
                        
                        LoggingUtil.info("StandardTransformer", 
                                       f"      ğŸ¤– LLM ë³€í™˜ ì‹œì‘: {agg_name}")
                        
                        # BC, Agg ì •ë³´ ì¶”ì¶œ (í´ë¡œì €ë¥¼ ìœ„í•´ ë¡œì»¬ ë³€ìˆ˜ë¡œ ì €ì¥)
                        bc_name_val = bounded_context.get("name", bounded_context.get("boundedContext", "Unknown"))
                        agg_name_val = agg_name
                        
                        # BC, Agg ì •ë³´ë¥¼ í¬í•¨í•œ ì½œë°± ë˜í¼ ìƒì„±
                        def _update_progress_with_context(progress: int, stage: str, 
                                                          property_type: Optional[str] = None,
                                                          chunk_info: Optional[str] = None,
                                                          status: str = "processing",
                                                          error_message: Optional[str] = None,
                                                          bc_name: Optional[str] = None,
                                                          agg_name: Optional[str] = None):
                            _update_job_progress(
                                progress=progress,
                                stage=stage,
                                bc_name=bc_name or bc_name_val,
                                agg_name=agg_name or agg_name_val,
                                property_type=property_type,
                                chunk_info=chunk_info,
                                status=status,
                                error_message=error_message
                            )
                        
                        # ì›ë³¸ optionì˜ boundedContext ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì²­í‚¹ ì²˜ë¦¬ìš©)
                        original_option_bounded_context = None
                        if opt_idx < len(original_draft_options) and original_draft_options[opt_idx]:
                            original_option_bounded_context = original_draft_options[opt_idx].get("boundedContext")
                        
                        transformed_single_structure = self._transform_single_structure_with_llm(
                            structure_item=struct_item,
                            bounded_context=bounded_context,
                            relevant_standards=relevant_standards,
                            query_search_results=query_search_results,
                            original_structure_item=original_structure_item,
                            mapping_context=mapping_context,
                            update_progress_callback=_update_progress_with_context,
                            original_option_bounded_context=original_option_bounded_context
                        )
                        
                        # ë³€í™˜ ê²°ê³¼ê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
                        if not transformed_single_structure:
                            LoggingUtil.warning("StandardTransformer", 
                                              f"âš ï¸  structure ë³€í™˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ, ì›ë³¸ ì‚¬ìš© (opt_idx={opt_idx}, struct_idx={struct_idx})")
                            transformed_single_structure = struct_item
                        
                        transformed_structure.append(transformed_single_structure)
                        LoggingUtil.info("StandardTransformer", 
                                       f"      âœ… Structure {struct_idx + 1}/{len(structure)} ë³€í™˜ ì™„ë£Œ: {agg_name}")
                        
                        # ë³€í™˜ ì™„ë£Œ ì•Œë¦¼
                        if firebase_update_callback:
                            try:
                                bc_name_val = bounded_context.get("name", bounded_context.get("boundedContext", "Unknown"))
                                _update_job_progress(100, f"ë³€í™˜ ì™„ë£Œ: {agg_name}",
                                                    bc_name=bc_name_val,
                                                    agg_name=agg_name,
                                                    property_type="aggregate",
                                                    status="completed")
                            except Exception as update_e:
                                LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_e}")
                        
                    except Exception as e:
                        LoggingUtil.error("StandardTransformer", 
                                        f"âŒ Structure {struct_idx + 1}/{len(structure)} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {agg_name}, {e}")
                        import traceback
                        traceback.print_exc()
                        
                        # ì˜¤ë¥˜ ì•Œë¦¼
                        if firebase_update_callback:
                            try:
                                _update_job_progress(0, f"ë³€í™˜ ì‹¤íŒ¨: {agg_name}",
                                                    bc_name=bc_name,
                                                    agg_name=agg_name,
                                                    property_type="aggregate",
                                                    status="error",
                                                    error_message=str(e))
                            except Exception as update_e:
                                LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_e}")
                        
                        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ì‚¬ìš©
                        transformed_structure.append(struct_item)
                
                # ë³€í™˜ëœ structureë“¤ì„ ì›ë˜ optionì— í•©ì¹˜ê¸°
                # ğŸ”’ CRITICAL: original_draft_optionsì—ì„œ í•„í„°ë§ëœ í•„ë“œ ë³µì›
                if opt_idx < len(original_draft_options):
                    # original_draft_optionsì—ì„œ ì›ë³¸ ì˜µì…˜ ê°€ì ¸ì˜¤ê¸°
                    original_option = original_draft_options[opt_idx]
                    transformed_option = copy.deepcopy(original_option)
                    # ë³€í™˜ëœ structureë§Œ êµì²´
                    transformed_option["structure"] = transformed_structure
                else:
                    # original_draft_optionsê°€ ì—†ìœ¼ë©´ í˜„ì¬ option ì‚¬ìš©
                    transformed_option = copy.deepcopy(option)
                    transformed_option["structure"] = transformed_structure
                transformed_options.append(transformed_option)
            
            transformation_logs.append(f"LLM ë³€í™˜ ì™„ë£Œ: {len(transformed_options)}ê°œ ì˜µì…˜, ì´ {sum(len(opt.get('structure', [])) for opt in transformed_options)}ê°œ structure")
            _update_job_progress(80, "í›„ì²˜ë¦¬ ì¤‘...")
            
            LoggingUtil.info("StandardTransformer", f"âœ… í‘œì¤€ ë³€í™˜ ì™„ë£Œ: {len(transformed_options)}ê°œ ì˜µì…˜ ë³€í™˜ë¨")
            transformation_logs.append(f"ë³€í™˜ ì™„ë£Œ: {len(transformed_options)}ê°œ ì˜µì…˜")
            _update_job_progress(80, "í›„ì²˜ë¦¬ ì¤‘...")
            
            # ë³€í™˜ ê²°ê³¼ ê²€ì¦: ì›ë³¸ë³´ë‹¤ ì˜µì…˜ì´ ë§ì´ ì¤„ì–´ë“¤ë©´ ê²½ê³ 
            if len(transformed_options) < len(draft_options) * 0.5:
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  ë³€í™˜ëœ ì˜µì…˜ì´ ì›ë³¸ì˜ 50% ë¯¸ë§Œì…ë‹ˆë‹¤. "
                                  f"ì›ë³¸: {len(draft_options)}ê°œ, ë³€í™˜: {len(transformed_options)}ê°œ")
            
            # ë³€í™˜ ì „í›„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
            if job_id:
                # ê²€ìƒ‰ëœ í‘œì¤€ ì •ë³´ì™€ ì „ì²˜ë¦¬ ë§¤í•‘ ì •ë³´ ìˆ˜ì§‘
                search_info = {
                    "relevant_standards": all_relevant_standards,  # ëª¨ë“  structureì˜ ê²€ìƒ‰ëœ í‘œì¤€ (ì¤‘ë³µ ì œê±°)
                    "query_search_results": all_query_search_results,  # ëª¨ë“  structureì˜ ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ (top-k=3)
                    "mapping_context": mapping_context,  # ì „ì²˜ë¦¬ ë§¤í•‘ ì»¨í…ìŠ¤íŠ¸
                    "mapped_draft_options": mapped_draft_options  # ì „ì²˜ë¦¬ ë§¤í•‘ í›„ ì˜µì…˜
                }
                self._save_transformation_results(
                    job_id, 
                    draft_options, 
                    transformed_options, 
                    bounded_context,
                    search_info=search_info
                )
            
            # ì´ë¦„ ê°œìˆ˜ ê³„ì‚° (BC ì „ì²´ ê¸°ì¤€)
            all_names = self._extract_names_from_draft(draft_options, bounded_context)
            total_names = len(all_names)
            
            return {
                "transformed_options": transformed_options,  # snake_case (ë‚´ë¶€ ì‚¬ìš©)
                "transformedOptions": transformed_options,   # camelCase (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜)
                "transformation_log": " â†’ ".join(transformation_logs),
                "transformationLog": " â†’ ".join(transformation_logs),
                "is_completed": True,
                "isCompleted": True
                # errorê°€ Noneì´ë©´ í•„ë“œ ìì²´ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ
            }
        
        except Exception as e:
            LoggingUtil.error("StandardTransformer", f"âŒ í‘œì¤€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {
                "transformed_options": draft_options,  # ì›ë³¸ ë°˜í™˜
                "transformedOptions": draft_options,   # camelCase (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜)
                "transformation_log": f"ë³€í™˜ ì‹¤íŒ¨: {str(e)}",
                "transformationLog": f"ë³€í™˜ ì‹¤íŒ¨: {str(e)}",
                "is_completed": False,
                "isCompleted": False,
                "error": str(e)
            }
    
    def _extract_names_from_draft(self, draft_options: List[Dict], bounded_context: Dict) -> List[str]:
        """
        Aggregate ì´ˆì•ˆì—ì„œ ëª¨ë“  ì´ë¦„ ì¶”ì¶œ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
        nameê³¼ aliasë¥¼ "name alias" í˜•íƒœë¡œ ë¬¶ì–´ì„œ ì²˜ë¦¬
        
        Returns:
            ì´ë¦„ ë¦¬ìŠ¤íŠ¸: ["Customer ê³ ê°", "Order ì£¼ë¬¸", "customer_id ê³ ê°ID", ...]
        """
        all_names = set()  # ëª¨ë“  ì´ë¦„ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ, "name alias" í˜•íƒœë¡œ ë¬¶ìŒ)
        
        # BoundedContext ì´ë¦„ ì¶”ê°€ ("name alias" í˜•íƒœ)
        bc_name = bounded_context.get("name", "")
        bc_alias = bounded_context.get("alias", "")
        if bc_name and bc_alias:
            all_names.add(f"{bc_name} {bc_alias}")
        elif bc_name:
            all_names.add(bc_name)
        elif bc_alias:
            all_names.add(bc_alias)
        
        for option in draft_options:
            structure = option.get("structure", [])
            
            for item in structure:
                # Aggregate ì´ë¦„ ("name alias" í˜•íƒœ)
                aggregate = item.get("aggregate", {})
                agg_name = aggregate.get("name", "")
                agg_alias = aggregate.get("alias", "")
                if agg_name and agg_alias:
                    all_names.add(f"{agg_name} {agg_alias}")
                elif agg_name:
                    all_names.add(agg_name)
                elif agg_alias:
                    all_names.add(agg_alias)
                
                # Enumeration ì´ë¦„ ("name alias" í˜•íƒœ)
                enumerations = item.get("enumerations", [])
                for enum in enumerations:
                    enum_name = enum.get("name", "")
                    enum_alias = enum.get("alias", "")
                    if enum_name and enum_alias:
                        all_names.add(f"{enum_name} {enum_alias}")
                    elif enum_name:
                        all_names.add(enum_name)
                    elif enum_alias:
                        all_names.add(enum_alias)
                
                # ValueObject ì´ë¦„ ("name alias" í˜•íƒœ)
                value_objects = item.get("valueObjects", [])
                for vo in value_objects:
                    vo_name = vo.get("name", "")
                    vo_alias = vo.get("alias", "")
                    if vo_name and vo_alias:
                        all_names.add(f"{vo_name} {vo_alias}")
                    elif vo_name:
                        all_names.add(vo_name)
                    elif vo_alias:
                        all_names.add(vo_alias)
                    # referencedAggregateNameì€ ë³„ë„ë¡œ ì¶”ê°€
                    if vo.get("referencedAggregateName"):
                        all_names.add(vo["referencedAggregateName"])
                
                # previewAttributes í•„ë“œ ì´ë¦„ ("fieldName fieldAlias" í˜•íƒœ)
                preview_attributes = item.get("previewAttributes", [])
                for attr in preview_attributes:
                    if isinstance(attr, dict):
                        field_name = attr.get("fieldName", "")
                        field_alias = attr.get("fieldAlias", "")
                        if field_name and field_alias:
                            all_names.add(f"{field_name} {field_alias}")
                        elif field_name:
                            all_names.add(field_name)
                        elif field_alias:
                            all_names.add(field_alias)
                    else:
                        # backward compatibility
                        if attr:
                            all_names.add(str(attr))
                
                # ddlFields í•„ë“œ ì´ë¦„ ("fieldName fieldAlias" í˜•íƒœ)
                ddl_fields = item.get("ddlFields", [])
                for field in ddl_fields:
                    if isinstance(field, dict):
                        field_name = field.get("fieldName", "")
                        field_alias = field.get("fieldAlias", "")
                        if field_name and field_alias:
                            all_names.add(f"{field_name} {field_alias}")
                        elif field_name:
                            all_names.add(field_name)
                        elif field_alias:
                            all_names.add(field_alias)
                    else:
                        # backward compatibility
                        if field:
                            all_names.add(str(field))
        
        return list(all_names)
    
    def _extract_names_from_structure(self, draft_options: List[Dict], bounded_context: Dict) -> List[str]:
        """
        ë‹¨ì¼ structureì—ì„œ ì´ë¦„ ì¶”ì¶œ (structure ë‹¨ìœ„ ì²˜ë¦¬ìš©)
        nameê³¼ aliasë¥¼ "name alias" í˜•íƒœë¡œ ë¬¶ì–´ì„œ ì²˜ë¦¬
        
        Returns:
            ì´ë¦„ ë¦¬ìŠ¤íŠ¸: ["Customer ê³ ê°", "customer_id ê³ ê°ID", ...]
        """
        all_names = set()
        
        # BoundedContext ì´ë¦„ ì¶”ê°€
        bc_name = bounded_context.get("name", "")
        bc_alias = bounded_context.get("alias", "")
        if bc_name and bc_alias:
            all_names.add(f"{bc_name} {bc_alias}")
        elif bc_name:
            all_names.add(bc_name)
        elif bc_alias:
            all_names.add(bc_alias)
        
        for option in draft_options:
            structure = option.get("structure", [])
            
            for item in structure:
                # Aggregate ì´ë¦„
                aggregate = item.get("aggregate", {})
                agg_name = aggregate.get("name", "")
                agg_alias = aggregate.get("alias", "")
                if agg_name and agg_alias:
                    all_names.add(f"{agg_name} {agg_alias}")
                elif agg_name:
                    all_names.add(agg_name)
                elif agg_alias:
                    all_names.add(agg_alias)
                
                # Enumeration ì´ë¦„
                enumerations = item.get("enumerations", [])
                for enum in enumerations:
                    enum_name = enum.get("name", "")
                    enum_alias = enum.get("alias", "")
                    if enum_name and enum_alias:
                        all_names.add(f"{enum_name} {enum_alias}")
                    elif enum_name:
                        all_names.add(enum_name)
                    elif enum_alias:
                        all_names.add(enum_alias)
                
                # ValueObject ì´ë¦„
                value_objects = item.get("valueObjects", [])
                for vo in value_objects:
                    vo_name = vo.get("name", "")
                    vo_alias = vo.get("alias", "")
                    if vo_name and vo_alias:
                        all_names.add(f"{vo_name} {vo_alias}")
                    elif vo_name:
                        all_names.add(vo_name)
                    elif vo_alias:
                        all_names.add(vo_alias)
                    if vo.get("referencedAggregateName"):
                        all_names.add(vo["referencedAggregateName"])
                
                # previewAttributes í•„ë“œ ì´ë¦„
                preview_attributes = item.get("previewAttributes", [])
                for attr in preview_attributes:
                    if isinstance(attr, dict):
                        field_name = attr.get("fieldName", "")
                        field_alias = attr.get("fieldAlias", "")
                        if field_name and field_alias:
                            all_names.add(f"{field_name} {field_alias}")
                        elif field_name:
                            all_names.add(field_name)
                        elif field_alias:
                            all_names.add(field_alias)
                    else:
                        if attr:
                            all_names.add(str(attr))
                
                # ddlFields í•„ë“œ ì´ë¦„
                ddl_fields = item.get("ddlFields", [])
                for field in ddl_fields:
                    if isinstance(field, dict):
                        field_name = field.get("fieldName", "")
                        field_alias = field.get("fieldAlias", "")
                        if field_name and field_alias:
                            all_names.add(f"{field_name} {field_alias}")
                        elif field_name:
                            all_names.add(field_name)
                        elif field_alias:
                            all_names.add(field_alias)
                    else:
                        if field:
                            all_names.add(str(field))
        
        return list(all_names)
    
    def _build_standard_queries(
        self, 
        names: List[str], 
        bounded_context: Dict,
        draft_options: Optional[List[Dict]] = None
    ) -> List[StandardQuery]:
        """
        ì´ë¦„ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ìˆœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
        
        Args:
            names: ì¶”ì¶œëœ ëª¨ë“  ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            bounded_context: Bounded Context ì •ë³´
            draft_options: Aggregate Draft ì˜µì…˜ë“¤ (ì„ íƒì‚¬í•­)
            
        Returns:
            StandardQuery ë¦¬ìŠ¤íŠ¸
        """
        queries: List[StandardQuery] = []
        
        # ë„ë©”ì¸ íŒíŠ¸ ì¶”ì¶œ
        domain_hint = bounded_context.get("domain")
        if not domain_hint:
            domain_hint = bounded_context.get("name") or bounded_context.get("alias")
        
        # ëª¨ë“  ì´ë¦„ì„ ì¿¼ë¦¬ë¡œ ë³€í™˜ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
        for name in names:
            keyword = name.strip()
            if not keyword:
                continue
            
            queries.append(StandardQuery(
                query=keyword,
                domain_hint=domain_hint
            ))
        
        return queries
    
    def _build_queries(self, names: List[str], bounded_context: Dict) -> List[str]:
        """
        ì´ë¦„ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
        
        Args:
            names: ì¶”ì¶œëœ ì´ë¦„ë“¤
            bounded_context: Bounded Context ì •ë³´
            
        Returns:
            ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        # ê¸°ì¡´ ë°©ì‹ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
        queries = []
        bc_name = bounded_context.get("name", "")
        bc_alias = bounded_context.get("alias", "")
        
        for name in names:
            # ë°ì´í„°ë² ì´ìŠ¤ í‘œì¤€ ì¿¼ë¦¬
            queries.append(f"{name} aggregate table naming standard")
            queries.append(f"{name} database naming convention")
            
            # API í‘œì¤€ ì¿¼ë¦¬
            queries.append(f"{name} API endpoint naming standard")
            queries.append(f"{name} REST API naming convention")
            
            # ìš©ì–´ í‘œì¤€ ì¿¼ë¦¬
            queries.append(f"{name} terminology standard")
            queries.append(f"{name} domain terminology")
        
        # Bounded Context ê´€ë ¨ ì¿¼ë¦¬
        if bc_name:
            queries.append(f"{bc_name} bounded context naming standard")
        if bc_alias:
            queries.append(f"{bc_alias} ë„ë©”ì¸ í‘œì¤€ ìš©ì–´")
        
        return queries
    
    def _retrieve_relevant_standards(self, queries: List[str], 
                                   k_per_query: int = 5) -> List[Dict]:
        """
        ì¿¼ë¦¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ í‘œì¤€ ê²€ìƒ‰ (ê¸°ì¡´ ë°©ì‹ - í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
        
        Args:
            queries: ê²€ìƒ‰ ì¿¼ë¦¬ë“¤
            k_per_query: ì¿¼ë¦¬ë‹¹ ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ëœ í‘œì¤€ ì²­í¬ë“¤ (ì¤‘ë³µ ì œê±°)
        """
        if not self.rag_retriever:
            return []
        
        all_results = []
        seen_result_keys = set()  # (source + chunk_index)ë¡œ ì¤‘ë³µ ì œê±°
        total_results_found = 0  # ê²€ìƒ‰ ê²°ê³¼ ì´ ê°œìˆ˜ (ì¤‘ë³µ í¬í•¨)
        failed_queries = 0
        processed_queries = 0  # ì²˜ë¦¬ëœ ì¿¼ë¦¬ ìˆ˜
        
        # Vector Store ì´ˆê¸°í™” ìƒíƒœ í™•ì¸ (ê²€ìƒ‰ ì „)
        vectorstore_initialized = False
        if self.rag_retriever:
            vectorstore_initialized = self.rag_retriever._initialized and self.rag_retriever.vectorstore is not None
        
        # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        for idx, query in enumerate(queries):
            try:
                # ë°ì´í„°ë² ì´ìŠ¤ í‘œì¤€ ê²€ìƒ‰ (ì ìˆ˜ í•„í„°ë§ í¬í•¨, ì„ê³„ê°’ 0.3)
                # ìœ ì‚¬ë„ 0.3 ì´ìƒ = ê±°ë¦¬ 0.7 ì´í•˜ (ì¶©ë¶„íˆ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œ)
                # similarity = 1 - distance ë³€í™˜ ì‚¬ìš©
                db_results = self.rag_retriever.search_company_standards(
                    query, k=k_per_query, score_threshold=0.3
                )
                
                # API í‘œì¤€ ê²€ìƒ‰ (ì ìˆ˜ í•„í„°ë§ í¬í•¨, ì„ê³„ê°’ 0.3)
                api_results = self.rag_retriever.search_api_standards(
                    query, k=k_per_query, score_threshold=0.3
                )
                
                # ìš©ì–´ í‘œì¤€ ê²€ìƒ‰ (ì ìˆ˜ í•„í„°ë§ í¬í•¨, ì„ê³„ê°’ 0.3)
                term_results = self.rag_retriever.search_terminology_standards(
                    query, k=k_per_query, score_threshold=0.3
                )
                
                # ì²« ë²ˆì§¸ ì¿¼ë¦¬ì—ì„œ Vector Store ì´ˆê¸°í™” ì‹¤íŒ¨ ê°ì§€
                if idx == 0 and vectorstore_initialized and not db_results and not api_results and not term_results:
                    # Vector Storeê°€ ì´ˆê¸°í™”ë˜ì—ˆë‹¤ê³  í‘œì‹œë˜ì–´ ìˆì§€ë§Œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŒ
                    # ì‹¤ì œë¡œëŠ” ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ (ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ì†ìƒ ë“±)
                    LoggingUtil.warning("StandardTransformer", 
                                      "âš ï¸  Vector Store ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: ì´ˆê¸°í™” ìƒíƒœëŠ” ìˆìœ¼ë‚˜ ì‹¤ì œ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì¬ì¸ë±ì‹± ì‹œë„ë©ë‹ˆë‹¤")
                
                # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì§‘ê³„ (ì¤‘ë³µ í¬í•¨)
                total_found = len(db_results) + len(api_results) + len(term_results)
                total_results_found += total_found
                processed_queries += 1
                
                # ì¤‘ë³µ ì œê±°: (source + chunk_index)ë¡œ íŒë‹¨
                for result in db_results + api_results + term_results:
                    # README íŒŒì¼ ì œì™¸
                    metadata = result.get("metadata", {})
                    source = metadata.get("source", "")
                    if source:
                        source_name = Path(source).name.lower()
                        if source_name.startswith("readme"):
                            continue
                    
                    # ì¤‘ë³µ ì œê±°: source + chunk_index ì¡°í•©
                    chunk_index = metadata.get("chunk_index", "")
                    result_key = f"{source}::{chunk_index}"
                    
                    if result_key not in seen_result_keys:
                        seen_result_keys.add(result_key)
                        result_with_tracking = result.copy()
                        result_with_tracking["_matched_queries"] = [query]
                        all_results.append(result_with_tracking)
                    else:
                        # ì¤‘ë³µëœ ê²°ê³¼: ì¿¼ë¦¬ ëª©ë¡ì—ë§Œ ì¶”ê°€
                        for r in all_results:
                            r_metadata = r.get("metadata", {})
                            r_source = r_metadata.get("source", "")
                            r_chunk_index = r_metadata.get("chunk_index", "")
                            if f"{r_source}::{r_chunk_index}" == result_key:
                                if query not in r.get("_matched_queries", []):
                                    r["_matched_queries"].append(query)
                                break
            
            except Exception as e:
                failed_queries += 1
                continue
        
        return all_results
    
    def _retrieve_relevant_standards_with_categories(
        self, 
        standard_queries: List[StandardQuery],
        k_per_query: int = 3,  # top-k 3ê°œ ì‚¬ìš© (structure ë‹¨ìœ„ ì²˜ë¦¬)
        transformation_session_id: Optional[str] = None
    ) -> tuple:
        """
        StandardQuery ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ í‘œì¤€ ê²€ìƒ‰ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
        
        Args:
            standard_queries: í‘œì¤€ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤
            k_per_query: ì¿¼ë¦¬ë‹¹ ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            (ê²€ìƒ‰ëœ í‘œì¤€ ì²­í¬ë“¤, ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼) íŠœí”Œ
        """
        if not self.standard_service:
            # Fallback: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            queries = [sq["query"] for sq in standard_queries]
            results = self._retrieve_relevant_standards(queries, k_per_query)
            return (results, [])
        
        all_results = []
        seen_result_keys = set()  # (source + chunk_index)ë¡œ ì¤‘ë³µ ì œê±°
        total_results_found = 0
        failed_queries = 0
        processed_queries = 0
        query_search_results = []  # ê° ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ (summaryìš©)
        
        # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
        for sq in standard_queries:
            try:
                query = sq["query"]
                domain_hint = sq.get("domain_hint")
                
                # ë‹¨ì¼ ê²€ìƒ‰ ë©”ì„œë“œ ì‚¬ìš© (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
                found = self.standard_service.search_standards(
                    query, domain_hint=domain_hint, k=k_per_query, transformation_session_id=transformation_session_id
                )
                
                total_found = len(found)
                
                total_results_found += total_found
                processed_queries += 1
                
                # ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ ì¶”ì  (summaryìš©)
                # top-k 3ê°œ ëª¨ë‘ ì €ì¥ (LLMì´ ì°¸ê³ í•  í›„ë³´êµ°)
                if found:
                    # ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ top-3 ì„ íƒ
                    sorted_found = sorted(found, key=lambda x: x.get("score", 0.0), reverse=True)
                    top_results = sorted_found[:3]  # top-3
                    
                    # ê° ê²°ê³¼ì˜ structured_data ì¶”ì¶œ
                    results_list = []
                    for result_item in top_results:
                        metadata = result_item.get("metadata", {})
                        structured_data_str = metadata.get("structured_data", "")
                        structured_data_json = None
                        if structured_data_str:
                            try:
                                structured_data_json = json.loads(structured_data_str)
                            except:
                                pass
                        
                        results_list.append({
                            "similarity_score": result_item.get("score", 0.0),
                            "result": structured_data_json
                        })
                    
                    query_search_results.append({
                        "query": query,
                        "results": results_list  # top-3 ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
                    })
                else:
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ëŠ” summaryì— í¬í•¨í•˜ì§€ ì•ŠìŒ
                    pass
                
                # ì¤‘ë³µ ì œê±° ë° ê²°ê³¼ ë³€í™˜
                for item in found:
                    metadata = item.get("metadata", {})
                    source = metadata.get("source", "")
                    
                    # README íŒŒì¼ ì œì™¸
                    if source:
                        source_name = Path(source).name.lower()
                        if source_name.startswith("readme"):
                            continue
                    
                    # ì¤‘ë³µ ì œê±°: source + chunk_index ì¡°í•©
                    chunk_index = metadata.get("chunk_index", "")
                    result_key = f"{source}::{chunk_index}"
                    
                    if result_key not in seen_result_keys:
                        seen_result_keys.add(result_key)
                        # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)
                        result_dict = {
                            "content": item["content"],
                            "metadata": item["metadata"],
                            "score": item.get("score"),
                            "_matched_queries": [query]
                        }
                        all_results.append(result_dict)
                    else:
                        # ì¤‘ë³µëœ ê²°ê³¼: ì¿¼ë¦¬ ëª©ë¡ì—ë§Œ ì¶”ê°€
                        for r in all_results:
                            r_metadata = r.get("metadata", {})
                            r_source = r_metadata.get("source", "")
                            r_chunk_index = r_metadata.get("chunk_index", "")
                            if f"{r_source}::{r_chunk_index}" == result_key:
                                if query not in r.get("_matched_queries", []):
                                    r["_matched_queries"].append(query)
                                break
            
            except Exception as e:
                failed_queries += 1
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨ (query: {sq.get('query')}): {e}")
                continue
        
        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë¡œê·¸: ì±„íƒëœ í‘œì¤€ ì •ë³´ í‘œì‹œ
        if processed_queries > 0:
            # ì¤‘ë³µ ì œê±°ëœ ê³ ìœ  ê²°ê³¼ ìˆ˜ì™€ ê° ì¿¼ë¦¬ë³„ ì±„íƒ ìˆ˜
            unique_results = len(all_results)
            total_adopted = processed_queries  # ê° ì¿¼ë¦¬ë§ˆë‹¤ top-1 ì±„íƒ
            LoggingUtil.info("StandardTransformer", 
                           f"âœ… ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ: {processed_queries}ê°œ ì¿¼ë¦¬ ì²˜ë¦¬, {unique_results}ê°œ ê³ ìœ  í‘œì¤€ ë¬¸ì„œ ì±„íƒ (ì¤‘ë³µ ì œê±°), {len(query_search_results)}ê°œ ì¿¼ë¦¬ë³„ ê²°ê³¼ ì €ì¥")
            # ì±„íƒëœ í‘œì¤€ ì •ë³´ ìƒì„¸ ë¡œê·¸ (ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ)
            for i, result in enumerate(all_results[:10], 1):
                content = result.get("content", "")[:80]  # ìµœëŒ€ 80ì
                score = result.get("score", 0.0)
                queries = result.get("_matched_queries", [])
                query_count = len(queries)
                query_preview = ", ".join(queries[:3]) if queries else "N/A"
                if query_count > 3:
                    query_preview += f" ì™¸ {query_count - 3}ê°œ"
                metadata = result.get("metadata", {})
                source = Path(metadata.get("source", "")).name if metadata.get("source") else "unknown"
                LoggingUtil.info("StandardTransformer", 
                               f"   [{i}] {query_count}ê°œ ì¿¼ë¦¬ ë§¤ì¹­ â†’ ìœ ì‚¬ë„ {score:.3f} ({content}...) [ì¶œì²˜: {source}]")
            if len(all_results) > 10:
                LoggingUtil.info("StandardTransformer", 
                               f"   ... ì™¸ {len(all_results) - 10}ê°œ ê²°ê³¼ ìƒëµ")
            if failed_queries > 0:
                LoggingUtil.warning("StandardTransformer", f"âš ï¸ {failed_queries}ê°œ ì¿¼ë¦¬ ì‹¤íŒ¨")
        
        # ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ì €ì¥ (summary ìƒì„± ì‹œ ì‚¬ìš©, í•˜ìœ„ í˜¸í™˜ì„±)
        self._last_query_search_results = query_search_results
        
        # íŠœí”Œë¡œ ë°˜í™˜: (relevant_standards, query_search_results)
        return (all_results, query_search_results)
    
    # ============================================================================
    # Standard Mapping Builder (Terminology/Standard Mapping ë ˆì´ì–´)
    # ============================================================================
    
    def _build_global_standard_mapping_context(
        self, 
        draft_options: Optional[List[Dict]] = None,
        bounded_context: Optional[Dict] = None,
        transformation_session_id: Optional[str] = None,
        should_index_vectorstore: bool = False
    ) -> StandardMappingContext:
        """
        ì „ì²´ í‘œì¤€ ì›ë³¸ì„ ì§ì ‘ ì½ì–´ì„œ StandardMappingContext ìƒì„±
        (ìœ ì‚¬ë„ ê²€ìƒ‰ê³¼ ë¬´ê´€í•˜ê²Œ í•„ìˆ˜ ë§¤í•‘ì„ êµ¬ì„±)
        
        í•µì‹¬ ì›ì¹™:
        - í•„ìˆ˜ ë§¤í•‘ì€ RAG ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ì™€ ë¬´ê´€í•˜ê²Œ ì „ì²´ í‘œì¤€ ì›ë³¸ì—ì„œ ì§ì ‘ êµ¬ì„±
        - RAGëŠ” "ì°¸ê³ ìš© ì»¨í…ìŠ¤íŠ¸ ì œê³µìš©"ìœ¼ë¡œë§Œ ì‚¬ìš©
        - ì´ë ‡ê²Œ í•˜ë©´ thresholdë¥¼ ì˜¬ë¦¬ë“  ë‚´ë¦¬ë“  í•µì‹¬ ë§¤í•‘ì€ ì ˆëŒ€ ì•ˆ ê¹¨ì§
        
        Returns:
            StandardMappingContext: ë§¤í•‘ ì‚¬ì „
        """
        # ì´ˆê¸°í™”
        mapping: StandardMappingContext = {
            "table": {
                "entity_to_table": {},
                "table_standards": {},  # ëª¨ë“  í‘œì¤€ ë§¤í•‘ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
                "column_standards": {},  # ëª¨ë“  í‘œì¤€ ë§¤í•‘ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
                "domain_to_tables": {}
            },
            "domain": {
                "name_to_domain": {},
                "table_to_domain": {}
            },
            "column": {
                "column_desc_by_table": {},
                "desc_to_columns": {}
            },
            "api": {
                "resource_abbrev": {},
                "action_to_path": {},
                "http_method_by_action": {}
            }
        }
        
        # HTTP Method ê¸°ë³¸ ë§¤í•‘ (í‘œì¤€ ê·œì¹™)
        mapping["api"]["http_method_by_action"] = {
            "ìƒì„±": "POST",
            "ì¡°íšŒ": "GET",
            "ìˆ˜ì •": "PATCH",
            "ì‚­ì œ": "DELETE",
            "create": "POST",
            "read": "GET",
            "update": "PATCH",
            "delete": "DELETE"
        }
        
        # StandardLoaderê°€ ì—†ìœ¼ë©´ ë¹ˆ ë§¤í•‘ ë°˜í™˜
        if not self.standard_loader:
            LoggingUtil.warning("StandardTransformer", 
                              "âš ï¸  StandardLoaderê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¹ˆ ë§¤í•‘ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return mapping
        
        try:
            # ì „ì²´ í‘œì¤€ ë¬¸ì„œ ë¡œë“œ (ì—‘ì…€ íŒŒì¼ ì§ì ‘ ì½ê¸°)
            # ì‚¬ìš©ìë³„ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
            standards_path = self.user_standards_path if self.user_standards_path and self.user_standards_path.exists() else Config.COMPANY_STANDARDS_PATH
            
            if not standards_path.exists():
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  í‘œì¤€ ë¬¸ì„œ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {standards_path}")
                return mapping
            
            # ì—‘ì…€ íŒŒì¼ ì§ì ‘ ì½ê¸°
            try:
                import pandas as pd
            except ImportError:
                LoggingUtil.warning("StandardTransformer", 
                                  "âš ï¸  pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì—‘ì…€ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return mapping
            
            processed_files = 0
            processed_rows = 0
            
            # ì„¸ì…˜ ë ˆë²¨ì—ì„œ should_index ê²°ì • (ëª¨ë“  ì‹œíŠ¸ì— ë™ì¼í•˜ê²Œ ì ìš©)
            # ì‚¬ìš©ìë³„ Vector Store ì¸ë±ì‹± ìƒíƒœ í™•ì¸
            should_index = False
            if self.user_standards_path and self.user_standards_path.exists():
                # ì‚¬ìš©ìë³„ ë¬¸ì„œê°€ ìˆê³ , ì•„ì§ ì¸ë±ì‹±ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¸ë±ì‹±
                if should_index_vectorstore:
                    should_index = True
                    LoggingUtil.info("StandardTransformer", 
                                  f"ğŸ“ ì‚¬ìš©ìë³„ ë¬¸ì„œ ê°ì§€: ì¸ë±ì‹± ìˆ˜í–‰ (ê²½ë¡œ: {self.user_standards_path})")
                else:
                    LoggingUtil.info("StandardTransformer", 
                                  f"â™»ï¸  ì‚¬ìš©ìë³„ Vector Store ì¬ì‚¬ìš© (ì´ë¯¸ ì¸ë±ì‹±ë¨)")
            elif transformation_session_id:
                # ì´ ì„¸ì…˜ì—ì„œ ì•„ì§ ì¸ë±ì‹±í•˜ì§€ ì•Šì•˜ìœ¼ë©´ ì¸ë±ì‹±
                if transformation_session_id not in AggregateDraftStandardTransformer._base_standards_indexed:
                    should_index = True
                    AggregateDraftStandardTransformer._base_standards_indexed.add(transformation_session_id)
            else:
                # transformation_session_idê°€ ì—†ìœ¼ë©´ í•­ìƒ ì¸ë±ì‹± (ê¸°ì¡´ ë™ì‘ ìœ ì§€)
                should_index = True
            
            # ì¸ë±ì‹±í•  ë¬¸ì„œ ìˆ˜ì§‘ (ëª¨ë“  íŒŒì¼/ì‹œíŠ¸ ì²˜ë¦¬ í›„ í•œë²ˆì— ì¸ë±ì‹±)
            all_documents_to_index = []
            
            # Excel íŒŒì¼ ì²˜ë¦¬
            for file_path in standards_path.rglob('*.xlsx'):
                if file_path.name.lower().startswith('readme'):
                    continue
                
                try:
                    excel_file = pd.ExcelFile(file_path)
                    
                    for sheet_name in excel_file.sheet_names:
                        df = pd.read_excel(excel_file, sheet_name=sheet_name)
                        
                        if df.empty:
                            continue
                        
                        # ì‹œíŠ¸ ì´ë¦„ì´ë‚˜ ì»¬ëŸ¼ëª…ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , rowì˜ ëª¨ë“  ê°’ì„ ê·¸ëŒ€ë¡œ ì²˜ë¦¬
                        
                        # ëª¨ë“  ë¬¸ì„œëŠ” company_standards ë‚´ìš©ê³¼ ì´ˆì•ˆ ì •ë³´ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì¸ë±ì‹±
                        # "ê¸°ë³¸ í‘œì¤€ë¬¸ì„œ"ì™€ "ì´ˆì•ˆì •ë³´ í¬í•¨ ë¬¸ì„œ"ë¥¼ ë”°ë¡œ êµ¬ë¶„í•˜ì§€ ì•ŠìŒ
                        # íš¨ìœ¨ì„±: ì„¸ì…˜ë‹¹ í•œ ë²ˆë§Œ ì¸ë±ì‹± (ê°™ì€ ì„¸ì…˜ì˜ í›„ì† BCëŠ” ìŠ¤í‚µ)
                        # í•˜ì§€ë§Œ ëª¨ë“  ì‹œíŠ¸ëŠ” ì²˜ë¦¬í•´ì•¼ í•¨ (ì²« ë²ˆì§¸ ì‹œíŠ¸ì—ì„œë§Œ ì¸ë±ì‹±í•˜ëŠ” ê²ƒì´ ì•„ë‹˜)
                        
                        # ë””ë²„ê¹…: ì‹œíŠ¸ ì²˜ë¦¬ ë¡œê·¸
                        if should_index:
                            print(f"ğŸ“‹ [StandardTransformer] ì‹œíŠ¸ '{sheet_name}' ì¸ë±ì‹± ì‹œì‘ (í–‰ ìˆ˜: {len(df)})")
                        else:
                            print(f"â­ï¸  [StandardTransformer] ì‹œíŠ¸ '{sheet_name}' ì¸ë±ì‹± ìŠ¤í‚µ (ì´ë¯¸ ì¸ë±ì‹±ë¨)")
                        
                        # ì´ˆì•ˆ ì •ë³´ êµ¬ì„± (draft_context)
                        # ëª¨ë“  ë¬¸ì„œëŠ” ì´ˆì•ˆ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ semantic_text ìƒì„±
                        draft_context = None
                        if draft_options and bounded_context:
                            # ì´ˆì•ˆ Aggregate ì •ë³´ ì¶”ì¶œ
                            aggregates = []
                            for option in draft_options:
                                structure = option.get("structure", [])
                                for item in structure:
                                    aggregate = item.get("aggregate", {})
                                    preview_attrs = item.get("previewAttributes", [])
                                    value_objects = item.get("valueObjects", [])
                                    
                                    aggregates.append({
                                        "alias": aggregate.get("alias", ""),
                                        "name": aggregate.get("name", ""),
                                        "previewAttributes": preview_attrs,
                                        "valueObjects": value_objects
                                    })
                            
                            draft_context = {
                                "bounded_context": bounded_context,
                                "aggregates": aggregates
                            }
                        
                        # ë§¤í•‘ íŒŒì‹± ë° ì¸ë±ì‹± ë¬¸ì„œ ìˆ˜ì§‘
                        # ê° rowë§ˆë‹¤ ê°œë³„ Documentë¡œ ì¸ë±ì‹±
                        rows_list = [row for _, row in df.iterrows()]
                        
                        if not rows_list:
                            continue
                        
                        # ëª¨ë“  rowë¥¼ ì¸ë±ì‹± (ì²« ë²ˆì§¸ rowë„ ë°ì´í„°ë¡œ ì²˜ë¦¬)
                        for row_idx, row in enumerate(rows_list):
                            # ê° rowì˜ ê°’ë“¤ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ page_content ìƒì„± (í•œê¸€ í¬í•¨)
                            row_values = []
                            structured_data_by_column = {}  # ì¸ë±ì‹±ìš© structured_data
                            
                            # rowì˜ ëª¨ë“  ê°’ì„ ìˆœíšŒ (ì»¬ëŸ¼ëª… ë¬´ê´€)
                            for col_name, val in row.items():
                                if pd.notna(val) and str(val).strip():
                                    val_str = str(val).strip()
                                    row_values.append(val_str)
                                    
                                    # structured_data: ì»¬ëŸ¼ëª…ì„ í‚¤ë¡œ í•˜ê³ , í˜„ì¬ rowì˜ ê°’ì„ ë‹¨ì¼ ê°’ìœ¼ë¡œ ì €ì¥ (ì¸ë±ì‹±ìš©)
                                    structured_data_by_column[col_name] = val_str
                            
                            # page_content: í˜„ì¬ rowì˜ ê°’ë“¤ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²° (í•œê¸€ í¬í•¨)
                            page_content = " ".join(row_values) if row_values else ""
                            
                            processed_rows += 1
                            
                            # ë§¤í•‘ íŒŒì‹± (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ í…ìŠ¤íŠ¸ ìƒì„± (ë§¤í•‘ íŒŒì‹±ìš©)
                            text, structured_data = self.standard_loader._format_excel_row_as_standard_text(
                                row,
                                sheet_name,
                                draft_context=draft_context
                            )
                            
                            if structured_data:
                                # íŒŒì‹± (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
                                self._parse_single_row_data(structured_data, mapping)
                            
                            # ê° rowë§ˆë‹¤ ê°œë³„ Document ìƒì„±
                            if should_index and page_content:
                                doc_metadata = {
                                    "source": str(file_path),
                                    "sheet": sheet_name,
                                    "format": "excel",
                                    "structured_data": json.dumps(structured_data_by_column, ensure_ascii=False)
                                }
                                
                                doc = Document(
                                    page_content=page_content,
                                    metadata=doc_metadata
                                )
                                all_documents_to_index.append(doc)
                    
                    processed_files += 1
                
                except Exception as e:
                    LoggingUtil.warning("StandardTransformer", 
                                      f"âš ï¸  í‘œì¤€ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({file_path.name}): {e}")
                    continue
            
            # PowerPoint íŒŒì¼ ì²˜ë¦¬ (StandardLoader ì‚¬ìš©)
            for file_path in standards_path.rglob('*.pptx'):
                if file_path.name.lower().startswith('readme'):
                    continue
                
                try:
                    # StandardLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ PPT íŒŒì¼ ë¡œë“œ
                    ppt_documents = self.standard_loader._load_ppt(file_path)
                    
                    if should_index and ppt_documents:
                        # PPT ë¬¸ì„œë¥¼ ì¸ë±ì‹±ìš© Documentë¡œ ë³€í™˜
                        for doc in ppt_documents:
                            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ìœ ì§€í•˜ë©´ì„œ ì¸ë±ì‹±
                            all_documents_to_index.append(doc)
                            processed_rows += 1
                        
                        LoggingUtil.info("StandardTransformer", 
                                      f"âœ… PowerPoint íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {file_path.name} ({len(ppt_documents)}ê°œ ìŠ¬ë¼ì´ë“œ)")
                    
                    processed_files += 1
                
                except Exception as e:
                    LoggingUtil.warning("StandardTransformer", 
                                      f"âš ï¸  PowerPoint íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({file_path.name}): {e}")
                    continue
            
            # ëª¨ë“  íŒŒì¼/ì‹œíŠ¸ ì²˜ë¦¬ í›„ í•œë²ˆì— ì¸ë±ì‹± (ì„¸ì…˜ë‹¹ í•œ ë²ˆë§Œ)
            if all_documents_to_index and self.rag_retriever and should_index:
                try:
                    # Vector Store ì´ˆê¸°í™” ìƒíƒœ í™•ì¸ ë° ì¬ì´ˆê¸°í™” ì‹œë„
                    if not self.rag_retriever._initialized or not self.rag_retriever.vectorstore:
                        LoggingUtil.warning("StandardTransformer", 
                                          f"âš ï¸  Vector Storeê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ. ì¬ì´ˆê¸°í™” ì‹œë„...")
                        # ì¬ì´ˆê¸°í™” ì‹œë„
                        if hasattr(self.rag_retriever, 'vectorstore_path'):
                            try:
                                from src.project_generator.workflows.common.rag_retriever import RAGRetriever
                                self.rag_retriever = RAGRetriever(vectorstore_path=self.rag_retriever.vectorstore_path)
                                if self.rag_retriever._initialized and self.rag_retriever.vectorstore:
                                    LoggingUtil.info("StandardTransformer", "âœ… Vector Store ì¬ì´ˆê¸°í™” ì„±ê³µ")
                                else:
                                    LoggingUtil.warning("StandardTransformer", 
                                                      f"âš ï¸  Vector Store ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: _initialized={self.rag_retriever._initialized}")
                            except Exception as reinit_e:
                                LoggingUtil.warning("StandardTransformer", 
                                                  f"âš ï¸  Vector Store ì¬ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {reinit_e}")
                    
                    # ì¤‘ë³µ ì²´í¬ í™œì„±í™”í•˜ì—¬ ì¸ë±ì‹±
                    add_success = self.rag_retriever.add_documents(all_documents_to_index, check_duplicates=True)
                    if add_success:
                        LoggingUtil.info("StandardTransformer", 
                                       f"ğŸ“ í‘œì¤€ ë¬¸ì„œ {len(all_documents_to_index)}ê°œ Vector Storeì— ì¸ë±ì‹± ì™„ë£Œ")
                        
                        # Vector Storeê°€ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦ (ì¸ë±ì‹±ëœ ë¬¸ì„œì˜ ì‹¤ì œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰)
                        # ê²€ì¦ì€ ì„ íƒì ì´ë©°, ì‹¤íŒ¨í•´ë„ ì¸ë±ì‹±ì€ ì„±ê³µí•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
                        # ê²€ì¦ì€ ë””ë²„ê¹… ëª©ì ì´ë©°, ì‹¤ì œ ì‚¬ìš© ì‹œ RAG ê²€ìƒ‰ìœ¼ë¡œ í™•ì¸ë¨
                        try:
                            # ê²€ì¦ì€ ì„ íƒì ìœ¼ë¡œë§Œ ìˆ˜í–‰ (ë””ë²„ê·¸ ëª¨ë“œë‚˜ íŠ¹ì • ì¡°ê±´ì—ì„œë§Œ)
                            # ì‹¤ì œ ê²€ìƒ‰ ê¸°ëŠ¥ì€ RAG ê²€ìƒ‰ ì‹œ í™•ì¸ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìŠ¤í‚µ
                            # ì¸ë±ì‹±ì´ ì„±ê³µí–ˆìœ¼ë©´ Vector StoreëŠ” ì •ìƒì ìœ¼ë¡œ ì‘ë™í•  ê²ƒìœ¼ë¡œ ê°€ì •
                            LoggingUtil.info("StandardTransformer", 
                                           "â„¹ï¸  Vector Store ê²€ì¦ ìŠ¤í‚µ: ì¸ë±ì‹± ì„±ê³µ (ê²€ìƒ‰ ê¸°ëŠ¥ì€ RAG ê²€ìƒ‰ ì‹œ í™•ì¸ë¨)")
                        except Exception as verify_e:
                            # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ì¸ë±ì‹±ì€ ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ê²½ê³ ë§Œ ì¶œë ¥
                            LoggingUtil.warning("StandardTransformer", 
                                              f"âš ï¸  Vector Store ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {verify_e}. ì¸ë±ì‹±ì€ ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰")
                            # ê²€ì¦ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ì¸ë±ì‹± ìƒíƒœëŠ” ìœ ì§€
                    else:
                        LoggingUtil.warning("StandardTransformer", 
                                          "âš ï¸  Vector Store ì¸ë±ì‹± ì‹¤íŒ¨: add_documentsê°€ False ë°˜í™˜")
                        # ì¸ë±ì‹± ìƒíƒœ ì œê±°
                        if self.user_id and transformation_session_id:
                            user_key = (self.user_id, transformation_session_id)
                            AggregateDraftStandardTransformer._user_vectorstores_indexed.discard(user_key)
                        elif transformation_session_id:
                            AggregateDraftStandardTransformer._base_standards_indexed.discard(transformation_session_id)
                except Exception as e:
                    LoggingUtil.warning("StandardTransformer", 
                                      f"âš ï¸  Vector Store ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    # ì¸ë±ì‹± ìƒíƒœ ì œê±°
                    if self.user_id and transformation_session_id:
                        user_key = (self.user_id, transformation_session_id)
                        AggregateDraftStandardTransformer._user_vectorstores_indexed.discard(user_key)
                    elif transformation_session_id:
                        AggregateDraftStandardTransformer._base_standards_indexed.discard(transformation_session_id)
        
        except Exception as e:
            LoggingUtil.warning("StandardTransformer", 
                              f"âš ï¸  ì „ì²´ í‘œì¤€ ë§¤í•‘ êµ¬ì„± ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        
        # ë¡œê¹…
        table_count = len(mapping["table"]["entity_to_table"])
        column_count = sum(len(cols) for cols in mapping["column"]["column_desc_by_table"].values())
        
        LoggingUtil.info("StandardTransformer", 
                       f"ğŸ“‹ í‘œì¤€ ë§¤í•‘ ì»¨í…ìŠ¤íŠ¸ ìƒì„±: í…Œì´ë¸” {table_count}ê°œ, í•„ë“œ {column_count}ê°œ")
        
        return mapping
    
    def _build_standard_mapping_context(self, relevant_standards: List[Dict]) -> StandardMappingContext:
        """
        ê²€ìƒ‰ëœ í‘œì¤€ ë¬¸ì„œë“¤ë¡œë¶€í„° StandardMappingContext ìƒì„±
        
        Args:
            relevant_standards: RAGë¡œ ê²€ìƒ‰ëœ í‘œì¤€ ì²­í¬ë“¤
            
        Returns:
            StandardMappingContext: ë§¤í•‘ ì‚¬ì „
        """
        # ì´ˆê¸°í™”
        mapping: StandardMappingContext = {
            "table": {
                "entity_to_table": {},
                "table_standards": {},  # ëª¨ë“  í‘œì¤€ ë§¤í•‘ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
                "column_standards": {},  # ëª¨ë“  í‘œì¤€ ë§¤í•‘ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
                "domain_to_tables": {}
            },
            "domain": {
                "name_to_domain": {},
                "table_to_domain": {}
            },
            "column": {
                "column_desc_by_table": {},
                "desc_to_columns": {}
            },
            "api": {
                "resource_abbrev": {},
                "action_to_path": {},
                "http_method_by_action": {}
            }
        }
        
        # HTTP Method ê¸°ë³¸ ë§¤í•‘ (í‘œì¤€ ê·œì¹™)
        mapping["api"]["http_method_by_action"] = {
            "ìƒì„±": "POST",
            "ì¡°íšŒ": "GET",
            "ìˆ˜ì •": "PATCH",
            "ì‚­ì œ": "DELETE",
            "create": "POST",
            "read": "GET",
            "update": "PATCH",
            "delete": "DELETE"
        }
        
        # ê° í‘œì¤€ ë¬¸ì„œ íŒŒì‹±
        parsed_count = 0
        skipped_count = 0
        
        for std in relevant_standards:
            metadata = std.get("metadata", {})
            structured_data_str = metadata.get("structured_data", "")
            
            if not structured_data_str:
                skipped_count += 1
                continue
            
            try:
                # structured_dataëŠ” ì¸ë±ì‹± ë°©ì‹ì— ë”°ë¼ í‚¤ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
                # ë‹¨ì¼ ê°’ í˜•ì‹: {"ì»¬ëŸ¼ëª…1": "ê°’1", "ì»¬ëŸ¼ëª…2": "ê°’2"} (ë‹¨ì¼ ê°’)
                # ë°°ì—´ í˜•ì‹: {"ì»¬ëŸ¼ëª…1": ["ê°’1", "ê°’2"], "ì»¬ëŸ¼ëª…2": ["ê°’3", "ê°’4"]} (ë°°ì—´)
                structured_data = json.loads(structured_data_str)
                
                if not isinstance(structured_data, dict):
                    skipped_count += 1
                    continue
                
                # ë‹¨ì¼ ê°’ í˜•ì‹ì¸ì§€ ë°°ì—´ í˜•ì‹ì¸ì§€ í™•ì¸
                is_array_format = any(isinstance(v, list) for v in structured_data.values() if v)
                
                if is_array_format:
                    # ë°°ì—´ í˜•ì‹: ê° ì»¬ëŸ¼ì˜ ë°°ì—´ ê¸¸ì´ ì¤‘ ìµœëŒ€ê°’ì„ êµ¬í•˜ì—¬ row ê°œìˆ˜ íŒŒì•…
                    max_rows = max((len(v) for v in structured_data.values() if isinstance(v, list)), default=0)
                    
                    # ê° rowë¥¼ ì¬êµ¬ì„±í•˜ì—¬ íŒŒì‹±
                    for i in range(max_rows):
                        row_data = {}
                        for key, values in structured_data.items():
                            if isinstance(values, list) and i < len(values):
                                row_data[key] = values[i]
                        
                        if not row_data:
                            continue
                        
                        # íŒŒì‹± (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
                        self._parse_single_row_data(row_data, mapping)
                        parsed_count += 1
                else:
                    # ë‹¨ì¼ ê°’ í˜•ì‹: ë°”ë¡œ íŒŒì‹± (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
                    self._parse_single_row_data(structured_data, mapping)
                    parsed_count += 1
            
            except json.JSONDecodeError as e:
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  í‘œì¤€ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                skipped_count += 1
                continue
            except Exception as e:
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  í‘œì¤€ ë§¤í•‘ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                skipped_count += 1
                continue
        
        # ë¡œê¹…
        table_count = len(mapping["table"]["entity_to_table"])
        domain_count = len(mapping["domain"]["name_to_domain"])
        column_count = sum(len(cols) for cols in mapping["column"]["column_desc_by_table"].values())
        column_standards_count = len(mapping["table"]["column_standards"])
        api_count = len(mapping["api"]["resource_abbrev"])
        
        LoggingUtil.info("StandardTransformer", 
                       f"ğŸ“‹ StandardMappingContext ìƒì„± ì™„ë£Œ: "
                       f"í…Œì´ë¸” {table_count}ê°œ, ë„ë©”ì¸ {domain_count}ê°œ, ì»¬ëŸ¼ {column_count}ê°œ, API {api_count}ê°œ")
        LoggingUtil.info("StandardTransformer", 
                       f"   íŒŒì‹±: {parsed_count}ê°œ ì„±ê³µ, {skipped_count}ê°œ ìŠ¤í‚µ (ì´ {len(relevant_standards)}ê°œ í‘œì¤€ ë¬¸ì„œ)")
        
        return mapping
    
    def _parse_single_row_data(self, row_data: Dict, mapping: StandardMappingContext):
        """ë‹¨ì¼ row ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ì—¬ ë§¤í•‘ì— ì¶”ê°€ (RAG ê²€ìƒ‰ ê²°ê³¼ìš©)
        
        Args:
            row_data: structured_data ë”•ì…”ë„ˆë¦¬ (ê°’ íŒ¨í„´ë§Œìœ¼ë¡œ ì¶”ë¡ )
            mapping: ë§¤í•‘ ì»¨í…ìŠ¤íŠ¸
        """
        # í‚¤ ì´ë¦„ì„ í•˜ë“œì½”ë”©í•˜ì§€ ì•Šê³ , ê°’ íŒ¨í„´ë§Œìœ¼ë¡œ ì¶”ë¡ 
        korean_name = None
        english_name = None
        standard_name = None
        
        # ëª¨ë“  ê°’ì„ ìˆœíšŒí•˜ë©° íŒ¨í„´ìœ¼ë¡œ ì¶”ë¡  (í‚¤ ì´ë¦„ ë¬´ê´€)
        for key, value in row_data.items():
            if not value:
                continue
            
            # ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
            if isinstance(value, list):
                if len(value) == 0:
                    continue
                value = value[0]
            
            val_str = str(value).strip()
            if not val_str:
                continue
            
            # í•œê¸€ëª…: í•œê¸€ì´ í¬í•¨ëœ ê°’
            if not korean_name and any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in val_str):
                if len(val_str) <= 50:  # ë„ˆë¬´ ê¸´ ê°’ì€ ì œì™¸
                    korean_name = val_str
            
            # ì˜ë¬¸ëª… ë˜ëŠ” í‘œì¤€ëª…: ì˜ë¬¸ë§Œ í¬í•¨ëœ ê°’
            if val_str.isascii() and len(val_str) > 1:
                # ì˜ë¬¸ëª…: CamelCase ë˜ëŠ” ì¼ë°˜ ì˜ë¬¸ (í‘œì¤€ëª…ë³´ë‹¤ ì§§ê±°ë‚˜ CamelCase)
                if not english_name:
                    if '_' not in val_str and (val_str[0].isupper() or len(val_str) <= 20):
                        english_name = val_str
                
                # í‘œì¤€ëª…: snake_case ë˜ëŠ” ì–¸ë”ìŠ¤ì½”ì–´ í¬í•¨ (ì˜ë¬¸ëª…ë³´ë‹¤ ê¸´ ê²½ìš°ë„ ìˆìŒ)
                if not standard_name:
                    if '_' in val_str or (len(val_str) > 5 and val_str.islower()):
                        standard_name = val_str
        
        # ëª¨ë“  í‘œì¤€ì„ í…Œì´ë¸”ëª…ê³¼ ì»¬ëŸ¼ëª… ë§¤í•‘ì— ëª¨ë‘ ì¶”ê°€ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
        if korean_name and standard_name:
            parsed_row = {
                "korean_name": korean_name,
                "english_name": english_name,
                "table_name": standard_name
            }
            # í…Œì´ë¸”ëª… í‘œì¤€ íŒŒì‹± (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ì´ ëª¨ë‘ ì¶”ê°€)
            self._parse_table_name_standard(parsed_row, mapping)
        
        # ì»¬ëŸ¼ëª… í‘œì¤€ íŒŒì‹± (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ì´ ëª¨ë‘ ì¶”ê°€)
        if korean_name or english_name:
            self._parse_column_name_standard(row_data, mapping)
        
        # API í‘œì¤€ íŒŒì‹±: "/v" íŒ¨í„´ì´ í¬í•¨ëœ ê°’ì´ ìˆìœ¼ë©´ íŒŒì‹±
        has_api_pattern = any(
            '/v' in str(val) and re.search(r'/v\d+/', str(val))
            for val in row_data.values() if val
        )
        if has_api_pattern:
            self._parse_api_standard(row_data, mapping)
        
        # ìš©ì–´ í‘œì¤€ íŒŒì‹±: ì§§ì€ ì•½ì–´(2-5ì)ê°€ ìˆìœ¼ë©´ íŒŒì‹±
        has_terminology = any(
            str(val).strip().isascii() and 2 <= len(str(val).strip()) <= 5 and str(val).strip().isupper()
            for val in row_data.values() if val
        )
        if has_terminology:
            self._parse_terminology_standard(row_data, mapping)
    
    def _parse_table_name_standard(self, row: Dict, mapping: StandardMappingContext):
        """í…Œì´ë¸”ëª… í‘œì¤€ íŒŒì‹± (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ìŒ)
        
        ê°’ íŒ¨í„´ë§Œìœ¼ë¡œ ì¶”ë¡ í•˜ì—¬ ë§¤í•‘ì— ì¶”ê°€
        
        Args:
            row: í‘œì¤€ í–‰ ë°ì´í„° (structured_data ë”•ì…”ë„ˆë¦¬)
            mapping: ë§¤í•‘ ì»¨í…ìŠ¤íŠ¸
        """
        # í‚¤ ì´ë¦„ì„ í•˜ë“œì½”ë”©í•˜ì§€ ì•Šê³ , ê°’ íŒ¨í„´ë§Œìœ¼ë¡œ ì¶”ë¡ 
        entity_name = None
        table_name = None
        english_name = None
        domain = None
        
        # ëª¨ë“  ê°’ì„ ìˆœíšŒí•˜ë©° íŒ¨í„´ìœ¼ë¡œ ì¶”ë¡  (í‚¤ ì´ë¦„ ë¬´ê´€)
        for key, value in row.items():
            if not value:
                continue
            
            # ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
            if isinstance(value, list):
                if len(value) == 0:
                    continue
                value = value[0]
            
            val_str = str(value).strip()
            if not val_str:
                continue
            
            # entity_name: í•œê¸€ì´ í¬í•¨ëœ ê°’
            if not entity_name and any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in val_str):
                if len(val_str) <= 50:
                    entity_name = val_str
            
            # table_name: ì˜ë¬¸ snake_case ë˜ëŠ” CamelCase (prefix ë¬´ê´€)
            if not table_name and val_str.isascii() and len(val_str) > 1:
                if '_' in val_str or (val_str[0].isupper() and any(c.islower() for c in val_str)):
                    table_name = val_str
            
            # english_name: ì˜ë¬¸ CamelCase ë˜ëŠ” ì¼ë°˜ ì˜ë¬¸ (snake_caseê°€ ì•„ë‹Œ ê²½ìš°)
            if not english_name and val_str.isascii() and len(val_str) > 1:
                if '_' not in val_str and (val_str[0].isupper() or len(val_str) <= 20):
                    english_name = val_str
            
            # domain: ì§§ì€ ì˜ë¬¸ ì•½ì–´ (2-5ì) ë˜ëŠ” í•œê¸€ ì•½ì–´
            if not domain:
                if val_str.isascii() and 2 <= len(val_str) <= 5 and val_str.isupper():
                    domain = val_str
                elif any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in val_str) and len(val_str) <= 10:
                    domain = val_str
        
        if entity_name and table_name:
            # í•œê¸€/ì˜ë¬¸ ëª¨ë‘ ë§¤í•‘ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
            entity_clean = str(entity_name).strip()
            table_clean = str(table_name).strip()
            
            if entity_clean and table_clean:
                # ëª¨ë“  ë§¤í•‘ì— ì¶”ê°€ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì—†ì´ ëª¨ë‘ ì¶”ê°€)
                mapping["table"]["entity_to_table"][entity_clean] = table_clean
                mapping["table"]["table_standards"][entity_clean] = table_clean
                mapping["table"]["column_standards"][entity_clean] = table_clean
                
                # ì˜ë¬¸ëª… ë§¤í•‘
                if english_name:
                    english_clean = str(english_name).strip()
                    mapping["table"]["entity_to_table"][english_clean] = table_clean
                    mapping["table"]["table_standards"][english_clean] = table_clean
                    mapping["table"]["column_standards"][english_clean] = table_clean
                    
                    mapping["table"]["entity_to_table"][english_clean.lower()] = table_clean
                    mapping["table"]["entity_to_table"][english_clean.upper()] = table_clean
                    mapping["table"]["table_standards"][english_clean.lower()] = table_clean
                    mapping["table"]["table_standards"][english_clean.upper()] = table_clean
                    mapping["table"]["column_standards"][english_clean.lower()] = table_clean
                    mapping["table"]["column_standards"][english_clean.upper()] = table_clean
                # entity_nameì´ ì˜ë¬¸ì¸ ê²½ìš°ë„ ì²˜ë¦¬
                elif entity_clean.isascii():
                    mapping["table"]["entity_to_table"][entity_clean.lower()] = table_clean
                    mapping["table"]["entity_to_table"][entity_clean.upper()] = table_clean
                    mapping["table"]["table_standards"][entity_clean.lower()] = table_clean
                    mapping["table"]["table_standards"][entity_clean.upper()] = table_clean
                    mapping["table"]["column_standards"][entity_clean.lower()] = table_clean
                    mapping["table"]["column_standards"][entity_clean.upper()] = table_clean
        
        # domain -> table_name ê·¸ë£¹
        if domain and table_name:
            if domain not in mapping["table"]["domain_to_tables"]:
                mapping["table"]["domain_to_tables"][domain] = []
            if table_name not in mapping["table"]["domain_to_tables"][domain]:
                mapping["table"]["domain_to_tables"][domain].append(table_name)
            
            # table_name -> domain ì—­ë§¤í•‘
            mapping["domain"]["table_to_domain"][table_name] = domain
    
    def _parse_column_name_standard(self, row: Dict, mapping: StandardMappingContext):
        """ì»¬ëŸ¼ëª… í‘œì¤€ íŒŒì‹±
        
        structured_dataì˜ í‚¤ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì¶”ì¶œ
        """
        # í‚¤ ì´ë¦„ì„ í•˜ë“œì½”ë”©í•˜ì§€ ì•Šê³ , ê°’ íŒ¨í„´ë§Œìœ¼ë¡œ ì¶”ë¡ 
        entity_name = None
        standard_name = None
        english_name = None
        
        # ëª¨ë“  ê°’ì„ ìˆœíšŒí•˜ë©° íŒ¨í„´ìœ¼ë¡œ ì¶”ë¡  (í‚¤ ì´ë¦„ ë¬´ê´€)
        for key, value in row.items():
            if not value:
                continue
            
            # ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
            if isinstance(value, list):
                if len(value) == 0:
                    continue
                value = value[0]
            
            val_str = str(value).strip()
            if not val_str:
                continue
            
            # entity_name: í•œê¸€ì´ í¬í•¨ëœ ê°’
            if not entity_name and any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in val_str):
                if len(val_str) <= 50:
                    entity_name = val_str
            
            # standard_name: ì˜ë¬¸ snake_case (ì–¸ë”ìŠ¤ì½”ì–´ í¬í•¨)
            if not standard_name and val_str.isascii() and len(val_str) > 1:
                if '_' in val_str:
                    standard_name = val_str
            
            # english_name: ì˜ë¬¸ CamelCase ë˜ëŠ” ì¼ë°˜ ì˜ë¬¸ (snake_caseê°€ ì•„ë‹Œ ê²½ìš°)
            if not english_name and val_str.isascii() and len(val_str) > 1:
                if '_' not in val_str and (val_str[0].isupper() or len(val_str) <= 20):
                    english_name = val_str
        
        # column_standardsì— ë§¤í•‘ ì¶”ê°€ (í•„ë“œëª… ë³€í™˜ì— ì‚¬ìš©)
        if entity_name and standard_name:
            entity_clean = str(entity_name).strip()
            standard_clean = str(standard_name).strip()
            
            if entity_clean and standard_clean:
                # í•œê¸€ëª… -> í‘œì¤€ëª… ë§¤í•‘
                mapping["table"]["column_standards"][entity_clean] = standard_clean
        
        if english_name and standard_name:
            english_clean = str(english_name).strip()
            standard_clean = str(standard_name).strip()
            
            if english_clean and standard_clean:
                # ì˜ë¬¸ëª… -> í‘œì¤€ëª… ë§¤í•‘ (ëŒ€ì†Œë¬¸ì ë³€í˜• í¬í•¨)
                mapping["table"]["column_standards"][english_clean] = standard_clean
                mapping["table"]["column_standards"][english_clean.lower()] = standard_clean
                mapping["table"]["column_standards"][english_clean.upper()] = standard_clean
        
        # column_desc_by_tableì— ì¶”ê°€ (ë¡œê¹…ìš©, ì‹¤ì œ ë³€í™˜ì—ëŠ” ì‚¬ìš© ì•ˆ í•¨)
        if entity_name and standard_name:
            # table_nameì€ í‘œì¤€ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš©
            table_name = standard_name
            if table_name:
                if table_name not in mapping["column"]["column_desc_by_table"]:
                    mapping["column"]["column_desc_by_table"][table_name] = {}
                
                column_name = english_name or entity_name
                if column_name:
                    description = entity_name if any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in str(entity_name)) else None
                    mapping["column"]["column_desc_by_table"][table_name][column_name] = description or ""
                    
                    # ì—­ë°©í–¥ ë§¤í•‘ (ì„¤ëª… -> ì»¬ëŸ¼ëª…)
                    if description:
                        if description not in mapping["column"]["desc_to_columns"]:
                            mapping["column"]["desc_to_columns"][description] = []
                        if column_name not in mapping["column"]["desc_to_columns"][description]:
                            mapping["column"]["desc_to_columns"][description].append(column_name)
    
    def _parse_api_standard(self, row: Dict, mapping: StandardMappingContext):
        """API í‘œì¤€ íŒŒì‹±
        
        ì»¬ëŸ¼ëª…ì— ì „í˜€ ì˜ì¡´í•˜ì§€ ì•Šê³  ê°’ íŒ¨í„´ë§Œìœ¼ë¡œ ì¶”ì¶œ
        """
        api_format = None
        description = None
        
        # ëª¨ë“  ê°’ì„ ìˆœíšŒí•˜ë©° íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì¶œ (ì»¬ëŸ¼ëª… ë¬´ê´€)
        for col_name, col_value in row.items():
            if not col_value or not str(col_value).strip():
                continue
            
            col_value_str = str(col_value).strip()
            
            # api_format: "/v1/..." íŒ¨í„´ì´ í¬í•¨ëœ ê°’
            if not api_format:
                if '/v' in col_value_str and re.search(r'/v\d+/', col_value_str):
                    api_format = col_value_str
            
            # description: í•œê¸€ì´ í¬í•¨ëœ ê¸´ ì„¤ëª… ë˜ëŠ” "â†’" íŒ¨í„´ì´ í¬í•¨ëœ ê°’
            if not description:
                if 'â†’' in col_value_str or '->' in col_value_str:
                    description = col_value_str
                elif any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in col_value_str) and len(col_value_str) > 20:
                    description = col_value_str
        
        # API ê²½ë¡œ íŒ¨í„´ íŒŒì‹± (ì˜ˆ: "/v1/odr, /v1/dly, /v1/cst")
        if api_format:
            # "/v1/odr" -> "odr" ì¶”ì¶œ
            paths = re.findall(r'/v\d+/([^,\s]+)', api_format)
            for path in paths:
                # pathëŠ” "odr", "dly" ë“±
                # descriptionì—ì„œ "Orderâ†’odr" ê°™ì€ íŒ¨í„´ ì°¾ê¸°
                if description:
                    # "Orderâ†’odr" ë˜ëŠ” "Order -> odr" íŒ¨í„´ ì°¾ê¸°
                    patterns = re.findall(r'(\w+)\s*[â†’->]\s*(\w+)', description, re.IGNORECASE)
                    for concept, abbrev in patterns:
                        if abbrev.lower() == path.lower():
                            mapping["api"]["resource_abbrev"][concept] = abbrev
                            mapping["api"]["resource_abbrev"][concept.lower()] = abbrev
                            mapping["api"]["resource_abbrev"][concept.upper()] = abbrev
        
        # descriptionì—ì„œ ì§ì ‘ ë§¤í•‘ ì¶”ì¶œ
        if description:
            # "Orderâ†’odr, Deliveryâ†’dly" ê°™ì€ íŒ¨í„´
            patterns = re.findall(r'(\w+)\s*[â†’->]\s*(\w+)', description, re.IGNORECASE)
            for concept, abbrev in patterns:
                mapping["api"]["resource_abbrev"][concept] = abbrev
                mapping["api"]["resource_abbrev"][concept.lower()] = abbrev
                mapping["api"]["resource_abbrev"][concept.upper()] = abbrev
    
    def _parse_terminology_standard(self, row: Dict, mapping: StandardMappingContext):
        """ìš©ì–´ í‘œì¤€ íŒŒì‹± (ë„ë©”ì¸ ì½”ë“œ ë§¤í•‘)
        
        ì»¬ëŸ¼ëª…ì— ì „í˜€ ì˜ì¡´í•˜ì§€ ì•Šê³  ê°’ íŒ¨í„´ë§Œìœ¼ë¡œ ì¶”ì¶œ
        """
        terminology = None
        entity_name = None
        table_name = None
        
        # ëª¨ë“  ê°’ì„ ìˆœíšŒí•˜ë©° íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì¶œ (ì»¬ëŸ¼ëª… ë¬´ê´€)
        for col_name, col_value in row.items():
            if not col_value or not str(col_value).strip():
                continue
            
            col_value_str = str(col_value).strip()
            
            # terminology: ì§§ì€ ì˜ë¬¸ ì•½ì–´ (2-5ì, ëŒ€ë¬¸ì) ë˜ëŠ” í•œê¸€ ì•½ì–´
            if not terminology:
                if col_value_str.isascii() and 2 <= len(col_value_str) <= 5 and col_value_str.isupper():
                    terminology = col_value_str
                elif any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in col_value_str) and len(col_value_str) <= 10:
                    terminology = col_value_str
            
            # entity_name: í•œê¸€ì´ í¬í•¨ëœ ê°’
            if not entity_name:
                if any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in col_value_str):
                    if len(col_value_str) <= 50:
                        entity_name = col_value_str
            
            # table_name: ì˜ë¬¸ snake_case ë˜ëŠ” CamelCase (prefix ë¬´ê´€)
            if not table_name and col_value_str.isascii() and len(col_value_str) > 1:
                if '_' in col_value_str or (col_value_str[0].isupper() and any(c.islower() for c in col_value_str)):
                    table_name = col_value_str
        
        # ìš©ì–´ í‘œì¤€ì—ë„ í…Œì´ë¸”ëª… ì •ë³´ê°€ ìˆì„ ìˆ˜ ìˆìŒ (ë„ë©”ì¸í‘œì¤€ ì‹œíŠ¸)
        if entity_name and table_name:
            entity_clean = str(entity_name).strip()
            table_clean = str(table_name).strip()
            
            if entity_clean and table_clean:
                # í…Œì´ë¸”ëª… ë§¤í•‘ë„ ì¶”ê°€
                mapping["table"]["entity_to_table"][entity_clean] = table_clean
                if entity_clean.isascii():
                    mapping["table"]["entity_to_table"][entity_clean.lower()] = table_clean
                    mapping["table"]["entity_to_table"][entity_clean.upper()] = table_clean
        
        if terminology and entity_name:
            # entity_name (í•œê¸€/ì˜ë¬¸) -> terminology (ë„ë©”ì¸ ì½”ë“œ)
            mapping["domain"]["name_to_domain"][entity_name] = terminology
            # ì˜ë¬¸ëª…ë„ ë§¤í•‘
            if entity_name.isascii():
                mapping["domain"]["name_to_domain"][entity_name.lower()] = terminology
                mapping["domain"]["name_to_domain"][entity_name.upper()] = terminology
    
    def _count_fields(self, draft_options: List[Dict]) -> List[str]:
        """í•„ë“œëª… ëª©ë¡ ì¶”ì¶œ (ë³€í™˜ ì „/í›„ ë¹„êµìš©)"""
        field_names = []
        for option in draft_options:
            structure = option.get("structure", [])
            for item in structure:
                preview_attrs = item.get("previewAttributes", [])
                for attr in preview_attrs:
                    if isinstance(attr, dict):
                        field_name = attr.get("fieldName", "")
                        if field_name:
                            field_names.append(field_name)
                ddl_fields = item.get("ddlFields", [])
                for field in ddl_fields:
                    if isinstance(field, dict):
                        field_name = field.get("fieldName", "")
                        if field_name:
                            field_names.append(field_name)
        return field_names
    
    def _apply_standard_mappings(self, draft_options: List[Dict], 
                                 mapping: StandardMappingContext) -> List[Dict]:
        """
        StandardMappingContextë¥¼ ì‚¬ìš©í•˜ì—¬ draft_optionsì— deterministic ë£° ì ìš©
        
        Args:
            draft_options: ì›ë³¸ ì˜µì…˜ë“¤
            mapping: StandardMappingContext
            
        Returns:
            ë§¤í•‘ì´ ì ìš©ëœ ì˜µì…˜ë“¤ (ìƒˆë¡œìš´ ë³µì‚¬ë³¸)
        """
        import copy
        mapped_options = copy.deepcopy(draft_options)
        
        applied_count = 0
        
        for option in mapped_options:
            structure = option.get("structure", [])
            
            for item in structure:
                aggregate = item.get("aggregate", {})
                alias = aggregate.get("alias", "")  # í•œê¸€ ì´ë¦„ (ì˜ˆ: "ì£¼ë¬¸ ë§ˆìŠ¤í„°", "ì¿ í° ë§ˆìŠ¤í„°")
                name = aggregate.get("name", "")  # ì˜ë¬¸ ì´ë¦„ (ì˜ˆ: "Order", "Coupon")
                
                # ============================================================
                # ê° aggregateì— ëŒ€í•´ ëª¨ë“  ë³€í™˜ ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰
                # (í…Œì´ë¸”ëª…, Enum, VO, í•„ë“œëª…ì€ ë…ë¦½ì ì¸ ì‘ì—…ì´ë¯€ë¡œ ëª¨ë‘ ì‹¤í–‰)
                # ============================================================
                
                # 1. Aggregate í…Œì´ë¸”ëª… ì¹˜í™˜: alias -> table_name (ì •í™• ë§¤ì¹­)
                # âœ… table_standards ì‚¬ìš© (m_ prefixë§Œ)
                if alias:
                    alias_clean = str(alias).strip()
                    if alias_clean in mapping["table"]["table_standards"]:
                        new_table_name = mapping["table"]["table_standards"][alias_clean]
                        if aggregate.get("name") != new_table_name:
                            aggregate["name"] = new_table_name
                            applied_count += 1
                
                # 2. ì˜ë¬¸ëª…ìœ¼ë¡œë„ í…Œì´ë¸”ëª… ì°¾ê¸° (name -> table_name)
                if name:
                    name_variants = [name, name.lower(), name.upper(), name.capitalize()]
                    for variant in name_variants:
                        if variant in mapping["table"]["table_standards"]:
                            new_table_name = mapping["table"]["table_standards"][variant]
                            if aggregate.get("name") != new_table_name:
                                aggregate["name"] = new_table_name
                                applied_count += 1
                            break
                
                # 3. Enumeration ì„ ì²˜ë¦¬ (í•œê¸€ alias ë˜ëŠ” ì˜ë¬¸ name ë§¤ì¹­)
                # âœ… table_standards ì‚¬ìš© (m_ prefixë§Œ, fld_ ìë™ ì œì™¸ë¨)
                enumerations = item.get("enumerations", [])
                for enum_item in enumerations:
                    enum_alias = enum_item.get("alias", "")
                    enum_name = enum_item.get("name", "")
                    
                    # alias (í•œê¸€) ë§¤ì¹­ ì‹œë„
                    if enum_alias:
                        enum_alias_clean = str(enum_alias).strip()
                        if enum_alias_clean in mapping["table"]["table_standards"]:
                            new_enum_name = mapping["table"]["table_standards"][enum_alias_clean]
                            if enum_item.get("name") != new_enum_name:
                                enum_item["name"] = new_enum_name
                                applied_count += 1
                                continue  # ì´ continueëŠ” enum_item ë£¨í”„ ë‚´ë¶€ì´ë¯€ë¡œ ë‹¤ìŒ enumìœ¼ë¡œ ë„˜ì–´ê° (ì •ìƒ)
                    
                    # name (ì˜ë¬¸) ë§¤ì¹­ ì‹œë„
                    if enum_name:
                        name_variants = [enum_name, enum_name.lower(), enum_name.upper(), enum_name.capitalize()]
                        for variant in name_variants:
                            if variant in mapping["table"]["table_standards"]:
                                new_enum_name = mapping["table"]["table_standards"][variant]
                                if enum_item.get("name") != new_enum_name:
                                    enum_item["name"] = new_enum_name
                                    applied_count += 1
                                break
                
                # 4. ValueObject ì„ ì²˜ë¦¬ (í•œê¸€ alias ë˜ëŠ” ì˜ë¬¸ name ë§¤ì¹­)
                # âœ… table_standards ì‚¬ìš© (m_ prefixë§Œ, fld_ ìë™ ì œì™¸ë¨)
                value_objects = item.get("valueObjects", [])
                for vo_item in value_objects:
                    vo_alias = vo_item.get("alias", "")
                    vo_name = vo_item.get("name", "")
                    
                    # alias (í•œê¸€) ë§¤ì¹­ ì‹œë„
                    if vo_alias:
                        vo_alias_clean = str(vo_alias).strip()
                        if vo_alias_clean in mapping["table"]["table_standards"]:
                            new_vo_name = mapping["table"]["table_standards"][vo_alias_clean]
                            if vo_item.get("name") != new_vo_name:
                                vo_item["name"] = new_vo_name
                                applied_count += 1
                                continue  # ì´ continueëŠ” vo_item ë£¨í”„ ë‚´ë¶€ì´ë¯€ë¡œ ë‹¤ìŒ voë¡œ ë„˜ì–´ê° (ì •ìƒ)
                    
                    # name (ì˜ë¬¸) ë§¤ì¹­ ì‹œë„
                    if vo_name:
                        name_variants = [vo_name, vo_name.lower(), vo_name.upper(), vo_name.capitalize()]
                        for variant in name_variants:
                            if variant in mapping["table"]["table_standards"]:
                                new_vo_name = mapping["table"]["table_standards"][variant]
                                if vo_item.get("name") != new_vo_name:
                                    vo_item["name"] = new_vo_name
                                    applied_count += 1
                                break
                
                # 5. í•„ë“œëª… ì¹˜í™˜: previewAttributes, ddlFields
                # âœ… column_standards ì‚¬ìš©
                # âš ï¸ ì¤‘ìš”: í…Œì´ë¸”ëª… ë³€í™˜ê³¼ ë¬´ê´€í•˜ê²Œ í•­ìƒ ì‹¤í–‰ë˜ì–´ì•¼ í•¨
                preview_attrs = item.get("previewAttributes", [])
                
                for attr in preview_attrs:
                    if isinstance(attr, dict):
                        field_name = attr.get("fieldName", "")
                        if field_name:
                            field_name_clean = str(field_name).strip()
                            original_field_name = field_name_clean
                            
                            # ë§¤í•‘ í™•ì¸
                            if field_name_clean in mapping["table"]["column_standards"]:
                                new_field_name = mapping["table"]["column_standards"][field_name_clean]
                                attr["fieldName"] = new_field_name
                                applied_count += 1
                            else:
                                # ì˜ë¬¸ í•„ë“œëª…ì˜ ë³€í˜•ë„ ì‹œë„ (camelCase -> snake_case ë³€í™˜ í¬í•¨)
                                field_name_variants = [
                                    field_name_clean.lower(),
                                    field_name_clean.upper(),
                                    field_name_clean
                                ]
                                
                                # camelCaseë¥¼ snake_caseë¡œ ë³€í™˜í•œ ë³€í˜•ë„ ì¶”ê°€
                                import re
                                snake_case = re.sub(r'(?<!^)(?=[A-Z])', '_', field_name_clean).lower()
                                if snake_case != field_name_clean.lower():
                                    field_name_variants.append(snake_case)
                                
                                matched = False
                                for variant in field_name_variants:
                                    if variant in mapping["table"]["column_standards"]:
                                        new_field_name = mapping["table"]["column_standards"][variant]
                                        attr["fieldName"] = new_field_name
                                        applied_count += 1
                                        matched = True
                                        break
                
                # 6. DDL í•„ë“œëª… ì¹˜í™˜
                # âœ… column_standards ì‚¬ìš©
                ddl_fields = item.get("ddlFields", [])
                for field in ddl_fields:
                    if isinstance(field, dict):
                        field_name = field.get("fieldName", "")
                        if field_name:
                            field_name_clean = str(field_name).strip()
                            
                            # ì˜ë¬¸ í•„ë“œëª… ë§¤í•‘ ì‹œë„
                            if field_name_clean in mapping["table"]["column_standards"]:
                                new_field_name = mapping["table"]["column_standards"][field_name_clean]
                                field["fieldName"] = new_field_name
                                applied_count += 1
                            else:
                                # ì˜ë¬¸ í•„ë“œëª…ì˜ ë³€í˜•ë„ ì‹œë„ (camelCase -> snake_case ë³€í™˜ í¬í•¨)
                                field_name_variants = [
                                    field_name_clean.lower(),
                                    field_name_clean.upper(),
                                    field_name_clean
                                ]
                                
                                # camelCaseë¥¼ snake_caseë¡œ ë³€í™˜í•œ ë³€í˜•ë„ ì¶”ê°€
                                import re
                                snake_case = re.sub(r'(?<!^)(?=[A-Z])', '_', field_name_clean).lower()
                                if snake_case != field_name_clean.lower():
                                    field_name_variants.append(snake_case)
                                
                                matched = False
                                for variant in field_name_variants:
                                    if variant in mapping["table"]["column_standards"]:
                                        new_field_name = mapping["table"]["column_standards"][variant]
                                        field["fieldName"] = new_field_name
                                        applied_count += 1
                                        matched = True
                                        break
        
        if applied_count > 0:
            LoggingUtil.info("StandardTransformer", 
                           f"âœ… ì„ ì²˜ë¦¬ ë§¤í•‘ ì™„ë£Œ: {applied_count}ê°œ í•­ëª© ë³€í™˜ë¨")
        else:
            LoggingUtil.info("StandardTransformer", 
                           "â„¹ï¸  ì„ ì²˜ë¦¬ ë§¤í•‘: ë§¤ì¹­ë˜ëŠ” ë£° ì—†ìŒ (LLM ì²˜ë¦¬)")
        
        return mapped_options
    
    def _strip_unnecessary_fields_for_llm(self, draft_options: List[Dict]) -> List[Dict]:
        """
        LLM ìš”ì²­ ì „ì— ë„¤ì´ë° ë³€í™˜ê³¼ ê´€ë ¨ ì—†ëŠ” í•„ë“œ ì œê±° (í† í° ì ˆì•½)
        
        ì œê±°í•  í•„ë“œ:
        - refs: ì¶”ì ì„± ì •ë³´ (LLM ë³€í™˜ì— ë¶ˆí•„ìš”)
        - description, pros, cons: ë³€í™˜ ëŒ€ìƒì´ ì•„ë‹˜
        - ddlFields: ì „ì²´ DDL ì •ë³´ (í•„ìš”ì‹œ previewAttributesë¡œ ì¶©ë¶„)
        
        ìœ ì§€í•  í•„ë“œ:
        - name, alias: ë³€í™˜ ëŒ€ìƒ
        - structure, aggregate, enumerations, valueObjects: êµ¬ì¡° ì •ë³´
        - previewAttributes: í•„ë“œëª… ë³€í™˜ìš© (ê°„ì†Œí™” ê°€ëŠ¥)
        """
        import copy
        stripped_options = []
        
        for option in draft_options:
            stripped = {}
            
            # boundedContext: aggregatesë§Œ ìœ ì§€ (description ì œê±°)
            if "boundedContext" in option:
                bc = option["boundedContext"]
                stripped["boundedContext"] = {
                    "name": bc.get("name"),
                    "alias": bc.get("alias"),
                    "aggregates": []
                }
                for agg in bc.get("aggregates", []):
                    stripped["boundedContext"]["aggregates"].append({
                        "name": agg.get("name"),
                        "alias": agg.get("alias")
                        # refs ì œê±°
                    })
            
            # structure: ë„¤ì´ë° ì •ë³´ë§Œ ìœ ì§€
            if "structure" in option:
                stripped["structure"] = []
                for item in option["structure"]:
                    stripped_item = {}
                    
                    # aggregate
                    if "aggregate" in item:
                        agg = item["aggregate"]
                        stripped_item["aggregate"] = {
                            "name": agg.get("name"),
                            "alias": agg.get("alias")
                            # refs ì œê±°
                        }
                    
                    # enumerations
                    if "enumerations" in item:
                        stripped_item["enumerations"] = []
                        for enum in item["enumerations"]:
                            stripped_item["enumerations"].append({
                                "name": enum.get("name"),
                                "alias": enum.get("alias")
                                # refs ì œê±°
                            })
                    
                    # valueObjects
                    if "valueObjects" in item:
                        stripped_item["valueObjects"] = []
                        for vo in item["valueObjects"]:
                            stripped_vo = {
                                "name": vo.get("name"),
                                "alias": vo.get("alias")
                                # refs ì œê±°
                            }
                            if "referencedAggregateName" in vo:
                                stripped_vo["referencedAggregateName"] = vo["referencedAggregateName"]
                            stripped_item["valueObjects"].append(stripped_vo)
                    
                    # previewAttributes: fieldNameê³¼ fieldAlias ìœ ì§€ (aliasëŠ” ë³€í™˜ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ë³´ì¡´)
                    if "previewAttributes" in item:
                        stripped_item["previewAttributes"] = []
                        for attr in item["previewAttributes"]:
                            if isinstance(attr, dict):
                                stripped_attr = {
                                    "fieldName": attr.get("fieldName")
                                    # refs ì œê±°
                                }
                                # fieldAliasëŠ” ë³€í™˜ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ë³´ì¡´
                                if "fieldAlias" in attr:
                                    stripped_attr["fieldAlias"] = attr.get("fieldAlias")
                                stripped_item["previewAttributes"].append(stripped_attr)
                            else:
                                stripped_item["previewAttributes"].append(attr)
                    
                    # ddlFields: fieldNameê³¼ fieldAlias ìœ ì§€ (aliasëŠ” ë³€í™˜ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ë³´ì¡´)
                    if "ddlFields" in item:
                        stripped_item["ddlFields"] = []
                        for field in item["ddlFields"]:
                            if isinstance(field, dict):
                                # ìµœì†Œ ì •ë³´ë§Œ ìœ ì§€
                                stripped_field = {
                                    "fieldName": field.get("fieldName"),
                                    "className": field.get("className")
                                }
                                if "type" in field:
                                    stripped_field["type"] = field["type"]
                                # fieldAliasëŠ” ë³€í™˜ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ë³´ì¡´
                                if "fieldAlias" in field:
                                    stripped_field["fieldAlias"] = field.get("fieldAlias")
                                stripped_item["ddlFields"].append(stripped_field)
                            else:
                                stripped_item["ddlFields"].append(field)
                    
                    stripped["structure"].append(stripped_item)
            
            # description, pros, cons ì œê±° (LLMì´ ë³€í™˜í•  í•„ìš” ì—†ìŒ)
            
            stripped_options.append(stripped)
        
        return stripped_options
    
    def _transform_single_structure_with_llm(
        self,
        structure_item: Dict,
        bounded_context: Dict,
        relevant_standards: List[Dict],
        query_search_results: List[Dict],
        original_structure_item: Optional[Dict] = None,
        mapping_context: Optional[StandardMappingContext] = None,
        update_progress_callback: Optional[callable] = None,
        skip_chunking: bool = False,  # ì²­í‚¹ ë‚´ë¶€ í˜¸ì¶œ ì‹œ ë¬´í•œ ì¬ê·€ ë°©ì§€
        bc_name: Optional[str] = None,
        agg_name: Optional[str] = None,
        original_option_bounded_context: Optional[Dict] = None  # ì›ë³¸ optionì˜ boundedContext ì •ë³´ (ì²­í‚¹ ì²˜ë¦¬ìš©)
    ) -> Dict:
        """
        ë‹¨ì¼ structure(aggregate)ë¥¼ LLMìœ¼ë¡œ ë³€í™˜
        
        Args:
            structure_item: ë³€í™˜í•  ë‹¨ì¼ structure í•­ëª©
            bounded_context: Bounded Context ì •ë³´
            relevant_standards: ê²€ìƒ‰ëœ í‘œì¤€ ì²­í¬ë“¤
            query_search_results: ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ (top-k=3)
            original_structure_item: ì›ë³¸ structure í•­ëª© (ì„ ì²˜ë¦¬ ì „)
            mapping_context: ì„ ì²˜ë¦¬ ë§¤í•‘ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ë³€í™˜ëœ structure í•­ëª©
        """
        # bc_nameê³¼ agg_nameì´ ì—†ìœ¼ë©´ ì¶”ì¶œ
        if not bc_name:
            bc_name = bounded_context.get("name", bounded_context.get("boundedContext", "Unknown"))
        if not agg_name:
            agg_name = structure_item.get("aggregate", {}).get("name", "Unknown")
        
        # ë‹¨ì¼ structureë¥¼ ì˜µì…˜ í˜•ì‹ìœ¼ë¡œ ë˜í•‘
        # boundedContext ì •ë³´ë„ í¬í•¨ (ì²­í‚¹ ì²˜ë¦¬ ë° ë³µì› ì‹œ ì°¸ì¡°ìš©)
        single_structure_option = {
            "structure": [structure_item]
        }
        # ì›ë³¸ optionì˜ boundedContext ì •ë³´ ìš°ì„  ì‚¬ìš© (ìˆëŠ” ê²½ìš°)
        # ì—†ìœ¼ë©´ bounded_context íŒŒë¼ë¯¸í„°ì—ì„œ ê¸°ë³¸ ì •ë³´ êµ¬ì„±
        if original_option_bounded_context:
            # ì›ë³¸ optionì˜ boundedContext ì •ë³´ ì‚¬ìš© (ì²­í‚¹ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì „ì²´ ì •ë³´ í¬í•¨)
            # ë‹¨, _strip_unnecessary_fields_for_llmì—ì„œ í•„ìš”í•œ í•„ë“œë§Œ ìœ ì§€ë¨
            single_structure_option["boundedContext"] = original_option_bounded_context
        elif bounded_context:
            # boundedContextì˜ name, alias, aggregatesë§Œ í¬í•¨ (ë‚˜ë¨¸ì§€ëŠ” ë³µì› ì‹œ ì²˜ë¦¬)
            bc_info = {
                "name": bounded_context.get("name"),
                "alias": bounded_context.get("alias"),
                "aggregates": bounded_context.get("aggregates", [])
            }
            single_structure_option["boundedContext"] = bc_info
        draft_options = [single_structure_option]
        
        # ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°
        stripped_draft_options = self._strip_unnecessary_fields_for_llm(draft_options)
        
        # ë””ë²„ê¹…: structure ì •ë³´ í™•ì¸
        if stripped_draft_options and len(stripped_draft_options) > 0:
            structure = stripped_draft_options[0].get("structure", [])
            if structure and len(structure) > 0:
                first_structure = structure[0]
                preview_attrs = first_structure.get("previewAttributes", [])
                ddl_fields = first_structure.get("ddlFields", [])
                LoggingUtil.info("StandardTransformer", 
                               f"      ğŸ“‹ Structure ì •ë³´: previewAttributes={len(preview_attrs)}ê°œ, ddlFields={len(ddl_fields)}ê°œ")
                if preview_attrs:
                    field_names = [attr.get("fieldName", "") for attr in preview_attrs[:5] if isinstance(attr, dict)]
                    LoggingUtil.info("StandardTransformer", 
                                   f"      ğŸ“‹ previewAttributes í•„ë“œ ì˜ˆì‹œ (ìµœëŒ€ 5ê°œ): {', '.join(field_names)}")
                if ddl_fields:
                    ddl_field_names = [field.get("fieldName", "") for field in ddl_fields[:5] if isinstance(field, dict)]
                    LoggingUtil.info("StandardTransformer", 
                                   f"      ğŸ“‹ ddlFields í•„ë“œ ì˜ˆì‹œ (ìµœëŒ€ 5ê°œ): {', '.join(ddl_field_names)}")
        
        # ë””ë²„ê¹…: ì¿¼ë¦¬ ê²°ê³¼ í™•ì¸ (í•„ë“œ ê´€ë ¨)
        if query_search_results:
            field_queries = []
            for qr in query_search_results:
                query = qr.get("query", "")
                # í•„ë“œ ê´€ë ¨ ì¿¼ë¦¬ì¸ì§€ í™•ì¸ (fieldName íŒ¨í„´ ë˜ëŠ” ì¼ë°˜ì ì¸ í•„ë“œëª… íŒ¨í„´)
                if any(keyword in query.lower() for keyword in ["_id", "_fee", "_amount", "_status", "_at", "id", "fee", "amount", "status"]):
                    field_queries.append(query)
            if field_queries:
                LoggingUtil.info("StandardTransformer", 
                               f"      ğŸ“‹ í•„ë“œ ê´€ë ¨ ì¿¼ë¦¬: {len(field_queries)}ê°œ (ì˜ˆ: {', '.join(field_queries[:5])})")
        
        # ì²­í‚¹ í•„ìš” ì—¬ë¶€ ì‚¬ì „ íŒë‹¨ (í”„ë¡¬í”„íŠ¸ ìƒì„± ì „)
        preview_attrs = structure_item.get("previewAttributes", [])
        ddl_fields = structure_item.get("ddlFields", [])
        enumerations = structure_item.get("enumerations", [])
        value_objects = structure_item.get("valueObjects", [])
        total_fields = len(preview_attrs) + len(ddl_fields)
        total_items = len(enumerations) + len(value_objects) + total_fields
        query_count = len(query_search_results) if query_search_results else 0
        
        # ì „ì²´ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ ì¶”ì •
        # 1. ê²€ìƒ‰ ê²°ê³¼ í† í°: ê° ì¿¼ë¦¬ ê²°ê³¼ë‹¹ í‰ê·  500 í† í° (ì´ì œ top-1ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ)
        estimated_query_tokens = query_count * 500  # ì¿¼ë¦¬ë‹¹ í‰ê·  500 í† í° ì¶”ì • (top-1 ê¸°ì¤€)
        # 2. ì•„ì´í…œ ì •ë³´ í† í°: ê° ì•„ì´í…œ(enum/vo/field)ë‹¹ í‰ê·  100 í† í°
        estimated_items_tokens = total_items * 100
        # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë° ì»¨í…ìŠ¤íŠ¸: ì•½ 2000 í† í°
        estimated_template_tokens = 2000
        # 4. Aggregate ì •ë³´: ì•½ 200 í† í°
        estimated_agg_tokens = 200
        # ì „ì²´ ì˜ˆìƒ í”„ë¡¬í”„íŠ¸ í† í°
        estimated_prompt_tokens_total = estimated_query_tokens + estimated_items_tokens + estimated_template_tokens + estimated_agg_tokens
        
        # ì²­í‚¹ í•„ìš” ì—¬ë¶€: 
        # 1. ì „ì²´ ì˜ˆìƒ í”„ë¡¬í”„íŠ¸ í† í°ì´ 15000 ì´ìƒì´ê±°ë‚˜
        # 2. í•„ë“œ+enum+voê°€ 10ê°œ ì´ìƒì´ê±°ë‚˜
        # 3. ê²€ìƒ‰ ê²°ê³¼ ì¿¼ë¦¬ê°€ 8ê°œ ì´ìƒì¸ ê²½ìš°
        # ë‹¨, skip_chunkingì´ Trueì´ë©´ ì²­í‚¹ ê±´ë„ˆë›°ê¸° (ë¬´í•œ ì¬ê·€ ë°©ì§€)
        should_chunk = not skip_chunking and (estimated_prompt_tokens_total > 15000 or total_items > 10 or query_count > 8)
        
        LoggingUtil.info("StandardTransformer", 
                       f"      ğŸ“Š ì²­í‚¹ íŒë‹¨: ì˜ˆìƒ í”„ë¡¬í”„íŠ¸ í† í°={estimated_prompt_tokens_total} (ì¿¼ë¦¬={estimated_query_tokens}, ì•„ì´í…œ={estimated_items_tokens}, í…œí”Œë¦¿={estimated_template_tokens}), í•„ë“œ={total_fields}, enum={len(enumerations)}, vo={len(value_objects)}, ì¿¼ë¦¬={query_count} â†’ ì²­í‚¹={'í•„ìš”' if should_chunk else 'ë¶ˆí•„ìš”'}")
        
        if should_chunk:
            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“¦ ì²­í‚¹ ì²˜ë¦¬ í•„ìš”: ì˜ˆìƒ í”„ë¡¬í”„íŠ¸ í† í°={estimated_prompt_tokens_total}, í•„ë“œ={total_fields}, enum={len(enumerations)}, vo={len(value_objects)}")
            # bc_nameê³¼ agg_nameì´ ì—†ìœ¼ë©´ structure_itemì—ì„œ ì¶”ì¶œ
            if not bc_name:
                bc_name = bounded_context.get("name", bounded_context.get("boundedContext", "Unknown"))
            if not agg_name:
                agg_name = structure_item.get("aggregate", {}).get("name", "Unknown")
            return self._transform_structure_with_chunking(
                structure_item=structure_item,
                bounded_context=bounded_context,
                relevant_standards=relevant_standards,
                query_search_results=query_search_results,
                original_structure_item=original_structure_item,
                mapping_context=mapping_context,
                update_progress_callback=update_progress_callback,
                estimated_prompt_tokens=estimated_prompt_tokens_total,
                bc_name=bc_name,
                agg_name=agg_name,
                original_option_bounded_context=original_option_bounded_context
            )
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë‹¨ì¼ structureìš© - ì²­í‚¹ ë¶ˆí•„ìš”í•œ ê²½ìš°ë§Œ)
        prompt = self._build_transformation_prompt(
            draft_options=stripped_draft_options,
            bounded_context=bounded_context,
            relevant_standards=relevant_standards,
            query_search_results=query_search_results
        )
        
        # LLM í˜¸ì¶œ (ê¸°ì¡´ ë°©ì‹ - ì²­í‚¹ ë¶ˆí•„ìš”)
        try:
            agg_name = structure_item.get("aggregate", {}).get("name", "Unknown")
            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“¤ LLM API í˜¸ì¶œ ì‹œì‘...")
            
            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸: LLM ë³€í™˜ ì‹œì‘
            if update_progress_callback:
                try:
                    update_progress_callback(0, f"LLM ë³€í™˜ ì¤‘: {agg_name}",
                                            bc_name=bc_name,
                                            agg_name=agg_name,
                                            property_type="aggregate",
                                            status="processing")
                except Exception as e:
                    LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            
            # íƒ€ì„ì•„ì›ƒì„ ëŠ˜ë¦¬ê³  ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
            max_retries = 2
            retry_count = 0
            response = None
            
            while retry_count <= max_retries:
                try:
                    response = self.llm_structured.invoke(prompt)
                    break
                except Exception as e:
                    retry_count += 1
                    error_msg = str(e)
                    
                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸: ì¬ì‹œë„ ì¤‘
                    if update_progress_callback and retry_count <= max_retries:
                        try:
                            if "length limit" in error_msg or "completion_tokens=32768" in error_msg:
                                update_progress_callback(0, f"âš ï¸ LLM ì‘ë‹µ ê¸¸ì´ ì´ˆê³¼ - ì¬ì‹œë„ ì¤‘ ({retry_count}/{max_retries}): {agg_name}",
                                                        bc_name=bc_name,
                                                        agg_name=agg_name,
                                                        property_type="aggregate",
                                                        status="processing",
                                                        error_message=f"ì‘ë‹µ ê¸¸ì´ ì´ˆê³¼ (ì¬ì‹œë„ {retry_count}/{max_retries})")
                            else:
                                update_progress_callback(0, f"âš ï¸ LLM í˜¸ì¶œ ì‹¤íŒ¨ - ì¬ì‹œë„ ì¤‘ ({retry_count}/{max_retries}): {agg_name}",
                                                        bc_name=bc_name,
                                                        agg_name=agg_name,
                                                        property_type="aggregate",
                                                        status="processing",
                                                        error_message=f"LLM í˜¸ì¶œ ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries})")
                        except Exception as update_e:
                            LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_e}")
                    
                    LoggingUtil.warning("StandardTransformer", 
                                      f"      âš ï¸  LLM API í˜¸ì¶œ ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries}): {e}")
                    
                    if retry_count > max_retries:
                        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸: ìµœì¢… ì‹¤íŒ¨
                        if update_progress_callback:
                            try:
                                update_progress_callback(0, f"âŒ LLM ë³€í™˜ ì‹¤íŒ¨: {agg_name} (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼)",
                                                        bc_name=bc_name,
                                                        agg_name=agg_name,
                                                        property_type="aggregate",
                                                        status="error",
                                                        error_message="ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                            except Exception as update_e:
                                LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_e}")
                        raise
                    
                    import time
                    time.sleep(2)  # 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
            
            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“¥ LLM API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
            result = response.get("result", {})
            transformed_options = result.get("transformedOptions", [])
            
            if not transformed_options or len(transformed_options) == 0:
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  LLM ì‘ë‹µì— transformedOptionsê°€ ì—†ìŒ, ì›ë³¸ ë°˜í™˜")
                return structure_item
            
            transformed_option = transformed_options[0]
            if not isinstance(transformed_option, dict):
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  transformedOptionì´ dictê°€ ì•„ë‹˜: {type(transformed_option)}, ì›ë³¸ ë°˜í™˜")
                return structure_item
            
            transformed_structure = transformed_option.get("structure", [])
            
            if not transformed_structure or len(transformed_structure) == 0:
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  transformedStructureê°€ ë¹„ì–´ìˆìŒ, ì›ë³¸ ë°˜í™˜")
                return structure_item
            
            if not isinstance(transformed_structure, list):
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  transformedStructureê°€ listê°€ ì•„ë‹˜: {type(transformed_structure)}, ì›ë³¸ ë°˜í™˜")
                return structure_item
            
            transformed_item = transformed_structure[0]
            if not isinstance(transformed_item, dict):
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  transformedItemì´ dictê°€ ì•„ë‹˜: {type(transformed_item)}, ì›ë³¸ ë°˜í™˜")
                return structure_item
            
            # ğŸ”’ CRITICAL: original_structure_itemì—ì„œ ì›ë³¸ êµ¬ì¡° ê°€ì ¸ì˜¤ê¸° (refs í¬í•¨)
            import copy
            if original_structure_item:
                # ì›ë³¸ structure_item ì‚¬ìš© (refs í¬í•¨)
                merged_item = copy.deepcopy(original_structure_item)
            else:
                # original_structure_itemì´ ì—†ìœ¼ë©´ í˜„ì¬ structure_item ì‚¬ìš©
                merged_item = copy.deepcopy(structure_item)
            
            # Aggregate ì´ë¦„ ë³€í™˜
            if "aggregate" in transformed_item and "aggregate" in merged_item:
                trans_agg = transformed_item["aggregate"]
                merged_agg = merged_item["aggregate"]
                # ğŸ”’ CRITICAL: refs ë³´ì¡´
                preserved_agg_refs = merged_agg.get("refs", []) if isinstance(merged_agg, dict) else []
                if "name" in trans_agg:
                    merged_agg["name"] = trans_agg["name"]
                if "alias" in trans_agg:
                    merged_agg["alias"] = trans_agg["alias"]
                # refs ë³µì› (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„)
                if isinstance(merged_agg, dict) and "refs" not in merged_agg:
                    merged_agg["refs"] = preserved_agg_refs
            
            # Enumeration ì´ë¦„ ë³€í™˜
            if "enumerations" in transformed_item and "enumerations" in merged_item:
                trans_enums = transformed_item["enumerations"]
                merged_enums = merged_item["enumerations"]
                for i, trans_enum in enumerate(trans_enums):
                    if i < len(merged_enums):
                        # ğŸ”’ CRITICAL: refs ë³´ì¡´ (original_structure_itemì—ì„œ ì´ë¯¸ ë³µì›ë¨)
                        preserved_refs = merged_enums[i].get("refs", []) if isinstance(merged_enums[i], dict) else []
                        if "name" in trans_enum:
                            merged_enums[i]["name"] = trans_enum["name"]
                        if "alias" in trans_enum:
                            merged_enums[i]["alias"] = trans_enum["alias"]
                        # refs ë³µì› (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„)
                        if isinstance(merged_enums[i], dict) and "refs" not in merged_enums[i]:
                            merged_enums[i]["refs"] = preserved_refs
            
            # ValueObject ì´ë¦„ ë³€í™˜
            if "valueObjects" in transformed_item and "valueObjects" in merged_item:
                trans_vos = transformed_item["valueObjects"]
                merged_vos = merged_item["valueObjects"]
                for i, trans_vo in enumerate(trans_vos):
                    if i < len(merged_vos):
                        # ğŸ”’ CRITICAL: refs ë³´ì¡´ (original_structure_itemì—ì„œ ì´ë¯¸ ë³µì›ë¨)
                        preserved_refs = merged_vos[i].get("refs", []) if isinstance(merged_vos[i], dict) else []
                        if "name" in trans_vo:
                            merged_vos[i]["name"] = trans_vo["name"]
                        if "alias" in trans_vo:
                            merged_vos[i]["alias"] = trans_vo["alias"]
                        if "referencedAggregateName" in trans_vo:
                            merged_vos[i]["referencedAggregateName"] = trans_vo["referencedAggregateName"]
                        # refs ë³µì› (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„)
                        if isinstance(merged_vos[i], dict) and "refs" not in merged_vos[i]:
                            merged_vos[i]["refs"] = preserved_refs
            
            # previewAttributes í•„ë“œëª… ë³€í™˜ (fieldAliasëŠ” ì›ë³¸ ë³´ì¡´)
            if "previewAttributes" in transformed_item and "previewAttributes" in merged_item:
                trans_attrs = transformed_item["previewAttributes"]
                merged_attrs = merged_item["previewAttributes"]
                for i, trans_attr in enumerate(trans_attrs):
                    if i < len(merged_attrs) and isinstance(trans_attr, dict) and isinstance(merged_attrs[i], dict):
                        # ğŸ”’ CRITICAL: refs ë³´ì¡´ (original_structure_itemì—ì„œ ì´ë¯¸ ë³µì›ë¨)
                        # original_structure_itemì—ì„œ ì˜¨ merged_itemì€ ì´ë¯¸ refsë¥¼ í¬í•¨í•˜ê³  ìˆìŒ
                        # í•˜ì§€ë§Œ í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ëª…ì‹œì ìœ¼ë¡œ ë³´ì¡´
                        preserved_refs = merged_attrs[i].get("refs", [])
                        # fieldNameë§Œ ë³€í™˜ (LLM ê²°ê³¼ ì‚¬ìš©)
                        if "fieldName" in trans_attr:
                            merged_attrs[i]["fieldName"] = trans_attr["fieldName"]
                        # fieldAliasëŠ” ì›ë³¸ ë³´ì¡´ (LLMì´ ë³€í™˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì›ë³¸ ìœ ì§€)
                        # LLM ê²°ê³¼ì— fieldAliasê°€ ìˆì–´ë„ ì›ë³¸ì„ ìš°ì„  (aliasëŠ” ë³€í™˜ ëŒ€ìƒì´ ì•„ë‹˜)
                        # ğŸ”’ CRITICAL: refs ëª…ì‹œì ìœ¼ë¡œ ë³´ì¡´ (ë¹ˆ ë°°ì—´ë„ ë³´ì¡´)
                        merged_attrs[i]["refs"] = preserved_refs
            
            # ddlFields í•„ë“œëª… ë³€í™˜ (fieldAliasëŠ” ì›ë³¸ ë³´ì¡´)
            if "ddlFields" in transformed_item and "ddlFields" in merged_item:
                trans_ddl = transformed_item["ddlFields"]
                merged_ddl = merged_item["ddlFields"]
                for i, trans_field in enumerate(trans_ddl):
                    if i < len(merged_ddl) and isinstance(trans_field, dict) and isinstance(merged_ddl[i], dict):
                        # ğŸ”’ CRITICAL: refs ë³´ì¡´ (original_structure_itemì—ì„œ ì´ë¯¸ ë³µì›ë¨)
                        # original_structure_itemì—ì„œ ì˜¨ merged_itemì€ ì´ë¯¸ refsë¥¼ í¬í•¨í•˜ê³  ìˆìŒ
                        # í•˜ì§€ë§Œ í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ëª…ì‹œì ìœ¼ë¡œ ë³´ì¡´
                        preserved_refs = merged_ddl[i].get("refs", [])
                        # fieldNameë§Œ ë³€í™˜ (LLM ê²°ê³¼ ì‚¬ìš©)
                        if "fieldName" in trans_field:
                            merged_ddl[i]["fieldName"] = trans_field["fieldName"]
                        # fieldAliasëŠ” ì›ë³¸ ë³´ì¡´ (LLMì´ ë³€í™˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì›ë³¸ ìœ ì§€)
                        # LLM ê²°ê³¼ì— fieldAliasê°€ ìˆì–´ë„ ì›ë³¸ì„ ìš°ì„  (aliasëŠ” ë³€í™˜ ëŒ€ìƒì´ ì•„ë‹˜)
                        # ğŸ”’ CRITICAL: refs ëª…ì‹œì ìœ¼ë¡œ ë³´ì¡´ (ë¹ˆ ë°°ì—´ë„ ë³´ì¡´)
                        merged_ddl[i]["refs"] = preserved_refs
            
            return merged_item
            
        except Exception as e:
            import time
            elapsed_time = time.time() - start_time if 'start_time' in locals() else 0
            LoggingUtil.error("StandardTransformer", 
                            f"âŒ ë‹¨ì¼ structure LLM ë³€í™˜ ì‹¤íŒ¨ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ): {e}")
            import traceback
            traceback.print_exc()
            return structure_item
    
    def _transform_with_llm(self, draft_options: List[Dict], 
                           bounded_context: Dict,
                           relevant_standards: List[Dict],
                           query_search_results: Optional[List[Dict]] = None,
                           original_draft_options: Optional[List[Dict]] = None) -> List[Dict]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ í‘œì¤€ì— ë§ê²Œ ë³€í™˜
        
        Args:
            draft_options: ì›ë³¸ ì˜µì…˜ë“¤
            bounded_context: Bounded Context ì •ë³´
            relevant_standards: ê²€ìƒ‰ëœ í‘œì¤€ ì²­í¬ë“¤
            
        Returns:
            ë³€í™˜ëœ ì˜µì…˜ë“¤
        """
        # LLM ìš”ì²­ ì „: ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±° (í† í° ì ˆì•½)
        stripped_draft_options = self._strip_unnecessary_fields_for_llm(draft_options)
        
        LoggingUtil.info("StandardTransformer", "âœ‚ï¸  ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±° ì™„ë£Œ (refs, description, pros, cons, ddlFields)")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì •ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)
        prompt = self._build_transformation_prompt(
            draft_options=stripped_draft_options,  # ì •ë¦¬ëœ ë°ì´í„° ì‚¬ìš©
            bounded_context=bounded_context,
            relevant_standards=relevant_standards,
            query_search_results=query_search_results or []  # ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        )
        
        # LLM í˜¸ì¶œ
        try:
            response = self.llm_structured.invoke(prompt)
            
            result = response.get("result", {})
            transformed_options = result.get("transformedOptions", [])
            
            if not transformed_options:
                LoggingUtil.warning("StandardTransformer", 
                                  "âš ï¸  transformedOptionsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
                return draft_options
            
            # ì˜µì…˜ ìˆ˜ ê²€ì¦ ì œê±°: ê° structureë³„ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì˜µì…˜ ìˆ˜ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
            
            # ğŸ”’ êµ¬ì¡° ë³´ì¡´ ì „ëµ: ì›ë³¸ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³  ë³€í™˜ëœ ì´ë¦„ë§Œ ë®ì–´ì“°ê¸°
            # LLM ì¶œë ¥ì„ ì°¸ê³ í•˜ë˜, ì›ë³¸ êµ¬ì¡°ë¥¼ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ì•ŠìŒ
            merged_options = []
            for i, transformed_option in enumerate(transformed_options):
                if i < len(draft_options):
                    import copy
                    # ì›ë³¸ì„ deep copy (êµ¬ì¡° 100% ë³´ì¡´)
                    original_option = draft_options[i]
                    merged_option = copy.deepcopy(original_option)
                    
                    original_structure = original_option.get("structure", [])
                    transformed_structure = transformed_option.get("structure", [])
                    
                    # ğŸ”’ êµ¬ì¡° ë³´ì¡´: LLM ê²°ê³¼ì—ì„œ ë³€í™˜ëœ ì´ë¦„ë§Œ ì¶”ì¶œí•˜ì—¬ ì›ë³¸ì— ë®ì–´ì“°ê¸°
                    # merged_optionì€ ì´ë¯¸ ì›ë³¸ì˜ deep copy
                    result_structure = merged_option["structure"]
                    
                    # Aggregate ì´ë¦„ ë§¤í•‘ ìƒì„± (ë³€í™˜ ì „ â†’ ë³€í™˜ í›„)
                    # âœ… ì„ ì²˜ë¦¬ ì „ ì›ë³¸ê³¼ ì„ ì²˜ë¦¬ í›„(í˜„ì¬)ë¥¼ ë¹„êµí•´ì„œ ë§¤í•‘ ì •ë³´ ì¶”ì¶œ
                    aggregate_name_mapping = {}
                    preprocessing_mapping = {}  # ì„ ì²˜ë¦¬ ë§¤í•‘ë§Œ ì¶”ì 
                    llm_aggregate_mapping = {}  # LLMì´ ë³€í™˜í•œ aggregate ì´ë¦„ ë§¤í•‘
                    
                    # ì„ ì²˜ë¦¬ ë§¤í•‘ ì •ë³´ ì¶”ì¶œ (original_draft_optionsê°€ ìˆìœ¼ë©´)
                    if original_draft_options and i < len(original_draft_options):
                        original_opt_structure = original_draft_options[i].get("structure", [])
                        current_opt_structure = draft_options[i].get("structure", [])  # ì„ ì²˜ë¦¬ í›„
                        
                        # alias ê¸°ë°˜ ë§¤ì¹­ìœ¼ë¡œ ì„ ì²˜ë¦¬ ë§¤í•‘ ì¶”ì¶œ
                        for orig_item in original_opt_structure:
                            orig_alias = orig_item.get("aggregate", {}).get("alias")
                            orig_name = orig_item.get("aggregate", {}).get("name")
                            
                            for curr_item in current_opt_structure:
                                curr_alias = curr_item.get("aggregate", {}).get("alias")
                                curr_name = curr_item.get("aggregate", {}).get("name")
                                
                                if orig_alias == curr_alias and orig_name and curr_name and orig_name != curr_name:
                                    # ì„ ì²˜ë¦¬ì—ì„œ ë³€í™˜ëœ ë§¤í•‘ (ì˜ˆ: "Customer" â†’ "m_cst")
                                    preprocessing_mapping[orig_name] = curr_name
                                    aggregate_name_mapping[orig_name] = curr_name
                                    # ë¡œê·¸ ê°„ì†Œí™”: ì„ ì²˜ë¦¬ ë§¤í•‘ ë¡œê·¸ ì œê±°
                                    break
                    
                    # ğŸ”§ CRITICAL FIX: ë¨¼ì € ëª¨ë“  aggregateì˜ LLM ë³€í™˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘
                    # ê·¸ë˜ì•¼ ë‚˜ì¤‘ì— Enum/VO ë³€í™˜ ì‹œ ëª¨ë“  aggregate ë§¤í•‘ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
                    # âš ï¸ ì¤‘ìš”: original_draft_optionsì—ì„œ ì›ë³¸ ì´ë¦„ì„ ê°€ì ¸ì™€ì•¼ í•¨!
                    if original_draft_options and i < len(original_draft_options):
                        original_opt_structure = original_draft_options[i].get("structure", [])
                        for trans_item in transformed_structure:
                            trans_agg_alias = trans_item.get("aggregate", {}).get("alias")
                            if not trans_agg_alias:
                                continue
                            
                            # original_draft_optionsì—ì„œ ì›ë³¸ ì´ë¦„ ì°¾ê¸° (ì„ ì²˜ë¦¬ ì „!)
                            for orig_item in original_opt_structure:
                                if orig_item.get("aggregate", {}).get("alias") == trans_agg_alias:
                                    orig_agg_name = orig_item.get("aggregate", {}).get("name")  # ì›ë³¸ ì´ë¦„ (ì˜ˆ: "Customer")
                                    trans_agg_name = trans_item.get("aggregate", {}).get("name")  # LLM ê²°ê³¼ (ì˜ˆ: "m_cst")
                                    
                                    # LLMì´ ë³€í™˜í–ˆëŠ”ì§€ í™•ì¸ (ì„ ì²˜ë¦¬ í›„ ì´ë¦„ê³¼ ë¹„êµ)
                                    # result_structureì—ì„œ ì„ ì²˜ë¦¬ í›„ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                                    current_agg_name = None
                                    for curr_item in result_structure:
                                        if curr_item.get("aggregate", {}).get("alias") == trans_agg_alias:
                                            current_agg_name = curr_item.get("aggregate", {}).get("name")  # ì„ ì²˜ë¦¬ í›„ (ì˜ˆ: "m_cst")
                                            break
                                    
                                    # LLMì´ ë³€í™˜í–ˆìœ¼ë©´ (trans_agg_name != current_agg_name) ë˜ëŠ”
                                    # ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì´ë¯¸ ìˆìœ¼ë©´ aggregate_name_mappingì— ì¶”ê°€
                                    if orig_agg_name:
                                        if trans_agg_name and trans_agg_name != current_agg_name:
                                            # LLMì´ ì¶”ê°€ ë³€í™˜í•¨ (ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì—†ê±°ë‚˜ ë‹¤ë¦„)
                                            llm_aggregate_mapping[orig_agg_name] = trans_agg_name
                                        elif orig_agg_name in preprocessing_mapping:
                                            # ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì´ë¯¸ ìˆìŒ (ê·¸ëŒ€ë¡œ ì‚¬ìš©)
                                            pass
                                        else:
                                            # LLMì´ ë³€í™˜í•˜ì§€ ì•Šì•˜ê³  ì„ ì²˜ë¦¬ ë§¤í•‘ë„ ì—†ìœ¼ë©´ ì›ë³¸ ìœ ì§€
                                            llm_aggregate_mapping[orig_agg_name] = orig_agg_name
                                    break
                    
                    # LLM ë§¤í•‘ì„ aggregate_name_mappingì— ë³‘í•© (ì„ ì²˜ë¦¬ ë§¤í•‘ë³´ë‹¤ ìš°ì„ )
                    for orig_name, new_name in llm_aggregate_mapping.items():
                        aggregate_name_mapping[orig_name] = new_name
                    
                    # ë§¤í•‘ ì •ë³´ë¥¼ merged_optionì— ì €ì¥ (summary ìƒì„± ì‹œ ì‚¬ìš©)
                    if "mapping_info" not in merged_option:
                        merged_option["mapping_info"] = {}
                    merged_option["mapping_info"][f"option_{i}"] = {
                        "preprocessing_mapping": preprocessing_mapping,
                        "llm_mapping": llm_aggregate_mapping
                    }
                    
                    # ğŸ”§ BC ê°„ ì°¸ì¡°ë¥¼ ìœ„í•œ ì „ì—­ ë§¤í•‘ì— í˜„ì¬ BCì˜ ë§¤í•‘ ì¶”ê°€
                    self._global_aggregate_name_mapping.update(aggregate_name_mapping)
                    
                    # ğŸ”§ enum/VO ë³€í™˜ ì‹œ ì „ì—­ ë§¤í•‘ ì‚¬ìš© (ë‹¤ë¥¸ BCì˜ aggregate ì°¸ì¡° ê°€ëŠ¥)
                    # ì „ì—­ ë§¤í•‘ê³¼ í˜„ì¬ BC ë§¤í•‘ì„ ë³‘í•© (ì „ì—­ ë§¤í•‘ì´ ìš°ì„ , í˜„ì¬ BC ë§¤í•‘ìœ¼ë¡œ ë³´ì™„)
                    combined_aggregate_mapping = {**aggregate_name_mapping, **self._global_aggregate_name_mapping}
                    
                    # ë¡œê·¸ ê°„ì†Œí™”: ë§¤í•‘ ìˆ˜ì§‘ ë¡œê·¸ ì œê±°
                    
                    # ğŸ”§ í•„ë“œëª… ë§¤í•‘ ì¶”ì : ì„ ì²˜ë¦¬ì—ì„œ ë³€í™˜ëœ í•„ë“œëª… ì¶”ì 
                    # original_draft_options (ì„ ì²˜ë¦¬ ì „) vs draft_options (ì„ ì²˜ë¦¬ í›„) ë¹„êµ
                    field_name_mapping = {}  # {aggregate_alias: {original_field: transformed_field}}
                    if original_draft_options and i < len(original_draft_options):
                        original_opt_structure = original_draft_options[i].get("structure", [])
                        current_opt_structure = draft_options[i].get("structure", [])  # ì„ ì²˜ë¦¬ í›„
                        
                        for orig_item in original_opt_structure:
                            orig_alias = orig_item.get("aggregate", {}).get("alias")
                            if not orig_alias:
                                continue
                            
                            # ê°™ì€ aliasë¥¼ ê°€ì§„ í˜„ì¬ í•­ëª© ì°¾ê¸°
                            for curr_item in current_opt_structure:
                                curr_alias = curr_item.get("aggregate", {}).get("alias")
                                if orig_alias != curr_alias:
                                    continue
                                
                                # previewAttributes í•„ë“œëª… ë§¤í•‘ ì¶”ì 
                                if orig_alias not in field_name_mapping:
                                    field_name_mapping[orig_alias] = {}
                                
                                orig_attrs = orig_item.get("previewAttributes", [])
                                curr_attrs = curr_item.get("previewAttributes", [])
                                for attr_idx in range(min(len(orig_attrs), len(curr_attrs))):
                                    if isinstance(orig_attrs[attr_idx], dict) and isinstance(curr_attrs[attr_idx], dict):
                                        orig_field = orig_attrs[attr_idx].get("fieldName")
                                        curr_field = curr_attrs[attr_idx].get("fieldName")
                                        if orig_field and curr_field and orig_field != curr_field:
                                            field_name_mapping[orig_alias][orig_field] = curr_field
                                
                                # ddlFields í•„ë“œëª… ë§¤í•‘ ì¶”ì 
                                orig_ddl_fields = orig_item.get("ddlFields", [])
                                curr_ddl_fields = curr_item.get("ddlFields", [])
                                for ddl_idx in range(min(len(orig_ddl_fields), len(curr_ddl_fields))):
                                    if isinstance(orig_ddl_fields[ddl_idx], dict) and isinstance(curr_ddl_fields[ddl_idx], dict):
                                        orig_field = orig_ddl_fields[ddl_idx].get("fieldName")
                                        curr_field = curr_ddl_fields[ddl_idx].get("fieldName")
                                        if orig_field and curr_field and orig_field != curr_field:
                                            field_name_mapping[orig_alias][orig_field] = curr_field
                                
                                break
                    
                    # ë¡œê·¸ ê°„ì†Œí™”: í•„ë“œ ë§¤í•‘ ìˆ˜ì§‘ ë¡œê·¸ ì œê±°
                    
                    # ğŸ”’ CRITICAL: alias ê¸°ë°˜ ë§¤ì¹­ìœ¼ë¡œ refs ë³´ì¡´ ë³´ì¥
                    # LLMì´ ìˆœì„œë¥¼ ë°”ê¾¸ë”ë¼ë„ aliasë¡œ ì˜¬ë°”ë¥¸ í•­ëª©ì„ ì°¾ì•„ì„œ ë§¤ì¹­
                    # ì´ë ‡ê²Œ í•´ì•¼ refsê°€ ì˜¬ë°”ë¥¸ Aggregateì— ìœ ì§€ë¨!
                    
                    # 2ë‹¨ê³„: Aggregate ì´ë¦„ ë®ì–´ì“°ê¸° ë° Enum/VO ì²˜ë¦¬
                    # âš ï¸ CRITICAL FIX: result_structureë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë£¨í”„ë¥¼ ëŒì•„ì•¼ ëª¨ë“  aggregateë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ!
                    # LLMì´ ì¼ë¶€ë§Œ ë°˜í™˜í•´ë„ ëª¨ë“  aggregateì˜ Enum/VOë¥¼ ì²˜ë¦¬í•´ì•¼ í•¨
                    
                    # result_structureë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë£¨í”„ (ëª¨ë“  aggregate ì²˜ë¦¬ ë³´ì¥)
                    for orig_item in result_structure:
                        orig_agg_alias = orig_item.get("aggregate", {}).get("alias")
                        
                        if not orig_agg_alias:
                            continue
                        
                        # transformed_structureì—ì„œ ê°™ì€ aliasë¥¼ ê°€ì§„ í•­ëª© ì°¾ê¸°
                        trans_item = None
                        for item in transformed_structure:
                            if item.get("aggregate", {}).get("alias") == orig_agg_alias:
                                trans_item = item
                                break
                        
                        # trans_itemì´ ì—†ì–´ë„ Enum/VO ì²˜ë¦¬ëŠ” ì§„í–‰ (aggregate_name_mapping ì‚¬ìš©)
                        
                        # 1. Aggregate ì´ë¦„ ë®ì–´ì“°ê¸° (aliasë¡œ ë§¤ì¹­í–ˆìœ¼ë¯€ë¡œ refsëŠ” ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìœ ì§€!)
                        if "aggregate" in orig_item and trans_item and "aggregate" in trans_item:
                            orig_agg = orig_item["aggregate"]
                            trans_agg = trans_item["aggregate"]
                            
                            # ğŸ”’ CRITICAL: alias ê²€ì¦ ë° ë³´í˜¸ (LLMì´ ì‹¤ìˆ˜ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŒ)
                            orig_agg_alias = orig_agg.get("alias")
                            trans_agg_alias = trans_agg.get("alias")
                            if trans_agg_alias and trans_agg_alias != orig_agg_alias:
                                LoggingUtil.warning("StandardTransformer", 
                                                  f"   [ê²½ê³ ] LLMì´ aliasë¥¼ ë³€ê²½ ì‹œë„: '{orig_agg_alias}' â†’ '{trans_agg_alias}' (ì›ë³¸ ìœ ì§€)")
                                # ì›ë³¸ aliasë¡œ ë³µêµ¬
                                trans_agg["alias"] = orig_agg_alias
                            
                            # ì´ë¦„ë§Œ ë®ì–´ì“°ê¸°
                            orig_agg_name = orig_agg.get("name")
                            trans_agg_name = trans_agg.get("name")
                            if trans_agg_name:
                                orig_agg["name"] = trans_agg_name
                                # aggregate_name_mappingì€ ì´ë¯¸ ìœ„ì—ì„œ ìˆ˜ì§‘í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë¡œê·¸ë§Œ ì¶œë ¥
                                # ë¡œê·¸ ê°„ì†Œí™”: ì´ë¦„ë³€í™˜ ë¡œê·¸ ì œê±° (summaryì—ì„œ í™•ì¸ ê°€ëŠ¥)
                                pass
                        
                        # ğŸ”§ Deterministic VO/Enum ë³€í™˜: Aggregate ì´ë¦„ì´ ë³€í™˜ë˜ë©´ ìë™ìœ¼ë¡œ prefix ì²˜ë¦¬
                        # LLMì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³  ë°±ì—”ë“œì—ì„œ ê°•ì œ ì²˜ë¦¬
                        if "aggregate" not in orig_item:
                            LoggingUtil.warning("StandardTransformer", 
                                              f"   [ê²½ê³ ] orig_itemì— aggregateê°€ ì—†ìŒ - Enum/VO ì²˜ë¦¬ ê±´ë„ˆëœ€")
                            continue
                        
                        orig_agg = orig_item["aggregate"]
                        current_agg_name = orig_agg.get("name")  # ë³€í™˜ëœ ì´ë¦„ (ì´ë¯¸ ë®ì–´ì¨ì§„ í›„)
                        
                        # ì›ë³¸ aggregate ì´ë¦„ ë³µì› (aggregate_name_mappingì—ì„œ ì—­ë§¤í•‘)
                        # âš ï¸ original_draft_optionsì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸° (ë” í™•ì‹¤í•¨)
                        original_agg_name = None
                        if original_draft_options and i < len(original_draft_options):
                            original_opt_structure = original_draft_options[i].get("structure", [])
                            for orig_opt_item in original_opt_structure:
                                if orig_opt_item.get("aggregate", {}).get("alias") == orig_agg_alias:
                                    original_agg_name = orig_opt_item.get("aggregate", {}).get("name")
                                    break
                        
                        # ì—­ë§¤í•‘ìœ¼ë¡œë„ ì‹œë„ (fallback)
                        if not original_agg_name:
                            for orig_name, new_name in aggregate_name_mapping.items():
                                if new_name == current_agg_name:
                                    original_agg_name = orig_name
                                    break
                        
                        # ë¡œê·¸ ê°„ì†Œí™”: Enum/VO ì²˜ë¦¬ ìƒì„¸ ë¡œê·¸ ì œê±°
                        pass
                        
                        # 2. Enumeration ì´ë¦„ ë®ì–´ì“°ê¸° (alias ë§¤ì¹­ìœ¼ë¡œ refs ë³´ì¡´)
                        orig_enums = orig_item.get("enumerations", [])
                        trans_enums = trans_item.get("enumerations", []) if trans_item else []
                        
                        # ğŸ”§ ì›ë³¸ Enum ì´ë¦„ ë° refs ê°€ì ¸ì˜¤ê¸° (original_draft_optionsì—ì„œ)
                        original_enum_names = {}  # {alias: original_name}
                        original_enum_refs = {}  # {alias: refs}
                        if original_draft_options and i < len(original_draft_options):
                            original_opt_structure = original_draft_options[i].get("structure", [])
                            for orig_opt_item in original_opt_structure:
                                if orig_opt_item.get("aggregate", {}).get("alias") == orig_agg_alias:
                                    for orig_opt_enum in orig_opt_item.get("enumerations", []):
                                        enum_alias = orig_opt_enum.get("alias")
                                        enum_name = orig_opt_enum.get("name")
                                        enum_refs = orig_opt_enum.get("refs", [])
                                        if enum_alias and enum_name:
                                            original_enum_names[enum_alias] = enum_name
                                            original_enum_refs[enum_alias] = enum_refs
                                    break
                        
                        # ğŸ”§ Deterministic Enum ë³€í™˜: Aggregate prefix ìë™ ì ìš©
                        # âœ… ëª¨ë“  BCì˜ aggregate ì´ë¦„ ë§¤í•‘ í™•ì¸ (ë‹¤ë¥¸ BC ì°¸ì¡°ë„ ì²˜ë¦¬)
                        for orig_enum in orig_enums:
                            orig_enum_alias = orig_enum.get("alias")
                            orig_enum_name = orig_enum.get("name")
                            # ì›ë³¸ ì´ë¦„ ì‚¬ìš© (ì„ ì²˜ë¦¬ ì „)
                            original_enum_name = original_enum_names.get(orig_enum_alias, orig_enum_name)
                            
                            # LLM ê²°ê³¼ì—ì„œ ë§¤ì¹­ë˜ëŠ” ê²ƒ ì°¾ê¸°
                            trans_enum_name = None
                            for trans_enum in trans_enums:
                                if trans_enum.get("alias") == orig_enum_alias:
                                    trans_enum_name = trans_enum.get("name")
                                    # ë¡œê·¸ ê°„ì†Œí™”: LLMê²°ê³¼ ë¡œê·¸ ì œê±°
                                    pass
                                    break
                            
                            # LLM ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ê±°ë‚˜ ì˜ëª»ë˜ì—ˆìœ¼ë©´ ìë™ ìƒì„±
                            # âœ… LLM ê²°ê³¼ ê²€ì¦: aggregate ì´ë¦„ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸ (í•˜ë“œì½”ë”©ëœ prefix ì²´í¬ ì œê±°)
                            new_name = None
                            
                            if trans_enum_name:
                                # ğŸ”§ CRITICAL FIX: LLM ê²°ê³¼ê°€ aggregate ì´ë¦„ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ê²€ì¦
                                # Enum ì´ë¦„ì´ aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                                is_valid_llm_result = False
                                
                                # 1. í˜„ì¬ BCì˜ aggregateì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸
                                if original_agg_name and original_enum_name and original_enum_name.startswith(original_agg_name):
                                    is_valid_llm_result = True
                                else:
                                    # 2. ë‹¤ë¥¸ BCì˜ aggregateì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸
                                    for mapped_orig_name, mapped_new_name in combined_aggregate_mapping.items():
                                        if original_enum_name.startswith(mapped_orig_name) and mapped_orig_name != original_enum_name:
                                            is_valid_llm_result = True
                                            break
                                
                                if is_valid_llm_result:
                                    # LLMì´ ì˜¬ë°”ë¥´ê²Œ ë³€í™˜í•œ ê²½ìš° (aggregateì™€ ê´€ë ¨ ìˆìŒ)
                                    new_name = trans_enum_name
                                    # ë¡œê·¸ ê°„ì†Œí™”: LLMì±„íƒ ë¡œê·¸ ì œê±°
                                    pass
                                else:
                                    # LLMì´ ì˜ëª» ë³€í™˜í•œ ê²½ìš° (aggregateì™€ ë¬´ê´€í•¨)
                                    # ë¡œê·¸ ê°„ì†Œí™”: LLMë¬´ì‹œ ë¡œê·¸ ì œê±° (ê²½ê³ ëŠ” ìœ ì§€í•˜ë˜ ê°„ì†Œí™”)
                                    pass
                            
                            # LLM ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¬´íš¨í•œ ê²½ìš° ìë™ ìƒì„± ì‹œë„
                            if not new_name:
                                # ìë™ ìƒì„±: aggregate_name + "_" + suffix
                                # ì˜ˆ: OrderStatus â†’ m_odr_status, CartStatus â†’ m_bkt_status
                                
                                # 1. í˜„ì¬ aggregate ì´ë¦„ í™•ì¸ (ë³€í™˜ ì „ ì›ë³¸ ì´ë¦„ ì‚¬ìš©!)
                                # âš ï¸ original_enum_name ì‚¬ìš© (ì„ ì²˜ë¦¬ ì „ ì›ë³¸ ì´ë¦„)
                                if original_agg_name and original_enum_name and original_enum_name.startswith(original_agg_name):
                                    # í˜„ì¬ BCì˜ aggregateê°€ ë³€í™˜ëœ ê²½ìš°
                                    # ì˜ˆ: "OrderStatus"ëŠ” "Order"ë¡œ ì‹œì‘ â†’ "Status" ì¶”ì¶œ
                                    suffix = original_enum_name[len(original_agg_name):]
                                    import re
                                    suffix_snake = re.sub(r'(?<!^)(?=[A-Z])', '_', suffix).lower()
                                    new_name = current_agg_name + "_" + suffix_snake if suffix_snake else current_agg_name + "_enum"
                                    # ë¡œê·¸ ê°„ì†Œí™”: ìë™ë³€í™˜ ë¡œê·¸ ì œê±°
                                    pass
                                    
                                    # ğŸ”§ Enum ë‚´ë¶€ í•„ë“œ ë³€í™˜ (Enumì´ ë³€í™˜ë˜ë©´ ë‚´ë¶€ í•„ë“œë„ ë³€í™˜)
                                    # Enum ë‚´ë¶€ì— í•„ë“œê°€ ìˆëŠ” ê²½ìš° (ì˜ˆ: previewAttributes, ddlFields)
                                    if "previewAttributes" in orig_enum or "ddlFields" in orig_enum:
                                        # Enum ë‚´ë¶€ í•„ë“œë„ aggregate ì´ë¦„ì— ë”°ë¼ ë³€í™˜
                                        enum_preview_attrs = orig_enum.get("previewAttributes", [])
                                        for enum_attr in enum_preview_attrs:
                                            if isinstance(enum_attr, dict):
                                                enum_field_name = enum_attr.get("fieldName", "")
                                                if enum_field_name:
                                                    # aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í•„ë“œëª… ë³€í™˜
                                                    # ì˜ˆ: "customerStatus" â†’ ë³€í™˜ëœ aggregate ì´ë¦„ + "_status"
                                                    if original_agg_name and enum_field_name.startswith(original_agg_name.lower()):
                                                        # aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í•„ë“œ
                                                        suffix = enum_field_name[len(original_agg_name.lower()):]
                                                        # m_ prefixë¥¼ fld_ë¡œ ë³€ê²½ (í‘œì¤€ëª… í˜•ì‹ì— ë§ì¶¤)
                                                        new_field_name = current_agg_name.replace("m_", "fld_") + "_" + suffix if suffix else current_agg_name.replace("m_", "fld_")
                                                        enum_attr["fieldName"] = new_field_name
                                                        LoggingUtil.info("StandardTransformer", 
                                                                       f"   [Enumí•„ë“œë³€í™˜] Enum '{orig_enum_alias}' ë‚´ë¶€ í•„ë“œ: '{enum_field_name}' â†’ '{new_field_name}'")
                                        
                                        enum_ddl_fields = orig_enum.get("ddlFields", [])
                                        for enum_ddl_field in enum_ddl_fields:
                                            if isinstance(enum_ddl_field, dict):
                                                enum_field_name = enum_ddl_field.get("fieldName", "")
                                                if enum_field_name:
                                                    if original_agg_name and enum_field_name.startswith(original_agg_name.lower()):
                                                        suffix = enum_field_name[len(original_agg_name.lower()):]
                                                        new_field_name = current_agg_name.replace("m_", "fld_") + "_" + suffix if suffix else current_agg_name.replace("m_", "fld_")
                                                        enum_ddl_field["fieldName"] = new_field_name
                                                        LoggingUtil.info("StandardTransformer", 
                                                                       f"   [Enumí•„ë“œë³€í™˜] Enum '{orig_enum_alias}' ë‚´ë¶€ DDLí•„ë“œ: '{enum_field_name}' â†’ '{new_field_name}'")
                                
                                # 2. ë‹¤ë¥¸ BCì˜ aggregate ì´ë¦„ í™•ì¸ (ì°¸ì¡° ê´€ê³„) - original_agg_nameì´ ì—†ì–´ë„ ì‹¤í–‰
                                if (not new_name or new_name == orig_enum_name) and original_enum_name:
                                    # ğŸ”§ combined_aggregate_mapping ì‚¬ìš© (ì „ì—­ ë§¤í•‘ í¬í•¨, ë‹¤ë¥¸ BC ì°¸ì¡° ê°€ëŠ¥)
                                    for mapped_orig_name, mapped_new_name in combined_aggregate_mapping.items():
                                        # Enum ì´ë¦„ì´ aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                                        # ì˜ˆ: "OrderStatus"ëŠ” "Order"ë¡œ ì‹œì‘, "CartStatus"ëŠ” "Cart"ë¡œ ì‹œì‘
                                        # âš ï¸ original_enum_name ì‚¬ìš© (ì„ ì²˜ë¦¬ ì „ ì›ë³¸ ì´ë¦„)
                                        if original_enum_name.startswith(mapped_orig_name) and mapped_orig_name != original_enum_name:
                                            # í¬í•¨ëœ aggregate ì´ë¦„ì„ ë³€í™˜ëœ ì´ë¦„ìœ¼ë¡œ êµì²´
                                            suffix = original_enum_name[len(mapped_orig_name):]  # ë‚˜ë¨¸ì§€ ë¶€ë¶„
                                            import re
                                            # snake_case ë³€í™˜
                                            suffix_snake = re.sub(r'(?<!^)(?=[A-Z])', '_', suffix).lower()
                                            new_name = mapped_new_name + "_" + suffix_snake if suffix_snake else mapped_new_name + "_enum"
                                            # ë¡œê·¸ ê°„ì†Œí™”: ì°¸ì¡°ë³€í™˜ ë¡œê·¸ ì œê±°
                                            break
                                
                                # 3. ë§¤í•‘ì´ ì—†ìœ¼ë©´ ì›ë³¸ ìœ ì§€ (aggregateì™€ ë¬´ê´€í•œ ë…ë¦½ì ì¸ Enum)
                                if not new_name:
                                    new_name = original_enum_name  # ì›ë³¸ ì´ë¦„ ìœ ì§€
                                    # ë¡œê·¸ ê°„ì†Œí™”: ì›ë³¸ìœ ì§€ ë¡œê·¸ ì œê±°
                            
                            if new_name and orig_enum_name != new_name:
                                # ğŸ”’ CRITICAL: refs ë³´ì¡´ (ë¹ˆ ë°°ì—´ë„ ë³´ì¡´)
                                preserved_refs = orig_enum.get("refs", [])
                                # original_draft_optionsì—ì„œ ì›ë³¸ refs ê°€ì ¸ì˜¤ê¸°
                                if orig_enum_alias in original_enum_refs:
                                    preserved_refs = original_enum_refs[orig_enum_alias]
                                
                                orig_enum["name"] = new_name
                                # refs ë³µêµ¬ (ë¹ˆ ë°°ì—´ë„ ë³´ì¡´)
                                if "refs" not in orig_enum or not orig_enum.get("refs"):
                                    orig_enum["refs"] = preserved_refs
                                # ë¡œê·¸ ê°„ì†Œí™”: ì´ë¦„ë³€í™˜ ë¡œê·¸ ì œê±°
                        
                        # ğŸ”§ Aggregate í•„ë“œëª… ë³€í™˜: aggregate ì´ë¦„ì´ ë³€í™˜ë˜ë©´ ê´€ë ¨ í•„ë“œëª…ë„ ë³€í™˜
                        # ì˜ˆ: Customer â†’ m_cstì´ë©´, customerStatus â†’ fld_cst_status, customerId â†’ fld_cst_id
                        if original_agg_name and current_agg_name and original_agg_name != current_agg_name:
                            import re
                            
                            # PreviewAttributes í•„ë“œëª… ë³€í™˜
                            orig_attrs = orig_item.get("previewAttributes", [])
                            for attr in orig_attrs:
                                if isinstance(attr, dict):
                                    field_name = attr.get("fieldName", "")
                                    if field_name:
                                        # aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í•„ë“œëª… ë³€í™˜
                                        # ì˜ˆ: customerStatus â†’ fld_cst_status, customerId â†’ fld_cst_id
                                        field_lower = field_name.lower()
                                        original_agg_lower = original_agg_name.lower()
                                        
                                        if field_lower.startswith(original_agg_lower):
                                            # aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í•„ë“œ
                                            suffix = field_name[len(original_agg_name):]
                                            # camelCaseë¥¼ snake_caseë¡œ ë³€í™˜
                                            suffix_snake = re.sub(r'(?<!^)(?=[A-Z])', '_', suffix).lower()
                                            new_field_name = current_agg_name.replace("m_", "fld_") + "_" + suffix_snake if suffix_snake else current_agg_name.replace("m_", "fld_")
                                            attr["fieldName"] = new_field_name
                                            # ë¡œê·¸ ê°„ì†Œí™”: Aggí•„ë“œë³€í™˜ ë¡œê·¸ ì œê±°
                            
                            # DDLFields í•„ë“œëª… ë³€í™˜
                            orig_ddl_fields = orig_item.get("ddlFields", [])
                            for ddl_field in orig_ddl_fields:
                                if isinstance(ddl_field, dict):
                                    field_name = ddl_field.get("fieldName", "")
                                    if field_name:
                                        # aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í•„ë“œëª… ë³€í™˜
                                        field_lower = field_name.lower()
                                        original_agg_lower = original_agg_name.lower()
                                        
                                        if field_lower.startswith(original_agg_lower):
                                            # aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í•„ë“œ
                                            suffix = field_name[len(original_agg_name):]
                                            # camelCaseë¥¼ snake_caseë¡œ ë³€í™˜
                                            suffix_snake = re.sub(r'(?<!^)(?=[A-Z])', '_', suffix).lower()
                                            new_field_name = current_agg_name.replace("m_", "fld_") + "_" + suffix_snake if suffix_snake else current_agg_name.replace("m_", "fld_")
                                            ddl_field["fieldName"] = new_field_name
                                            # ë¡œê·¸ ê°„ì†Œí™”: Aggí•„ë“œë³€í™˜ ë¡œê·¸ ì œê±°
                        
                        # 3. ValueObject ì´ë¦„ ë®ì–´ì“°ê¸° (alias ë§¤ì¹­ìœ¼ë¡œ refs ë³´ì¡´)
                        orig_vos = orig_item.get("valueObjects", [])
                        trans_vos = trans_item.get("valueObjects", []) if trans_item else []
                        
                        # ğŸ”§ ì›ë³¸ VO ì´ë¦„ ë° refs ê°€ì ¸ì˜¤ê¸° (original_draft_optionsì—ì„œ)
                        original_vo_names = {}  # {alias: original_name}
                        original_vo_refs = {}  # {alias: refs}
                        if original_draft_options and i < len(original_draft_options):
                            original_opt_structure = original_draft_options[i].get("structure", [])
                            for orig_opt_item in original_opt_structure:
                                if orig_opt_item.get("aggregate", {}).get("alias") == orig_agg_alias:
                                    for orig_opt_vo in orig_opt_item.get("valueObjects", []):
                                        vo_alias = orig_opt_vo.get("alias")
                                        vo_name = orig_opt_vo.get("name")
                                        vo_refs = orig_opt_vo.get("refs", [])
                                        if vo_alias and vo_name:
                                            original_vo_names[vo_alias] = vo_name
                                            original_vo_refs[vo_alias] = vo_refs
                                    break
                        
                        # ğŸ”§ Deterministic VO ë³€í™˜: Aggregate prefix ìë™ ì ìš©
                        # âœ… ëª¨ë“  BCì˜ aggregate ì´ë¦„ ë§¤í•‘ í™•ì¸ (ë‹¤ë¥¸ BC ì°¸ì¡°ë„ ì²˜ë¦¬)
                        import re
                        for orig_vo in orig_vos:
                            orig_vo_alias = orig_vo.get("alias")
                            orig_vo_name = orig_vo.get("name")
                            # ì›ë³¸ ì´ë¦„ ì‚¬ìš© (ì„ ì²˜ë¦¬ ì „)
                            original_vo_name = original_vo_names.get(orig_vo_alias, orig_vo_name)
                            
                            # LLM ê²°ê³¼ì—ì„œ ë§¤ì¹­ë˜ëŠ” ê²ƒ ì°¾ê¸°
                            trans_vo_name = None
                            for trans_vo in trans_vos:
                                if trans_vo.get("alias") == orig_vo_alias:
                                    trans_vo_name = trans_vo.get("name")
                                    # ë¡œê·¸ ê°„ì†Œí™”: LLMê²°ê³¼ ë¡œê·¸ ì œê±°
                                    break
                            
                            # LLM ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ê±°ë‚˜ ì˜ëª»ë˜ì—ˆìœ¼ë©´ ìë™ ìƒì„±
                            # âœ… LLM ê²°ê³¼ ê²€ì¦: aggregate ì´ë¦„ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸ (í•˜ë“œì½”ë”©ëœ prefix ì²´í¬ ì œê±°)
                            new_name = None
                            
                            if trans_vo_name:
                                # ğŸ”§ CRITICAL FIX: LLM ê²°ê³¼ê°€ aggregate ì´ë¦„ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ê²€ì¦
                                # VO ì´ë¦„ì´ aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                                is_valid_llm_result = False
                                
                                # 1. í˜„ì¬ BCì˜ aggregateì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸
                                if original_agg_name and original_vo_name and original_vo_name.startswith(original_agg_name):
                                    is_valid_llm_result = True
                                else:
                                    # 2. ë‹¤ë¥¸ BCì˜ aggregateì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸
                                    for mapped_orig_name, mapped_new_name in combined_aggregate_mapping.items():
                                        if original_vo_name.startswith(mapped_orig_name) and mapped_orig_name != original_vo_name:
                                            is_valid_llm_result = True
                                            break
                                
                                if is_valid_llm_result:
                                    # LLMì´ ì˜¬ë°”ë¥´ê²Œ ë³€í™˜í•œ ê²½ìš° (aggregateì™€ ê´€ë ¨ ìˆìŒ)
                                    new_name = trans_vo_name
                                    # ë¡œê·¸ ê°„ì†Œí™”: LLMì±„íƒ ë¡œê·¸ ì œê±°
                                else:
                                    # LLMì´ ì˜ëª» ë³€í™˜í•œ ê²½ìš° (aggregateì™€ ë¬´ê´€í•¨)
                                    # ë¡œê·¸ ê°„ì†Œí™”: LLMë¬´ì‹œ ë¡œê·¸ ì œê±°
                                    pass  # new_nameì€ Noneìœ¼ë¡œ ìœ ì§€ë˜ì–´ ìë™ ìƒì„± ë¡œì§ìœ¼ë¡œ ì§„í–‰
                            
                            # LLM ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¬´íš¨í•œ ê²½ìš° ìë™ ìƒì„± ì‹œë„
                            if not new_name:
                                # ìë™ ìƒì„±: aggregate_name + "_" + suffix
                                # ì˜ˆ: OrderItem â†’ m_odr_item, CartItem â†’ m_bkt_item, CustomerReference â†’ m_cst_reference
                                
                                # 1. í˜„ì¬ aggregate ì´ë¦„ í™•ì¸ (ë³€í™˜ ì „ ì›ë³¸ ì´ë¦„ ì‚¬ìš©!)
                                # âš ï¸ original_vo_name ì‚¬ìš© (ì„ ì²˜ë¦¬ ì „ ì›ë³¸ ì´ë¦„)
                                if original_agg_name and original_vo_name and original_vo_name.startswith(original_agg_name):
                                    # í˜„ì¬ BCì˜ aggregateê°€ ë³€í™˜ëœ ê²½ìš°
                                    # ì˜ˆ: "OrderItem"ì€ "Order"ë¡œ ì‹œì‘ â†’ "Item" ì¶”ì¶œ
                                    suffix = original_vo_name[len(original_agg_name):]
                                    suffix_snake = re.sub(r'(?<!^)(?=[A-Z])', '_', suffix).lower()
                                    new_name = current_agg_name + "_" + suffix_snake if suffix_snake else current_agg_name + "_vo"
                                    # ë¡œê·¸ ê°„ì†Œí™”: ìë™ë³€í™˜ ë¡œê·¸ ì œê±°
                                    
                                    # ğŸ”§ VO ë‚´ë¶€ í•„ë“œ ë³€í™˜ (VOê°€ ë³€í™˜ë˜ë©´ ë‚´ë¶€ í•„ë“œë„ ë³€í™˜)
                                    # VO ë‚´ë¶€ì— í•„ë“œê°€ ìˆëŠ” ê²½ìš° (ì˜ˆ: previewAttributes, ddlFields)
                                    if "previewAttributes" in orig_vo or "ddlFields" in orig_vo:
                                        # VO ë‚´ë¶€ í•„ë“œë„ aggregate ì´ë¦„ì— ë”°ë¼ ë³€í™˜
                                        vo_preview_attrs = orig_vo.get("previewAttributes", [])
                                        for vo_attr in vo_preview_attrs:
                                            if isinstance(vo_attr, dict):
                                                vo_field_name = vo_attr.get("fieldName", "")
                                                if vo_field_name:
                                                    # aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í•„ë“œëª… ë³€í™˜
                                                    if original_agg_name and vo_field_name.startswith(original_agg_name.lower()):
                                                        # aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í•„ë“œ
                                                        suffix = vo_field_name[len(original_agg_name.lower()):]
                                                        new_field_name = current_agg_name.replace("m_", "fld_") + "_" + suffix if suffix else current_agg_name.replace("m_", "fld_")
                                                        vo_attr["fieldName"] = new_field_name
                                                        # ë¡œê·¸ ê°„ì†Œí™”: VOí•„ë“œë³€í™˜ ë¡œê·¸ ì œê±°
                                        
                                        vo_ddl_fields = orig_vo.get("ddlFields", [])
                                        for vo_ddl_field in vo_ddl_fields:
                                            if isinstance(vo_ddl_field, dict):
                                                vo_field_name = vo_ddl_field.get("fieldName", "")
                                                if vo_field_name:
                                                    if original_agg_name and vo_field_name.startswith(original_agg_name.lower()):
                                                        suffix = vo_field_name[len(original_agg_name.lower()):]
                                                        new_field_name = current_agg_name.replace("m_", "fld_") + "_" + suffix if suffix else current_agg_name.replace("m_", "fld_")
                                                        vo_ddl_field["fieldName"] = new_field_name
                                                        # ë¡œê·¸ ê°„ì†Œí™”: VOí•„ë“œë³€í™˜ ë¡œê·¸ ì œê±°
                                
                                # 2. ë‹¤ë¥¸ BCì˜ aggregate ì´ë¦„ í™•ì¸ (ì°¸ì¡° ê´€ê³„) - original_agg_nameì´ ì—†ì–´ë„ ì‹¤í–‰
                                if (not new_name or new_name == orig_vo_name) and original_vo_name:
                                    # ğŸ”§ combined_aggregate_mapping ì‚¬ìš© (ì „ì—­ ë§¤í•‘ í¬í•¨, ë‹¤ë¥¸ BC ì°¸ì¡° ê°€ëŠ¥)
                                    for mapped_orig_name, mapped_new_name in combined_aggregate_mapping.items():
                                        # VO ì´ë¦„ì´ aggregate ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                                        # ì˜ˆ: "OrderItem"ëŠ” "Order"ë¡œ ì‹œì‘, "CartItem"ëŠ” "Cart"ë¡œ ì‹œì‘, "CustomerReference"ëŠ” "Customer"ë¡œ ì‹œì‘
                                        # âš ï¸ original_vo_name ì‚¬ìš© (ì„ ì²˜ë¦¬ ì „ ì›ë³¸ ì´ë¦„)
                                        if original_vo_name.startswith(mapped_orig_name) and mapped_orig_name != original_vo_name:
                                            # í¬í•¨ëœ aggregate ì´ë¦„ì„ ë³€í™˜ëœ ì´ë¦„ìœ¼ë¡œ êµì²´
                                            suffix = original_vo_name[len(mapped_orig_name):]  # ë‚˜ë¨¸ì§€ ë¶€ë¶„
                                            # snake_case ë³€í™˜
                                            suffix_snake = re.sub(r'(?<!^)(?=[A-Z])', '_', suffix).lower()
                                            new_name = mapped_new_name + "_" + suffix_snake if suffix_snake else mapped_new_name + "_vo"
                                            # ë¡œê·¸ ê°„ì†Œí™”: ì°¸ì¡°ë³€í™˜ ë¡œê·¸ ì œê±°
                                            break
                                
                                # 3. ë§¤í•‘ì´ ì—†ìœ¼ë©´ ì›ë³¸ ìœ ì§€ (aggregateì™€ ë¬´ê´€í•œ ë…ë¦½ì ì¸ VO/Enum)
                                if not new_name:
                                    new_name = original_vo_name  # ì›ë³¸ ì´ë¦„ ìœ ì§€
                                    # ë¡œê·¸ ê°„ì†Œí™”: ì›ë³¸ìœ ì§€ ë¡œê·¸ ì œê±°
                                    pass
                            
                            if new_name and orig_vo_name != new_name:
                                # ğŸ”’ CRITICAL: refs ë³´ì¡´ (ë¹ˆ ë°°ì—´ë„ ë³´ì¡´)
                                preserved_refs = orig_vo.get("refs", [])
                                # original_draft_optionsì—ì„œ ì›ë³¸ refs ê°€ì ¸ì˜¤ê¸°
                                if orig_vo_alias in original_vo_refs:
                                    preserved_refs = original_vo_refs[orig_vo_alias]
                                
                                orig_vo["name"] = new_name
                                # refs ë³µêµ¬ (ë¹ˆ ë°°ì—´ë„ ë³´ì¡´)
                                if "refs" not in orig_vo or not orig_vo.get("refs"):
                                    orig_vo["refs"] = preserved_refs
                                # ë¡œê·¸ ê°„ì†Œí™”: ì´ë¦„ë³€í™˜ ë¡œê·¸ ì œê±°
                        
                        # 4. PreviewAttributes fieldName ë®ì–´ì“°ê¸° (ì¸ë±ìŠ¤ ê¸°ë°˜ - fieldNameì€ ê³ ìœ  ì‹ë³„ì ì—†ìŒ)
                        orig_attrs = orig_item.get("previewAttributes", [])
                        trans_attrs = trans_item.get("previewAttributes", []) if trans_item else []
                        trans_agg_alias = orig_agg_alias  # orig_itemì˜ alias ì‚¬ìš©
                        
                        # ğŸ”’ CRITICAL: original_draft_optionsì—ì„œ ì›ë³¸ refs ë³µì›
                        # fieldAlias ë˜ëŠ” ì›ë³¸ fieldNameì„ í‚¤ë¡œ ì‚¬ìš© (ì¸ë±ìŠ¤ ê¸°ë°˜ì€ LLMì´ ìˆœì„œë¥¼ ë°”ê¿€ ìˆ˜ ìˆì–´ ë¶ˆì•ˆì •)
                        original_attrs_refs = {}  # {fieldAlias or fieldName: refs}
                        if original_draft_options and i < len(original_draft_options):
                            original_opt_structure = original_draft_options[i].get("structure", [])
                            for orig_opt_item in original_opt_structure:
                                if orig_opt_item.get("aggregate", {}).get("alias") == orig_agg_alias:
                                    orig_opt_attrs = orig_opt_item.get("previewAttributes", [])
                                    for orig_opt_attr in orig_opt_attrs:
                                        if isinstance(orig_opt_attr, dict):
                                            # fieldAlias ìš°ì„ , ì—†ìœ¼ë©´ ì›ë³¸ fieldName ì‚¬ìš©
                                            key = orig_opt_attr.get("fieldAlias") or orig_opt_attr.get("fieldName")
                                            if key:
                                                original_attrs_refs[key] = orig_opt_attr.get("refs", [])
                                    break
                        
                        if trans_item and trans_attrs:
                            for attr_idx in range(min(len(orig_attrs), len(trans_attrs))):
                                if isinstance(trans_attrs[attr_idx], dict) and "fieldName" in trans_attrs[attr_idx]:
                                    orig_attr = orig_attrs[attr_idx] if isinstance(orig_attrs[attr_idx], dict) else None
                                    if not orig_attr:
                                        continue
                                    
                                    orig_field = orig_attr.get("fieldName")
                                    new_field = trans_attrs[attr_idx]["fieldName"]
                                    
                                    # ğŸ”’ CRITICAL: fieldAlias ë˜ëŠ” ì›ë³¸ fieldNameìœ¼ë¡œ refs ë§¤ì¹­ (ì¸ë±ìŠ¤ ê¸°ë°˜ì€ ë¶ˆì•ˆì •)
                                    # fieldAlias ìš°ì„  (ë³€í™˜ë˜ì§€ ì•ŠëŠ” í•œê¸€ ì´ë¦„), ì—†ìœ¼ë©´ ì›ë³¸ fieldName ì‚¬ìš©
                                    match_key = orig_attr.get("fieldAlias") or orig_field
                                    if match_key and match_key in original_attrs_refs:
                                        orig_attr["refs"] = original_attrs_refs[match_key]
                                    elif "refs" not in orig_attr:
                                        orig_attr["refs"] = []
                                    
                                    if orig_field and orig_field != new_field:
                                        # ğŸ”§ CRITICAL FIX: ì„ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ìš°ì„  í™•ì¸
                                        # ì„ ì²˜ë¦¬ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì ìš© (deterministic mappingì´ ë” ì •í™•)
                                        preprocessed_field = None
                                        original_field_name = None
                                        if orig_agg_alias and orig_agg_alias in field_name_mapping:
                                            # ì„ ì²˜ë¦¬ ì „ ì›ë³¸ í•„ë“œëª… ì°¾ê¸°
                                            for orig_field_name, preprocessed_field_name in field_name_mapping[orig_agg_alias].items():
                                                if preprocessed_field_name == orig_field:
                                                    # orig_fieldê°€ ì„ ì²˜ë¦¬ ê²°ê³¼ì„
                                                    preprocessed_field = orig_field
                                                    original_field_name = orig_field_name
                                                    break
                                        
                                        # ì„ ì²˜ë¦¬ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì ìš© (LLM ê²°ê³¼ ë¬´ì‹œ)
                                        if preprocessed_field and original_field_name:
                                            # ì„ ì²˜ë¦¬ ê²°ê³¼ ìœ ì§€ (ì´ë¯¸ orig_fieldì— ì ìš©ë˜ì–´ ìˆìŒ)
                                            # ğŸ”’ CRITICAL: refs ë³´ì¡´ (fieldAlias/fieldName ê¸°ë°˜ ë§¤ì¹­)
                                            orig_attr = orig_attrs[attr_idx]
                                            if isinstance(orig_attr, dict):
                                                match_key = orig_attr.get("fieldAlias") or original_field_name
                                                if match_key and match_key in original_attrs_refs:
                                                    orig_attr["refs"] = original_attrs_refs[match_key]
                                                elif "refs" not in orig_attr:
                                                    orig_attr["refs"] = []
                                            # ë¡œê·¸ ê°„ì†Œí™”: ì„ ì²˜ë¦¬ìš°ì„  ë¡œê·¸ ì œê±°
                                            pass
                                        elif orig_field != new_field:
                                            # ì„ ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ì„ ë•Œë§Œ LLM ê²°ê³¼ ì‚¬ìš©
                                            # ğŸ”’ CRITICAL: refs ë³´ì¡´ (fieldAlias/fieldName ê¸°ë°˜ ë§¤ì¹­)
                                            orig_attr = orig_attrs[attr_idx]
                                            if isinstance(orig_attr, dict):
                                                orig_attr["fieldName"] = new_field
                                                # fieldAlias ë˜ëŠ” ì›ë³¸ fieldNameìœ¼ë¡œ ë§¤ì¹­
                                                match_key = orig_attr.get("fieldAlias") or orig_field
                                                if match_key and match_key in original_attrs_refs:
                                                    orig_attr["refs"] = original_attrs_refs[match_key]
                                                elif "refs" not in orig_attr:
                                                    orig_attr["refs"] = []
                                            else:
                                                # ìƒˆë¡œ ìƒì„±í•˜ëŠ” ê²½ìš°ë„ fieldAlias/fieldName ê¸°ë°˜ìœ¼ë¡œ refs ì°¾ê¸°
                                                match_key = orig_field
                                                refs = original_attrs_refs.get(match_key, []) if match_key else []
                                                orig_attrs[attr_idx] = {"fieldName": new_field, "refs": refs}
                                            # ë¡œê·¸ ê°„ì†Œí™”: LLMë³€í™˜ ë¡œê·¸ ì œê±°
                        elif not trans_item or not trans_attrs:
                            # LLM ê²°ê³¼ê°€ ì—†ê±°ë‚˜ trans_attrsê°€ ë¹„ì–´ìˆìœ¼ë©´ ì„ ì²˜ë¦¬ ê²°ê³¼ í™•ì¸
                            # current_opt_structureì—ì„œ ì„ ì²˜ë¦¬ëœ í•„ë“œëª… ê°€ì ¸ì˜¤ê¸°
                            if orig_agg_alias and i < len(draft_options):
                                current_opt_structure = draft_options[i].get("structure", [])
                                for curr_item in current_opt_structure:
                                    curr_alias = curr_item.get("aggregate", {}).get("alias")
                                    if curr_alias == orig_agg_alias:
                                        curr_attrs = curr_item.get("previewAttributes", [])
                                        # ğŸ”’ CRITICAL: original_draft_optionsì—ì„œ ì›ë³¸ refs ë³µì› (fieldAlias/fieldName ê¸°ë°˜)
                                        original_attrs_refs = {}  # {fieldAlias or fieldName: refs}
                                        if original_draft_options and i < len(original_draft_options):
                                            original_opt_structure = original_draft_options[i].get("structure", [])
                                            for orig_opt_item in original_opt_structure:
                                                if orig_opt_item.get("aggregate", {}).get("alias") == orig_agg_alias:
                                                    orig_opt_attrs = orig_opt_item.get("previewAttributes", [])
                                                    for orig_opt_attr in orig_opt_attrs:
                                                        if isinstance(orig_opt_attr, dict):
                                                            key = orig_opt_attr.get("fieldAlias") or orig_opt_attr.get("fieldName")
                                                            if key:
                                                                original_attrs_refs[key] = orig_opt_attr.get("refs", [])
                                                    break
                                        # ì„ ì²˜ë¦¬ëœ í•„ë“œëª… ì ìš©
                                        for attr_idx in range(min(len(orig_attrs), len(curr_attrs))):
                                            if isinstance(orig_attrs[attr_idx], dict) and isinstance(curr_attrs[attr_idx], dict):
                                                orig_attr = orig_attrs[attr_idx]
                                                orig_field = orig_attr.get("fieldName")
                                                curr_field = curr_attrs[attr_idx].get("fieldName")
                                                
                                                # ğŸ”’ CRITICAL: fieldAlias ë˜ëŠ” ì›ë³¸ fieldNameìœ¼ë¡œ refs ë§¤ì¹­
                                                match_key = orig_attr.get("fieldAlias") or orig_field
                                                if match_key and match_key in original_attrs_refs:
                                                    orig_attr["refs"] = original_attrs_refs[match_key]
                                                elif "refs" not in orig_attr:
                                                    orig_attr["refs"] = []
                                                
                                                if orig_field and curr_field and orig_field != curr_field:
                                                    orig_attr["fieldName"] = curr_field
                                                    # ë¡œê·¸ ê°„ì†Œí™”: ì„ ì²˜ë¦¬ì ìš© ë¡œê·¸ ì œê±°
                                        break
                            # ë¡œê·¸ ê°„ì†Œí™”: ì •ë³´ ë¡œê·¸ ì œê±°
                        
                        # 5. DDLFields fieldName ë®ì–´ì“°ê¸° (ì¸ë±ìŠ¤ ê¸°ë°˜ - fieldNameì€ ê³ ìœ  ì‹ë³„ì ì—†ìŒ)
                        orig_ddl_fields = orig_item.get("ddlFields", [])
                        trans_ddl_fields = trans_item.get("ddlFields", []) if trans_item else []
                        
                        # ğŸ”’ CRITICAL: original_draft_optionsì—ì„œ ì›ë³¸ ddlFields refs ë³µì›
                        # fieldAlias ë˜ëŠ” ì›ë³¸ fieldNameì„ í‚¤ë¡œ ì‚¬ìš© (ì¸ë±ìŠ¤ ê¸°ë°˜ì€ LLMì´ ìˆœì„œë¥¼ ë°”ê¿€ ìˆ˜ ìˆì–´ ë¶ˆì•ˆì •)
                        original_ddl_fields_refs = {}  # {fieldAlias or fieldName: refs}
                        if original_draft_options and i < len(original_draft_options):
                            original_opt_structure = original_draft_options[i].get("structure", [])
                            for orig_opt_item in original_opt_structure:
                                if orig_opt_item.get("aggregate", {}).get("alias") == orig_agg_alias:
                                    orig_opt_ddl_fields = orig_opt_item.get("ddlFields", [])
                                    for orig_opt_ddl_field in orig_opt_ddl_fields:
                                        if isinstance(orig_opt_ddl_field, dict):
                                            # fieldAlias ìš°ì„ , ì—†ìœ¼ë©´ ì›ë³¸ fieldName ì‚¬ìš©
                                            key = orig_opt_ddl_field.get("fieldAlias") or orig_opt_ddl_field.get("fieldName")
                                            if key:
                                                original_ddl_fields_refs[key] = orig_opt_ddl_field.get("refs", [])
                                    break
                        
                        if trans_item and trans_ddl_fields:
                            for ddl_idx in range(min(len(orig_ddl_fields), len(trans_ddl_fields))):
                                if isinstance(trans_ddl_fields[ddl_idx], dict) and "fieldName" in trans_ddl_fields[ddl_idx]:
                                    orig_ddl_attr = orig_ddl_fields[ddl_idx] if isinstance(orig_ddl_fields[ddl_idx], dict) else None
                                    if not orig_ddl_attr:
                                        continue
                                    
                                    orig_ddl_field = orig_ddl_attr.get("fieldName")
                                    new_ddl_field = trans_ddl_fields[ddl_idx]["fieldName"]
                                    
                                    # ğŸ”’ CRITICAL: fieldAlias ë˜ëŠ” ì›ë³¸ fieldNameìœ¼ë¡œ refs ë§¤ì¹­ (ì¸ë±ìŠ¤ ê¸°ë°˜ì€ ë¶ˆì•ˆì •)
                                    # fieldAlias ìš°ì„  (ë³€í™˜ë˜ì§€ ì•ŠëŠ” í•œê¸€ ì´ë¦„), ì—†ìœ¼ë©´ ì›ë³¸ fieldName ì‚¬ìš©
                                    match_key = orig_ddl_attr.get("fieldAlias") or orig_ddl_field
                                    if match_key and match_key in original_ddl_fields_refs:
                                        orig_ddl_attr["refs"] = original_ddl_fields_refs[match_key]
                                    elif "refs" not in orig_ddl_attr:
                                        orig_ddl_attr["refs"] = []
                                    
                                    if orig_ddl_field and orig_ddl_field != new_ddl_field:
                                        # ğŸ”§ CRITICAL FIX: ì„ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ìš°ì„  í™•ì¸
                                        # ì„ ì²˜ë¦¬ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì ìš© (deterministic mappingì´ ë” ì •í™•)
                                        preprocessed_field = None
                                        original_field_name = None
                                        if orig_agg_alias and orig_agg_alias in field_name_mapping:
                                            # ì„ ì²˜ë¦¬ ì „ ì›ë³¸ í•„ë“œëª… ì°¾ê¸°
                                            for orig_field_name, preprocessed_field_name in field_name_mapping[orig_agg_alias].items():
                                                if preprocessed_field_name == orig_ddl_field:
                                                    # orig_ddl_fieldê°€ ì„ ì²˜ë¦¬ ê²°ê³¼ì„
                                                    preprocessed_field = orig_ddl_field
                                                    original_field_name = orig_field_name
                                                    break
                                        
                                        # ì„ ì²˜ë¦¬ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì ìš© (LLM ê²°ê³¼ ë¬´ì‹œ)
                                        if preprocessed_field and original_field_name:
                                            # ì„ ì²˜ë¦¬ ê²°ê³¼ ìœ ì§€ (ì´ë¯¸ orig_ddl_fieldì— ì ìš©ë˜ì–´ ìˆìŒ)
                                            # ğŸ”’ CRITICAL: refs ë³´ì¡´ (fieldAlias/fieldName ê¸°ë°˜ ë§¤ì¹­)
                                            orig_ddl_attr = orig_ddl_fields[ddl_idx]
                                            if isinstance(orig_ddl_attr, dict):
                                                match_key = orig_ddl_attr.get("fieldAlias") or original_field_name
                                                if match_key and match_key in original_ddl_fields_refs:
                                                    orig_ddl_attr["refs"] = original_ddl_fields_refs[match_key]
                                                elif "refs" not in orig_ddl_attr:
                                                    orig_ddl_attr["refs"] = []
                                            # ë¡œê·¸ ê°„ì†Œí™”: ì„ ì²˜ë¦¬ìš°ì„  ë¡œê·¸ ì œê±°
                                            pass
                                        elif orig_ddl_field != new_ddl_field:
                                            # ì„ ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ì„ ë•Œë§Œ LLM ê²°ê³¼ ì‚¬ìš©
                                            # ğŸ”’ CRITICAL: refs ë³´ì¡´ (fieldAlias/fieldName ê¸°ë°˜ ë§¤ì¹­)
                                            orig_ddl_attr = orig_ddl_fields[ddl_idx]
                                            if isinstance(orig_ddl_attr, dict):
                                                orig_ddl_attr["fieldName"] = new_ddl_field
                                                # fieldAlias ë˜ëŠ” ì›ë³¸ fieldNameìœ¼ë¡œ ë§¤ì¹­
                                                match_key = orig_ddl_attr.get("fieldAlias") or orig_ddl_field
                                                if match_key and match_key in original_ddl_fields_refs:
                                                    orig_ddl_attr["refs"] = original_ddl_fields_refs[match_key]
                                                elif "refs" not in orig_ddl_attr:
                                                    orig_ddl_attr["refs"] = []
                                            else:
                                                # ìƒˆë¡œ ìƒì„±í•˜ëŠ” ê²½ìš°ë„ fieldAlias/fieldName ê¸°ë°˜ìœ¼ë¡œ refs ì°¾ê¸°
                                                match_key = orig_ddl_field
                                                refs = original_ddl_fields_refs.get(match_key, []) if match_key else []
                                                orig_ddl_fields[ddl_idx] = {"fieldName": new_ddl_field, "refs": refs}
                                            # ë¡œê·¸ ê°„ì†Œí™”: LLMë³€í™˜ ë¡œê·¸ ì œê±°
                        elif not trans_item or not trans_ddl_fields:
                            # LLM ê²°ê³¼ê°€ ì—†ê±°ë‚˜ trans_ddl_fieldsê°€ ë¹„ì–´ìˆìœ¼ë©´ ì„ ì²˜ë¦¬ ê²°ê³¼ í™•ì¸
                            # current_opt_structureì—ì„œ ì„ ì²˜ë¦¬ëœ í•„ë“œëª… ê°€ì ¸ì˜¤ê¸°
                            if orig_agg_alias and i < len(draft_options):
                                current_opt_structure = draft_options[i].get("structure", [])
                                for curr_item in current_opt_structure:
                                    curr_alias = curr_item.get("aggregate", {}).get("alias")
                                    if curr_alias == orig_agg_alias:
                                        curr_ddl_fields = curr_item.get("ddlFields", [])
                                        # ğŸ”’ CRITICAL: original_draft_optionsì—ì„œ ì›ë³¸ ddlFields refs ë³µì› (fieldAlias/fieldName ê¸°ë°˜)
                                        original_ddl_fields_refs = {}  # {fieldAlias or fieldName: refs}
                                        if original_draft_options and i < len(original_draft_options):
                                            original_opt_structure = original_draft_options[i].get("structure", [])
                                            for orig_opt_item in original_opt_structure:
                                                if orig_opt_item.get("aggregate", {}).get("alias") == orig_agg_alias:
                                                    orig_opt_ddl_fields = orig_opt_item.get("ddlFields", [])
                                                    for orig_opt_ddl_field in orig_opt_ddl_fields:
                                                        if isinstance(orig_opt_ddl_field, dict):
                                                            key = orig_opt_ddl_field.get("fieldAlias") or orig_opt_ddl_field.get("fieldName")
                                                            if key:
                                                                original_ddl_fields_refs[key] = orig_opt_ddl_field.get("refs", [])
                                                    break
                                        # ì„ ì²˜ë¦¬ëœ í•„ë“œëª… ì ìš©
                                        for ddl_idx in range(min(len(orig_ddl_fields), len(curr_ddl_fields))):
                                            if isinstance(orig_ddl_fields[ddl_idx], dict) and isinstance(curr_ddl_fields[ddl_idx], dict):
                                                orig_ddl_attr = orig_ddl_fields[ddl_idx]
                                                orig_field = orig_ddl_attr.get("fieldName")
                                                curr_field = curr_ddl_fields[ddl_idx].get("fieldName")
                                                
                                                # ğŸ”’ CRITICAL: fieldAlias ë˜ëŠ” ì›ë³¸ fieldNameìœ¼ë¡œ refs ë§¤ì¹­
                                                match_key = orig_ddl_attr.get("fieldAlias") or orig_field
                                                if match_key and match_key in original_ddl_fields_refs:
                                                    orig_ddl_attr["refs"] = original_ddl_fields_refs[match_key]
                                                elif "refs" not in orig_ddl_attr:
                                                    orig_ddl_attr["refs"] = []
                                                
                                                if orig_field and curr_field and orig_field != curr_field:
                                                    orig_ddl_attr["fieldName"] = curr_field
                                                    # ë¡œê·¸ ê°„ì†Œí™”: ì„ ì²˜ë¦¬ì ìš© ë¡œê·¸ ì œê±°
                                        break
                            # ë¡œê·¸ ê°„ì†Œí™”: ì •ë³´ ë¡œê·¸ ì œê±°
                            pass
                    
                    # ğŸ”’ CRITICAL: original_draft_optionsì—ì„œ í•„í„°ë§ëœ í•„ë“œ ë³µì›
                    if original_draft_options and i < len(original_draft_options):
                        original_option = original_draft_options[i]
                        
                        # boundedContext ë³µì› (aggregatesëŠ” ì œì™¸, descriptionë„ ì œì™¸ - ì›ë˜ ì—†ì—ˆìŒ)
                        if "boundedContext" in original_option:
                            orig_bc = original_option["boundedContext"]
                            if "boundedContext" not in merged_option:
                                merged_option["boundedContext"] = {}
                            
                            # aggregatesëŠ” ì´ë¯¸ ì—…ë°ì´íŠ¸ëœ ê²ƒ ì‚¬ìš©, descriptionì€ ì œì™¸
                            for key, value in orig_bc.items():
                                if key not in ["aggregates", "description"]:  # aggregatesëŠ” ì´ë¯¸ ì—…ë°ì´íŠ¸ë¨, descriptionì€ ì›ë˜ ì—†ì—ˆìŒ
                                    merged_option["boundedContext"][key] = value
                        
                        # pros ë³µì›
                        if "pros" in original_option:
                            merged_option["pros"] = original_option["pros"]
                        elif "pros" in transformed_option:
                            # LLM ê²°ê³¼ì— ìˆìœ¼ë©´ ì‚¬ìš© (fallback)
                            merged_option["pros"] = transformed_option["pros"]
                        
                        # cons ë³µì›
                        if "cons" in original_option:
                            merged_option["cons"] = original_option["cons"]
                        elif "cons" in transformed_option:
                            # LLM ê²°ê³¼ì— ìˆìœ¼ë©´ ì‚¬ìš© (fallback)
                            merged_option["cons"] = transformed_option["cons"]
                        
                        # description ë³µì› (ì˜µì…˜ ë ˆë²¨)
                        if "description" in original_option:
                            merged_option["description"] = original_option["description"]
                        
                        # ê¸°íƒ€ í•„ë“œ ë³µì› (original_optionì— ìˆì§€ë§Œ merged_optionì— ì—†ëŠ” ëª¨ë“  í•„ë“œ)
                        for key, value in original_option.items():
                            if key not in ["structure", "boundedContext", "pros", "cons", "description"]:
                                # structureëŠ” ì´ë¯¸ ì²˜ë¦¬ë¨, ë‚˜ë¨¸ì§€ëŠ” ë³µì›
                                if key not in merged_option:
                                    merged_option[key] = value
                    else:
                        # original_draft_optionsê°€ ì—†ìœ¼ë©´ LLM ê²°ê³¼ ì‚¬ìš© (fallback)
                        if "pros" in transformed_option:
                            merged_option["pros"] = transformed_option["pros"]
                        if "cons" in transformed_option:
                            merged_option["cons"] = transformed_option["cons"]
                    
                    # boundedContext.aggregates[].nameë„ structure[].aggregate.nameê³¼ ë™ê¸°í™”
                    if "boundedContext" in merged_option and "aggregates" in merged_option["boundedContext"]:
                        bc_aggregates = merged_option["boundedContext"]["aggregates"]
                        result_structure = merged_option["structure"]
                        # structureì˜ aggregate.nameìœ¼ë¡œ boundedContext.aggregates[].name ì—…ë°ì´íŠ¸
                        for idx, structure_item in enumerate(result_structure):
                            if idx < len(bc_aggregates):
                                structure_agg_name = structure_item.get("aggregate", {}).get("name")
                                if structure_agg_name:
                                    bc_aggregates[idx]["name"] = structure_agg_name
                                    # ë¡œê·¸ ê°„ì†Œí™”: ë™ê¸°í™” ë¡œê·¸ ì œê±°
                                    pass
                    
                    # referencedAggregateName ì—…ë°ì´íŠ¸ ë° referencedAggregate ê°ì²´ ìƒì„±
                    result_structure = merged_option["structure"]
                    
                    # aggregate ì´ë¦„ ë§¤í•‘ ìƒì„± (ì›ë³¸ê³¼ ê²°ê³¼ ë¹„êµ)
                    aggregate_name_mapping = {}
                    for idx in range(len(result_structure)):
                        # ê²°ê³¼ êµ¬ì¡°ì˜ í˜„ì¬ ì´ë¦„ (ì´ë¯¸ LLM ê²°ê³¼ë¡œ ë®ì–´ì”Œì›Œì§)
                        current_agg_name = result_structure[idx].get("aggregate", {}).get("name", "")
                        # ì›ë³¸ ì´ë¦„ (ë³€í™˜ ì „)
                        if idx < len(original_structure):
                            original_agg_name = original_structure[idx].get("aggregate", {}).get("name", "")
                            if original_agg_name and current_agg_name and original_agg_name != current_agg_name:
                                aggregate_name_mapping[original_agg_name] = current_agg_name
                                LoggingUtil.info("StandardTransformer", 
                                               f"   [LLM ë³€í™˜ ê²°ê³¼] Aggregate '{original_agg_name}' â†’ '{current_agg_name}'")
                    
                    # ëª¨ë“  VOì˜ referencedAggregateName ì—…ë°ì´íŠ¸
                    for structure_item in result_structure:
                        value_objects = structure_item.get("valueObjects", [])
                        for vo in value_objects:
                            ref_agg_name = vo.get("referencedAggregateName")
                            if ref_agg_name:
                                # referencedAggregateNameì´ ë³€í™˜ëœ aggregateë¥¼ ì°¸ì¡°í•˜ë©´ ì—…ë°ì´íŠ¸
                                if ref_agg_name in aggregate_name_mapping:
                                    new_ref_agg_name = aggregate_name_mapping[ref_agg_name]
                                    vo["referencedAggregateName"] = new_ref_agg_name
                                    LoggingUtil.info("StandardTransformer", 
                                                   f"   [ì°¸ì¡°ì—…ë°ì´íŠ¸] VO '{vo.get('alias')}' ref: '{ref_agg_name}' â†’ '{new_ref_agg_name}'")
                    
                    merged_options.append(merged_option)
                else:
                    # ì›ë³¸ì´ ì—†ëŠ” ê²½ìš° ë³€í™˜ëœ ì˜µì…˜ ì‚¬ìš©
                    merged_options.append(transformed_option)
            
            # ì²« ë²ˆì§¸ ì˜µì…˜ êµ¬ì¡° í™•ì¸
            if merged_options:
                first_option = merged_options[0]
                LoggingUtil.info("StandardTransformer", 
                               f"   ì²« ë²ˆì§¸ ì˜µì…˜ í‚¤: {list(first_option.keys()) if isinstance(first_option, dict) else 'N/A'}")
                if isinstance(first_option, dict) and "structure" in first_option:
                    structure = first_option.get("structure", [])
                    LoggingUtil.info("StandardTransformer", 
                                   f"   ì²« ë²ˆì§¸ ì˜µì…˜ structure í•­ëª© ìˆ˜: {len(structure)}ê°œ")
                    if structure and isinstance(structure[0], dict):
                        LoggingUtil.info("StandardTransformer", 
                                       f"   ì²« ë²ˆì§¸ structure í•­ëª© í‚¤: {list(structure[0].keys())}")
            
            return merged_options
        except Exception as e:
            LoggingUtil.error("StandardTransformer", f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            LoggingUtil.error("StandardTransformer", traceback.format_exc())
            # ì›ë³¸ ë°˜í™˜
            return draft_options
    
    def _build_transformation_prompt(self, draft_options: List[Dict],
                                   bounded_context: Dict,
                                   relevant_standards: List[Dict],
                                   query_search_results: Optional[List[Dict]] = None) -> str:
        """
        ë³€í™˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        """
        # í‘œì¤€ ë¬¸ì„œ í¬ë§·íŒ…
        standards_text = ""
        
        # ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‘œì¤€ì „í™˜ ëŒ€ìƒ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (top-k=3 ê²°ê³¼ ëª¨ë‘ í¬í•¨)
        # ì¿¼ë¦¬ë¥¼ keyë¡œ í•˜ê³ , í•´ë‹¹ ì¿¼ë¦¬ì˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ valueë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°
        transformed_query_results = {}
        if query_search_results:
            for qr in query_search_results:
                query = qr.get("query", "")
                if not query:
                    continue
                
                # top-k=3 ê²°ê³¼ê°€ "results" ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬ë¨
                if "results" in qr:
                    results_list = qr["results"]
                    # ê° ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (resultë§Œ í¬í•¨, similarity_score ì œê±°)
                    query_results = []
                    for result_item in results_list:
                        query_results.append(result_item.get("result", {}))
                    transformed_query_results[query] = query_results
                else:
                    # í•˜ìœ„ í˜¸í™˜ì„±: ê¸°ì¡´ í˜•ì‹ (ë‹¨ì¼ result)
                    transformed_query_results[query] = [qr.get("result", {})]
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í‘œì¤€ ë³€í™˜ ì •ë³´ ì¶”ê°€
        if transformed_query_results:
            standards_text += "\n\n## Standard Transformation Reference:\n\n"
            standards_text += "The following JSON contains search results from the company standards database.\n"
            standards_text += "Each query (standard transformation target) has up to 3 candidate results.\n\n"
            standards_text += "**JSON Structure**:\n"
            standards_text += "- Each key is a search query (e.g., \"Order ì£¼ë¬¸\", \"customer_id ê³ ê°ID\")\n"
            standards_text += "- Each value is a list of search results for that query\n"
            standards_text += "- Each result contains standard information\n\n"
            standards_text += "**Matching and Selection Rules**:\n"
            standards_text += "- If input `name` or `fieldName` matches or is contained in a JSON key, consider transformation\n"
            standards_text += "- Example: Input `name: \"Order\"` matches key `\"Order ì£¼ë¬¸\"`, Input `fieldName: \"customer_id\"` matches key `\"customer_id ê³ ê°ID\"`\n"
            standards_text += "- If matched, select the most appropriate item from the value list based on context and meaning, then use its `í‘œì¤€ëª…` (standard name)\n"
            standards_text += "- Standard name format: `\"m_cst\"`, `\"fld_cst_id\"`, `\"fld_odr_amt\"` (not common names)\n"
            standards_text += "- If no match or inappropriate, keep the original unchanged\n\n"
            
            # ë³€í™˜ëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì „ë‹¬ (ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°)
            standards_text += "```json\n"
            standards_text += json.dumps(transformed_query_results, ensure_ascii=False, indent=2)
            standards_text += "\n```\n\n"
        else:
            standards_text = "\n\nâš ï¸  **CRITICAL: No standard transformation information found.**\n\n"
            standards_text += "**STRICT REQUIREMENT**: Since no standard information is available:\n"
            standards_text += "- **DO NOT transform names** - keep original names as they are\n"
            standards_text += "- **DO NOT invent or guess standard names**\n"
            standards_text += "- **DO NOT apply general naming conventions (camelCase, etc.)**\n"
            standards_text += "- **Keep ALL names EXACTLY as they are in the input**\n"
            standards_text += "- **This means: aggregate.name, enum.name, vo.name, field.fieldName should ALL remain unchanged**\n\n"
            standards_text += "**REASON**: Vector Store indexing may have failed or no relevant standards were found.\n"
            standards_text += "Without company standards, transformation should NOT occur.\n\n"
            LoggingUtil.warning("StandardTransformer", 
                              f"âš ï¸  í‘œì¤€ ì „í™˜ ì •ë³´ ì—†ìŒ: query_search_resultsê°€ ë¹„ì–´ìˆìŒ - LLMì—ê²Œ ì›ë³¸ ìœ ì§€ ì§€ì‹œ")
        
        # Bounded Context ì •ë³´
        bc_name = bounded_context.get("name", "")
        bc_alias = bounded_context.get("alias", "")
        bc_desc = bounded_context.get("description", "")
        
        # ê° ì˜µì…˜ì˜ aggregate ìˆ˜ ê³„ì‚° ë° ëª…ì‹œ
        option_aggregate_counts = []
        for i, option in enumerate(draft_options):
            structure = option.get("structure", [])
            aggregate_count = len(structure)
            option_aggregate_counts.append(aggregate_count)
        
        aggregate_counts_text = "\n".join([
            f"- Option {i}: {count} aggregates in structure array"
            for i, count in enumerate(option_aggregate_counts)
        ])
        
        # ì¶”ê°€ ì˜µì…˜ë“¤ì˜ aggregate ìˆ˜ ìš”êµ¬ì‚¬í•­ í…ìŠ¤íŠ¸ ìƒì„±
        additional_options_text = ""
        if len(option_aggregate_counts) > 1:
            additional_lines = []
            for i, count in enumerate(option_aggregate_counts[1:], start=1):
                additional_lines.append(f"- Option {i} MUST have EXACTLY {count} aggregates in its structure array.")
            additional_options_text = "\n".join(additional_lines)
        
        # ì„¹ì…˜ 5ì˜ ì¶”ê°€ ì˜µì…˜ í…ìŠ¤íŠ¸ ìƒì„±
        section5_additional = ""
        if len(option_aggregate_counts) > 1:
            section5_lines = []
            for i, count in enumerate(option_aggregate_counts[1:], start=1):
                section5_lines.append(f"     * Option {i}: MUST have {count} aggregates (same as original)")
            section5_additional = "\n".join(section5_lines)
        
        # Output Format ì„¹ì…˜ì˜ ì¶”ê°€ ì˜µì…˜ í…ìŠ¤íŠ¸ ìƒì„±
        output_format_additional = ""
        if len(option_aggregate_counts) > 1:
            output_lines = []
            for i, count in enumerate(option_aggregate_counts[1:], start=1):
                output_lines.append(f"  * Option {i}: MUST have EXACTLY {count} aggregates")
            output_format_additional = "\n".join(output_lines)
        
        # Final Reminder ì„¹ì…˜ì˜ ì¶”ê°€ ì˜µì…˜ í…ìŠ¤íŠ¸ ìƒì„±
        reminder_additional = ""
        if len(option_aggregate_counts) > 1:
            reminder_lines = []
            for i, (j, count) in enumerate(enumerate(option_aggregate_counts[1:], start=1), start=3):
                reminder_lines.append(f"{i}. Option {j} MUST have EXACTLY {count} aggregates in its structure array.")
            reminder_additional = "\n".join(reminder_lines)
            final_reminder_num = len(option_aggregate_counts) + 2
        else:
            final_reminder_num = 3
        
        prompt = f"""You are a DDD Standard Compliance Specialist. Transform aggregate names to match company standards.

## Task: Transform ONLY the `name` and `fieldName` fields. Keep EVERYTHING else EXACTLY unchanged.

## Input:
{json.dumps(draft_options, ensure_ascii=False, indent=2)}

{standards_text}

## CRITICAL RULES (READ CAREFULLY):

**âš ï¸ STRUCTURE PRESERVATION (MOST IMPORTANT):**
- **MUST preserve EXACT structure for EACH option**: 
{aggregate_counts_text}
- **MUST preserve ALL options**: Input has {len(draft_options)} options â†’ Output MUST have EXACTLY {len(draft_options)} options
- **âš ï¸ CRITICAL: Each option's `structure` array MUST contain ALL aggregates from the input**:
  * If input has 3 aggregates in structure array, output MUST have EXACTLY 3 aggregates in structure array
  * DO NOT split aggregates into separate options
  * DO NOT merge aggregates into one
  * ALL aggregates must remain in the SAME structure array within the SAME option
- **MUST preserve ALL arrays**: `previewAttributes`, `ddlFields`, `enumerations`, `valueObjects` - keep ALL items, only transform `name`/`fieldName` values
- **MUST preserve ALL fields**: Every field in input must exist in output (only `name` and `fieldName` values may change)

**WHAT TO TRANSFORM (ALL EQUALLY IMPORTANT):**
- `aggregate.name` - Transform aggregate names
- `enumerations[].name` - Transform enumeration names  
- `valueObjects[].name` - Transform value object names
- `previewAttributes[].fieldName` - Transform preview field names (check EVERY field in the array)
- `ddlFields[].fieldName` - Transform DDL field names (check EVERY field in the array)

**Transformation Process:**
- For each `name` or `fieldName`, check if it matches a key in "Standard Transformation Reference"
- If matched, transform using the most appropriate `í‘œì¤€ëª…` from the candidate list
- If no match, keep original unchanged
- Apply this process to ALL transformation targets listed above - do not skip any

**WHAT TO KEEP UNCHANGED (CRITICAL - DO NOT MODIFY):**
- ğŸ”’ **ALL `alias` fields (Korean text) - NEVER CHANGE THESE!** They are used for matching and traceability.
- **ALL array structures** - Keep ALL items in `previewAttributes`, `ddlFields`, `enumerations`, `valueObjects`
- **ALL aggregate count for EACH option** - 
{aggregate_counts_text}
- **ALL other fields** - `className`, `type`, `referencedAggregateName`, etc. - keep unchanged

**âš ï¸ CRITICAL WARNING:**
- **DO NOT change any `alias` field** - they must match the input exactly
- **DO NOT translate or modify Korean text in `alias` fields**
- **ONLY transform `name` and `fieldName` fields**

**NOTE:** Input has been pre-processed to remove tracking fields (refs, description, pros, cons). You don't need to include them in output - they will be restored automatically.

**TRANSFORMATION RULES:**

1. Matching: If input `name` or `fieldName` is contained in or matches a JSON key in "Standard Transformation Reference", it matches.
   - Example: `"Order"` matches key `"Order ì£¼ë¬¸"`, `"customer_id"` matches key `"customer_id ê³ ê°ID"`

2. Transformation: If matched, select the most appropriate item from the value list based on context and meaning, then use its `í‘œì¤€ëª…` (standard name).
   - Standard name format: `"m_cst"`, `"fld_cst_id"`, `"fld_odr_amt"` (not common names)
   - Common names: `"Customer"`, `"customer_id"`, `"order_amount"` (not standard names)

3. Transformation Targets (ALL must be checked and transformed if matched):
   - `aggregate.name` - Transform aggregate names
   - `enumerations[].name` - Transform enumeration names
   - `valueObjects[].name` - Transform value object names
   - `previewAttributes[].fieldName` - Transform preview field names (CRITICAL: Check every field)
   - `ddlFields[].fieldName` - Transform DDL field names (CRITICAL: Check every field)

4. Parent-Child Relationship: If parent aggregate name is transformed, child names (enumerations, valueObjects) should use the transformed parent name as prefix.

5. No Match: Keep original unchanged.

## CRITICAL: What You MUST Preserve:

**DO NOT REMOVE:**
- âœ… ALL aggregates ({option_aggregate_counts[0] if option_aggregate_counts else 0} total)
- âœ… `previewAttributes` array (if exists, keep ALL items)
- âœ… ALL `alias` fields

**Where to Transform (ONLY these fields - ALL must be processed):**
- `structure[].aggregate.name` - Aggregate name
- `structure[].enumerations[].name` - Enumeration name
- `structure[].valueObjects[].name` - ValueObject name
- `structure[].previewAttributes[].fieldName` - Preview field name (process ALL fields in the array)
- `structure[].ddlFields[].fieldName` - DDL field name (process ALL fields in the array)

**IMPORTANT:** You must check and transform fields in `previewAttributes` and `ddlFields` arrays just like you do for aggregates, enumerations, and value objects. Do not skip field transformations.

## Output Format:

Return JSON with EXACT structure as input, ONLY changing `name`/`fieldName` values based on the "Standard Transformation Reference" section above.
Each option must preserve the EXACT structure from input.
Only transform names/fieldNames that match keys in the reference JSON above.

## FINAL CHECK (VERIFY BEFORE OUTPUT):
1. âœ… **Count aggregates in structure array**: Input has {option_aggregate_counts[0] if option_aggregate_counts else 0} aggregates in structure array â†’ Output MUST have EXACTLY {option_aggregate_counts[0] if option_aggregate_counts else 0} aggregates in structure array
   * **CRITICAL**: ALL aggregates must be in the SAME structure array, NOT split across multiple options
   * **CRITICAL**: If input has 3 aggregates (Customer, Cart, Order), output MUST have 3 aggregates in structure array
2. âœ… `previewAttributes` array: Output MUST always include this field (empty array `[]` if input doesn't have it)
   * **CRITICAL**: If input has `previewAttributes`, output MUST have it with ALL items (only `fieldName` values may change)
   * **CRITICAL**: Check EVERY field in `previewAttributes` array and transform `fieldName` if it matches Standard Transformation Reference
3. âœ… `ddlFields` array: Output MUST always include this field (empty array `[]` if input doesn't have it)
   * **CRITICAL**: If input has `ddlFields`, output MUST have it with ALL items (only `fieldName` values may change)
   * **CRITICAL**: Check EVERY field in `ddlFields` array and transform `fieldName` if it matches Standard Transformation Reference
4. âœ… `enumerations` array: If input has it, output MUST have it with ALL items (only `name` values may change)
5. âœ… `valueObjects` array: If input has it, output MUST have it with ALL items (only `name` values may change)
6. âœ… `boundedContext.aggregates[].name` = `structure[].aggregate.name` (must match - same count and order)
7. âœ… ALL `alias` fields unchanged (Korean text preserved exactly)

**âš ï¸ CRITICAL**: If you cannot preserve the exact structure, return the input unchanged rather than losing data!

**FINAL INSTRUCTION**: 
Transform `name` and `fieldName` values based on the "Standard Transformation Reference" section above.
If no match or inappropriate match is found, keep the original unchanged.
"""
        return prompt
    
    def _save_transformation_results(self, job_id: str, 
                                     draft_options: List[Dict], 
                                     transformed_options: List[Dict],
                                     bounded_context: Dict,
                                     search_info: Optional[Dict] = None) -> None:
        """
        ë³€í™˜ ì „í›„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            job_id: Job ID ë˜ëŠ” transformationSessionId (ë””ë ‰í† ë¦¬ëª…ìœ¼ë¡œ ì‚¬ìš©)
            draft_options: ë³€í™˜ ì „ ì˜µì…˜ë“¤
            transformed_options: ë³€í™˜ í›„ ì˜µì…˜ë“¤
            bounded_context: Bounded Context ì •ë³´
        """
        try:
            # result ë””ë ‰í† ë¦¬ ìƒì„±
            result_dir = Config._project_root / 'result' / job_id
            result_dir.mkdir(parents=True, exist_ok=True)
            
            # BC ì´ë¦„ì„ íŒŒì¼ëª…ì— í¬í•¨ (ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜)
            bc_name = bounded_context.get("name", "unknown")
            bc_name_safe = "".join(c if c.isalnum() or c in ('-', '_') else '_' for c in bc_name)
            
            LoggingUtil.info("StandardTransformer", 
                          f"ğŸ’¾ ë³€í™˜ ê²°ê³¼ ì €ì¥ ì¤‘: {result_dir} (BC: {bc_name})")
            
            # ê° ì˜µì…˜ë³„ë¡œ ë³€í™˜ ì „í›„ JSON ì €ì¥ (í•­ìƒ ì˜µì…˜ì€ í•˜ë‚˜ë¿ì´ë¯€ë¡œ option_0 ì œê±°)
            max_options = max(len(draft_options), len(transformed_options))
            
            for i in range(max_options):
                # ë³€í™˜ ì „ ì˜µì…˜ ì €ì¥ (BC ì´ë¦„ í¬í•¨)
                if i < len(draft_options):
                    before_file = result_dir / f'{bc_name_safe}_before.json'
                    with open(before_file, 'w', encoding='utf-8') as f:
                        json.dump(draft_options[i], f, ensure_ascii=False, indent=2)
                    LoggingUtil.info("StandardTransformer", 
                                  f"   ì €ì¥ë¨: {before_file.name}")
                
                # ë³€í™˜ í›„ ì˜µì…˜ ì €ì¥ (BC ì´ë¦„ í¬í•¨)
                if i < len(transformed_options):
                    after_file = result_dir / f'{bc_name_safe}_after.json'
                    with open(after_file, 'w', encoding='utf-8') as f:
                        json.dump(transformed_options[i], f, ensure_ascii=False, indent=2)
                    LoggingUtil.info("StandardTransformer", 
                                  f"   ì €ì¥ë¨: {after_file.name}")
            
            # BCë³„ ìš”ì•½ ì •ë³´ ì €ì¥ (BC ì´ë¦„ í¬í•¨)
            summary = {
                "job_id": job_id,
                "bounded_context_name": bc_name,
                "bounded_context": {
                    "name": bounded_context.get("name", ""),
                    "alias": bounded_context.get("alias", ""),
                    "description": bounded_context.get("description", "")
                },
                "transformation_timestamp": datetime.now().isoformat()
            }
            
            # ê²€ìƒ‰ëœ í‘œì¤€ ì •ë³´ ì¶”ê°€
            if search_info:
                # ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ (top-k=3) ì €ì¥
                if search_info.get("query_search_results"):
                    query_search_results = search_info["query_search_results"]
                    summary["search_queries"] = []
                    for qr in query_search_results:
                        query = qr.get("query", "")
                        # top-k=3 ê²°ê³¼ê°€ "results" ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬ë¨
                        if "results" in qr:
                            results_list = qr["results"]
                            for result_item in results_list:
                                query_info = {
                                    "query": query,
                                    "similarity_score": result_item.get("similarity_score", 0.0),
                                    "result": result_item.get("result", {})
                                }
                                summary["search_queries"].append(query_info)
                        else:
                            # í•˜ìœ„ í˜¸í™˜ì„±: ê¸°ì¡´ í˜•ì‹ (ë‹¨ì¼ result)
                            query_info = {
                                "query": query,
                                "similarity_score": qr.get("similarity_score", 0.0),
                                "result": qr.get("result", {})
                            }
                            summary["search_queries"].append(query_info)
                
                # ìœ ì‚¬ë„ ê²€ìƒ‰ ì±„íƒ ê²°ê³¼ (í‚¤ì›Œë“œ, ì¿¼ë¦¬, ìœ ì‚¬ë„, ì¸ë±ì‹± ë‚´ìš©)
                summary["rag_search_results"] = []
                if search_info.get("query_search_results"):
                    query_search_results = search_info["query_search_results"]
                    for qr in query_search_results:
                        query = qr.get("query", "")
                        # top-k=3 ê²°ê³¼ê°€ "results" ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬ë¨
                        if "results" in qr:
                            results_list = qr["results"]
                            for result_item in results_list:
                                if result_item.get("result"):
                                    summary["rag_search_results"].append({
                                        "query": query,
                                        "similarity_score": result_item.get("similarity_score", 0.0),
                                        "result": result_item.get("result", {})
                                    })
                        else:
                            # í•˜ìœ„ í˜¸í™˜ì„±: ê¸°ì¡´ í˜•ì‹ (ë‹¨ì¼ result)
                            if qr.get("result"):
                                summary["rag_search_results"].append({
                                    "query": query,
                                    "similarity_score": qr.get("similarity_score", 0.0),
                                    "result": qr.get("result", {})
                                })
                
                # ì „ì²˜ë¦¬ ë§¤í•‘ ì •ë³´ (ì‹¤ì œë¡œ ë³€í™˜ëœ ê²ƒë§Œ í‘œì‹œ)
                if search_info.get("mapping_context"):
                    mapping_ctx = search_info["mapping_context"]
                    
                    # ì‹¤ì œë¡œ ì„ ì²˜ë¦¬ ë§¤í•‘ìœ¼ë¡œ ë³€í™˜ëœ í•­ëª©ë§Œ ì¶”ì¶œ
                    # âš ï¸ ì¤‘ìš”: mapped_draft_optionsëŠ” ì„ ì²˜ë¦¬ ë§¤í•‘ í›„ ê²°ê³¼ì´ë¯€ë¡œ, ì„ ì²˜ë¦¬ ë§¤í•‘ë§Œ ì¶”ì¶œ
                    actual_table_mappings = {}
                    actual_column_mappings = {}
                    
                    # ì„ ì²˜ë¦¬ ë§¤í•‘ë§Œ ì¶”ì¶œ: original_draft_optionsì™€ mapped_draft_options ë¹„êµ
                    if search_info.get("mapped_draft_options") and len(draft_options) > 0:
                        # draft_optionsëŠ” original_draft_optionsì™€ ë™ì¼ (ì„ ì²˜ë¦¬ ì „ ì›ë³¸)
                        # mapped_draft_optionsëŠ” ì„ ì²˜ë¦¬ ë§¤í•‘ í›„ ê²°ê³¼
                        for i, (original, mapped) in enumerate(zip(draft_options, search_info["mapped_draft_options"])):
                            original_structure = original.get("structure", [])
                            mapped_structure = mapped.get("structure", [])
                            
                            # Aggregate ì´ë¦„ ë³€í™˜ ìˆ˜ì§‘ (ì„ ì²˜ë¦¬ ë§¤í•‘ë§Œ)
                            for orig_item, mapped_item in zip(original_structure, mapped_structure):
                                orig_agg = orig_item.get("aggregate", {})
                                mapped_agg = mapped_item.get("aggregate", {})
                                orig_name = orig_agg.get("name", "")
                                mapped_name = mapped_agg.get("name", "")
                                
                                # ì„ ì²˜ë¦¬ ë§¤í•‘ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                                # mapping_contextì˜ table_standardsì— ìˆëŠ”ì§€ í™•ì¸
                                if orig_name != mapped_name and mapping_ctx:
                                    # ì„ ì²˜ë¦¬ ë§¤í•‘ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                                    orig_alias = orig_agg.get("alias", "")
                                    is_preprocessing_mapping = False
                                    
                                    # aliasë¡œ ë§¤í•‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
                                    if orig_alias and orig_alias in mapping_ctx["table"]["table_standards"]:
                                        if mapping_ctx["table"]["table_standards"][orig_alias] == mapped_name:
                                            is_preprocessing_mapping = True
                                    
                                    # nameìœ¼ë¡œ ë§¤í•‘ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ëŒ€ì†Œë¬¸ì ë³€í˜• í¬í•¨)
                                    if not is_preprocessing_mapping and orig_name:
                                        name_variants = [orig_name, orig_name.lower(), orig_name.upper(), orig_name.capitalize()]
                                        for variant in name_variants:
                                            if variant in mapping_ctx["table"]["table_standards"]:
                                                if mapping_ctx["table"]["table_standards"][variant] == mapped_name:
                                                    is_preprocessing_mapping = True
                                                    break
                                    
                                    # ì„ ì²˜ë¦¬ ë§¤í•‘ìœ¼ë¡œ ë³€í™˜ëœ ê²½ìš°ë§Œ ì¶”ê°€
                                    if is_preprocessing_mapping:
                                        actual_table_mappings[orig_name] = mapped_name
                                        if orig_alias:
                                            actual_table_mappings[orig_alias] = mapped_name
                                
                                # Enum/VO ì´ë¦„ ë³€í™˜ ìˆ˜ì§‘ (ì„ ì²˜ë¦¬ ë§¤í•‘ë§Œ)
                                orig_enums = orig_item.get("enumerations", [])
                                mapped_enums = mapped_item.get("enumerations", [])
                                for orig_enum, mapped_enum in zip(orig_enums, mapped_enums):
                                    orig_enum_name = orig_enum.get("name", "")
                                    mapped_enum_name = mapped_enum.get("name", "")
                                    if orig_enum_name != mapped_enum_name and mapping_ctx:
                                        # ì„ ì²˜ë¦¬ ë§¤í•‘ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                                        orig_enum_alias = orig_enum.get("alias", "")
                                        is_preprocessing_mapping = False
                                        
                                        if orig_enum_alias and orig_enum_alias in mapping_ctx["table"]["table_standards"]:
                                            if mapping_ctx["table"]["table_standards"][orig_enum_alias] == mapped_enum_name:
                                                is_preprocessing_mapping = True
                                        
                                        if not is_preprocessing_mapping and orig_enum_name:
                                            name_variants = [orig_enum_name, orig_enum_name.lower(), orig_enum_name.upper(), orig_enum_name.capitalize()]
                                            for variant in name_variants:
                                                if variant in mapping_ctx["table"]["table_standards"]:
                                                    if mapping_ctx["table"]["table_standards"][variant] == mapped_enum_name:
                                                        is_preprocessing_mapping = True
                                                        break
                                        
                                        if is_preprocessing_mapping:
                                            actual_table_mappings[orig_enum_name] = mapped_enum_name
                                            if orig_enum_alias:
                                                actual_table_mappings[orig_enum_alias] = mapped_enum_name
                                
                                orig_vos = orig_item.get("valueObjects", [])
                                mapped_vos = mapped_item.get("valueObjects", [])
                                for orig_vo, mapped_vo in zip(orig_vos, mapped_vos):
                                    orig_vo_name = orig_vo.get("name", "")
                                    mapped_vo_name = mapped_vo.get("name", "")
                                    if orig_vo_name != mapped_vo_name and mapping_ctx:
                                        # ì„ ì²˜ë¦¬ ë§¤í•‘ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                                        orig_vo_alias = orig_vo.get("alias", "")
                                        is_preprocessing_mapping = False
                                        
                                        if orig_vo_alias and orig_vo_alias in mapping_ctx["table"]["table_standards"]:
                                            if mapping_ctx["table"]["table_standards"][orig_vo_alias] == mapped_vo_name:
                                                is_preprocessing_mapping = True
                                        
                                        if not is_preprocessing_mapping and orig_vo_name:
                                            name_variants = [orig_vo_name, orig_vo_name.lower(), orig_vo_name.upper(), orig_vo_name.capitalize()]
                                            for variant in name_variants:
                                                if variant in mapping_ctx["table"]["table_standards"]:
                                                    if mapping_ctx["table"]["table_standards"][variant] == mapped_vo_name:
                                                        is_preprocessing_mapping = True
                                                        break
                                        
                                        if is_preprocessing_mapping:
                                            actual_table_mappings[orig_vo_name] = mapped_vo_name
                                            if orig_vo_alias:
                                                actual_table_mappings[orig_vo_alias] = mapped_vo_name
                                
                                # í•„ë“œëª… ë³€í™˜ ìˆ˜ì§‘ (ì„ ì²˜ë¦¬ ë§¤í•‘ë§Œ)
                                orig_attrs = orig_item.get("previewAttributes", [])
                                mapped_attrs = mapped_item.get("previewAttributes", [])
                                for orig_attr, mapped_attr in zip(orig_attrs, mapped_attrs):
                                    if isinstance(orig_attr, dict) and isinstance(mapped_attr, dict):
                                        orig_field = orig_attr.get("fieldName", "")
                                        mapped_field = mapped_attr.get("fieldName", "")
                                        if orig_field != mapped_field and mapping_ctx:
                                            # ì„ ì²˜ë¦¬ ë§¤í•‘ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                                            if orig_field in mapping_ctx["table"]["column_standards"]:
                                                if mapping_ctx["table"]["column_standards"][orig_field] == mapped_field:
                                                    actual_column_mappings[orig_field] = mapped_field
                                
                                # DDL í•„ë“œëª… ë³€í™˜ ìˆ˜ì§‘ (ì„ ì²˜ë¦¬ ë§¤í•‘ë§Œ)
                                orig_ddl_fields = orig_item.get("ddlFields", [])
                                mapped_ddl_fields = mapped_item.get("ddlFields", [])
                                for orig_ddl_field, mapped_ddl_field in zip(orig_ddl_fields, mapped_ddl_fields):
                                    if isinstance(orig_ddl_field, dict) and isinstance(mapped_ddl_field, dict):
                                        orig_ddl_field_name = orig_ddl_field.get("fieldName", "")
                                        mapped_ddl_field_name = mapped_ddl_field.get("fieldName", "")
                                        if orig_ddl_field_name != mapped_ddl_field_name and mapping_ctx:
                                            # ì„ ì²˜ë¦¬ ë§¤í•‘ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                                            if orig_ddl_field_name in mapping_ctx["table"]["column_standards"]:
                                                if mapping_ctx["table"]["column_standards"][orig_ddl_field_name] == mapped_ddl_field_name:
                                                    actual_column_mappings[orig_ddl_field_name] = mapped_ddl_field_name
                    
                    summary["preprocessing_mappings"] = {
                        "table_standards": actual_table_mappings,  # ì‹¤ì œ ë³€í™˜ëœ ê²ƒë§Œ
                        "column_standards": actual_column_mappings,  # ì‹¤ì œ ë³€í™˜ëœ ê²ƒë§Œ
                        "total_table_mappings": len(actual_table_mappings),
                        "total_column_mappings": len(actual_column_mappings)
                    }
                
                # ì„ ì²˜ë¦¬ ë§¤í•‘ê³¼ ìœ ì‚¬ë„ ê²€ìƒ‰(LLM) ê²°ê³¼ë¥¼ êµ¬ë¶„í•´ì„œ í‘œì‹œ
                # ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ê°€ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
                has_rag_results = False
                
                # query_search_resultsê°€ ìˆê³  ì‹¤ì œ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                if search_info.get("query_search_results"):
                    query_search_results = search_info["query_search_results"]
                    for qr in query_search_results:
                        if qr.get("results") and len(qr.get("results", [])) > 0:
                            has_rag_results = True
                            break
                        # í•˜ìœ„ í˜¸í™˜ì„±: ë‹¨ì¼ result í˜•ì‹
                        if qr.get("result"):
                            has_rag_results = True
                            break
                
                # relevant_standardsê°€ ìˆëŠ”ì§€ í™•ì¸
                if not has_rag_results and search_info.get("relevant_standards"):
                    has_rag_results = len(search_info["relevant_standards"]) > 0
                
                # search_queriesì—ì„œ ì‹¤ì œë¡œ ì±„íƒëœ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                if not has_rag_results and search_info.get("standard_queries"):
                    for query_info in search_info["standard_queries"]:
                        if query_info.get("total_found", 0) > 0:
                            has_rag_results = True
                            break
                
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê²½ê³  ë¡œê·¸
                if not has_rag_results:
                    LoggingUtil.warning("StandardTransformer", 
                                      f"âš ï¸  RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: BC={bounded_context.get('name', 'Unknown')} - LLMì´ í‘œì¤€ ë¬¸ì„œ ì—†ì´ ë³€í™˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.")
                
                rag_llm_mappings = {
                    "table_standards": {},
                    "column_standards": {},
                    "total_table_mappings": 0,
                    "total_column_mappings": 0,
                    "used_rag_search": has_rag_results  # ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ê°€ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€
                }
                
                # transformed_optionsì—ì„œ ë§¤í•‘ ì •ë³´ ì¶”ì¶œ
                for i, transformed_option in enumerate(transformed_options):
                    if i < len(draft_options):
                        original_option = draft_options[i]
                        original_structure = original_option.get("structure", [])
                        transformed_structure = transformed_option.get("structure", [])
                        
                        # mapping_infoì—ì„œ ì„ ì²˜ë¦¬ ë§¤í•‘ê³¼ LLM ë§¤í•‘ ì¶”ì¶œ
                        mapping_info = transformed_option.get("mapping_info", {}).get(f"option_{i}", {})
                        llm_mapping = mapping_info.get("llm_mapping", {})
                        
                        # LLM ë§¤í•‘ìœ¼ë¡œ ë³€í™˜ëœ í•­ëª© ìˆ˜ì§‘ (ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì•„ë‹Œ ê²ƒë§Œ)
                        for orig_item, trans_item in zip(original_structure, transformed_structure):
                            orig_agg = orig_item.get("aggregate", {})
                            trans_agg = trans_item.get("aggregate", {})
                            orig_name = orig_agg.get("name", "")
                            trans_name = trans_agg.get("name", "")
                            
                            # LLM ë§¤í•‘ì— ìˆê³  ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                            if orig_name in llm_mapping and llm_mapping[orig_name] == trans_name:
                                if orig_name != trans_name:
                                    rag_llm_mappings["table_standards"][orig_name] = trans_name
                                    orig_alias = orig_agg.get("alias", "")
                                    if orig_alias:
                                        rag_llm_mappings["table_standards"][orig_alias] = trans_name
                            
                            # Enum/VO LLM ë§¤í•‘
                            orig_enums = orig_item.get("enumerations", [])
                            trans_enums = trans_item.get("enumerations", [])
                            for orig_enum, trans_enum in zip(orig_enums, trans_enums):
                                orig_enum_name = orig_enum.get("name", "")
                                trans_enum_name = trans_enum.get("name", "")
                                if orig_enum_name != trans_enum_name:
                                    # ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€ (preprocessing_mappingsì— ì—†ìœ¼ë©´)
                                    if orig_enum_name not in actual_table_mappings:
                                        rag_llm_mappings["table_standards"][orig_enum_name] = trans_enum_name
                                        orig_enum_alias = orig_enum.get("alias", "")
                                        if orig_enum_alias:
                                            rag_llm_mappings["table_standards"][orig_enum_alias] = trans_enum_name
                            
                            orig_vos = orig_item.get("valueObjects", [])
                            trans_vos = trans_item.get("valueObjects", [])
                            for orig_vo, trans_vo in zip(orig_vos, trans_vos):
                                orig_vo_name = orig_vo.get("name", "")
                                trans_vo_name = trans_vo.get("name", "")
                                if orig_vo_name != trans_vo_name:
                                    # ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                                    if orig_vo_name not in actual_table_mappings:
                                        rag_llm_mappings["table_standards"][orig_vo_name] = trans_vo_name
                                        orig_vo_alias = orig_vo.get("alias", "")
                                        if orig_vo_alias:
                                            rag_llm_mappings["table_standards"][orig_vo_alias] = trans_vo_name
                            
                            # í•„ë“œëª… LLM ë§¤í•‘
                            orig_attrs = orig_item.get("previewAttributes", [])
                            trans_attrs = trans_item.get("previewAttributes", [])
                            for orig_attr, trans_attr in zip(orig_attrs, trans_attrs):
                                if isinstance(orig_attr, dict) and isinstance(trans_attr, dict):
                                    orig_field = orig_attr.get("fieldName", "")
                                    trans_field = trans_attr.get("fieldName", "")
                                    if orig_field != trans_field:
                                        # ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                                        if orig_field not in actual_column_mappings:
                                            rag_llm_mappings["column_standards"][orig_field] = trans_field
                            
                            # DDL í•„ë“œëª… LLM ë§¤í•‘
                            orig_ddl_fields = orig_item.get("ddlFields", [])
                            trans_ddl_fields = trans_item.get("ddlFields", [])
                            for orig_ddl_field, trans_ddl_field in zip(orig_ddl_fields, trans_ddl_fields):
                                if isinstance(orig_ddl_field, dict) and isinstance(trans_ddl_field, dict):
                                    orig_ddl_field_name = orig_ddl_field.get("fieldName", "")
                                    trans_ddl_field_name = trans_ddl_field.get("fieldName", "")
                                    if orig_ddl_field_name != trans_ddl_field_name:
                                        # ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                                        if orig_ddl_field_name not in actual_column_mappings:
                                            rag_llm_mappings["column_standards"][orig_ddl_field_name] = trans_ddl_field_name
                
                rag_llm_mappings["total_table_mappings"] = len(rag_llm_mappings["table_standards"])
                rag_llm_mappings["total_column_mappings"] = len(rag_llm_mappings["column_standards"])
                
                summary["rag_llm_mappings"] = rag_llm_mappings
                
                # ì „ì²˜ë¦¬ ë§¤í•‘ ì „í›„ ë¹„êµ (ê°„ë‹¨í•œ ìš”ì•½)
                if search_info.get("mapped_draft_options") and len(draft_options) > 0:
                    preprocessing_comparison = []
                    for i, (original, mapped) in enumerate(zip(draft_options, search_info["mapped_draft_options"])):
                        original_structure = original.get("structure", [])
                        mapped_structure = mapped.get("structure", [])
                        
                        # Aggregate ì´ë¦„ ë³€ê²½ ì¶”ì  (ì„ ì²˜ë¦¬ ë§¤í•‘ë§Œ)
                        aggregate_changes = []
                        for orig_item, mapped_item in zip(original_structure, mapped_structure):
                            orig_agg = orig_item.get("aggregate", {})
                            mapped_agg = mapped_item.get("aggregate", {})
                            orig_name = orig_agg.get("name", "")
                            mapped_name = mapped_agg.get("name", "")
                            if orig_name != mapped_name:
                                # ì„ ì²˜ë¦¬ ë§¤í•‘ì¸ì§€ í™•ì¸
                                if orig_name in actual_table_mappings and actual_table_mappings[orig_name] == mapped_name:
                                    aggregate_changes.append({
                                        "alias": orig_agg.get("alias", ""),
                                        "before": orig_name,
                                        "after": mapped_name,
                                        "method": "preprocessing"
                                    })
                        
                        # Enum/VO ì´ë¦„ ë³€ê²½ ì¶”ì  (ì„ ì²˜ë¦¬ ë§¤í•‘ë§Œ)
                        enum_vo_changes = []
                        for orig_item, mapped_item in zip(original_structure, mapped_structure):
                            orig_enums = orig_item.get("enumerations", [])
                            mapped_enums = mapped_item.get("enumerations", [])
                            for orig_enum, mapped_enum in zip(orig_enums, mapped_enums):
                                orig_enum_name = orig_enum.get("name", "")
                                mapped_enum_name = mapped_enum.get("name", "")
                                if orig_enum_name != mapped_enum_name:
                                    # ì„ ì²˜ë¦¬ ë§¤í•‘ì¸ì§€ í™•ì¸
                                    if orig_enum_name in actual_table_mappings and actual_table_mappings[orig_enum_name] == mapped_enum_name:
                                        enum_vo_changes.append({
                                            "type": "enum",
                                            "alias": orig_enum.get("alias", ""),
                                            "before": orig_enum_name,
                                            "after": mapped_enum_name,
                                            "method": "preprocessing"
                                        })
                            
                            orig_vos = orig_item.get("valueObjects", [])
                            mapped_vos = mapped_item.get("valueObjects", [])
                            for orig_vo, mapped_vo in zip(orig_vos, mapped_vos):
                                orig_vo_name = orig_vo.get("name", "")
                                mapped_vo_name = mapped_vo.get("name", "")
                                if orig_vo_name != mapped_vo_name:
                                    # ì„ ì²˜ë¦¬ ë§¤í•‘ì¸ì§€ í™•ì¸
                                    if orig_vo_name in actual_table_mappings and actual_table_mappings[orig_vo_name] == mapped_vo_name:
                                        enum_vo_changes.append({
                                            "type": "value_object",
                                            "alias": orig_vo.get("alias", ""),
                                            "before": orig_vo_name,
                                            "after": mapped_vo_name,
                                            "method": "preprocessing"
                                        })
                        
                        # í•„ë“œëª… ë³€ê²½ ì¶”ì  (ì„ ì²˜ë¦¬ ë§¤í•‘ë§Œ)
                        field_changes = []
                        for orig_item, mapped_item in zip(original_structure, mapped_structure):
                            orig_attrs = orig_item.get("previewAttributes", [])
                            mapped_attrs = mapped_item.get("previewAttributes", [])
                            for orig_attr, mapped_attr in zip(orig_attrs, mapped_attrs):
                                if isinstance(orig_attr, dict) and isinstance(mapped_attr, dict):
                                    orig_field = orig_attr.get("fieldName", "")
                                    mapped_field = mapped_attr.get("fieldName", "")
                                    if orig_field != mapped_field:
                                        # ì„ ì²˜ë¦¬ ë§¤í•‘ì¸ì§€ í™•ì¸
                                        if orig_field in actual_column_mappings and actual_column_mappings[orig_field] == mapped_field:
                                            field_changes.append({
                                                "aggregate_alias": orig_item.get("aggregate", {}).get("alias", ""),
                                                "before": orig_field,
                                                "after": mapped_field,
                                                "method": "preprocessing"
                                            })
                        
                        if aggregate_changes or enum_vo_changes or field_changes:
                            preprocessing_comparison.append({
                                "option_index": i,
                                "aggregate_changes": aggregate_changes,
                                "enum_vo_changes": enum_vo_changes,
                                "field_changes": field_changes
                            })
                    
                    if preprocessing_comparison:
                        summary["preprocessing_comparison"] = preprocessing_comparison
                
                # ìœ ì‚¬ë„ ê²€ìƒ‰(LLM) ê²°ê³¼ ë¹„êµ (preprocessing_comparisonê³¼ ë™ì¼í•œ í˜•ì‹)
                if len(transformed_options) > 0:
                    rag_llm_comparison = []
                    # ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ê°€ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    method_name = "rag_llm" if has_rag_results else "llm_only"
                    
                    for i, (original, transformed) in enumerate(zip(draft_options, transformed_options)):
                        original_structure = original.get("structure", [])
                        transformed_structure = transformed.get("structure", [])
                        
                        # Aggregate ì´ë¦„ ë³€ê²½ ì¶”ì  (ìœ ì‚¬ë„ ê²€ìƒ‰ + LLMë§Œ)
                        aggregate_changes = []
                        for orig_item, trans_item in zip(original_structure, transformed_structure):
                            orig_agg = orig_item.get("aggregate", {})
                            trans_agg = trans_item.get("aggregate", {})
                            orig_name = orig_agg.get("name", "")
                            trans_name = trans_agg.get("name", "")
                            
                            # ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì•„ë‹ˆê³  ë³€í™˜ëœ ê²½ìš°ë§Œ ì¶”ê°€
                            if orig_name != trans_name:
                                is_preprocessing = orig_name in actual_table_mappings and actual_table_mappings[orig_name] == trans_name
                                if not is_preprocessing:
                                    aggregate_changes.append({
                                        "alias": orig_agg.get("alias", ""),
                                        "before": orig_name,
                                        "after": trans_name,
                                        "method": method_name  # "rag_llm" ë˜ëŠ” "llm_only"
                                    })
                        
                        # Enum/VO ì´ë¦„ ë³€ê²½ ì¶”ì  (ìœ ì‚¬ë„ ê²€ìƒ‰ + LLMë§Œ)
                        enum_vo_changes = []
                        for orig_item, trans_item in zip(original_structure, transformed_structure):
                            orig_enums = orig_item.get("enumerations", [])
                            trans_enums = trans_item.get("enumerations", [])
                            for orig_enum, trans_enum in zip(orig_enums, trans_enums):
                                orig_enum_name = orig_enum.get("name", "")
                                trans_enum_name = trans_enum.get("name", "")
                                if orig_enum_name != trans_enum_name:
                                    # ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                                    is_preprocessing = orig_enum_name in actual_table_mappings and actual_table_mappings[orig_enum_name] == trans_enum_name
                                    if not is_preprocessing:
                                        enum_vo_changes.append({
                                            "type": "enum",
                                            "alias": orig_enum.get("alias", ""),
                                            "before": orig_enum_name,
                                            "after": trans_enum_name,
                                            "method": method_name  # "rag_llm" ë˜ëŠ” "llm_only"
                                        })
                            
                            orig_vos = orig_item.get("valueObjects", [])
                            trans_vos = trans_item.get("valueObjects", [])
                            for orig_vo, trans_vo in zip(orig_vos, trans_vos):
                                orig_vo_name = orig_vo.get("name", "")
                                trans_vo_name = trans_vo.get("name", "")
                                if orig_vo_name != trans_vo_name:
                                    # ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                                    is_preprocessing = orig_vo_name in actual_table_mappings and actual_table_mappings[orig_vo_name] == trans_vo_name
                                    if not is_preprocessing:
                                        enum_vo_changes.append({
                                            "type": "value_object",
                                            "alias": orig_vo.get("alias", ""),
                                            "before": orig_vo_name,
                                            "after": trans_vo_name,
                                            "method": method_name  # "rag_llm" ë˜ëŠ” "llm_only"
                                        })
                        
                        # í•„ë“œëª… ë³€ê²½ ì¶”ì  (ìœ ì‚¬ë„ ê²€ìƒ‰ + LLMë§Œ)
                        field_changes = []
                        for orig_item, trans_item in zip(original_structure, transformed_structure):
                            orig_attrs = orig_item.get("previewAttributes", [])
                            trans_attrs = trans_item.get("previewAttributes", [])
                            for orig_attr, trans_attr in zip(orig_attrs, trans_attrs):
                                if isinstance(orig_attr, dict) and isinstance(trans_attr, dict):
                                    orig_field = orig_attr.get("fieldName", "")
                                    trans_field = trans_attr.get("fieldName", "")
                                    if orig_field != trans_field:
                                        # ì„ ì²˜ë¦¬ ë§¤í•‘ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                                        is_preprocessing = orig_field in actual_column_mappings and actual_column_mappings[orig_field] == trans_field
                                        if not is_preprocessing:
                                            field_changes.append({
                                                "aggregate_alias": orig_item.get("aggregate", {}).get("alias", ""),
                                                "before": orig_field,
                                                "after": trans_field,
                                                "method": method_name  # "rag_llm" ë˜ëŠ” "llm_only"
                                            })
                        
                        if aggregate_changes or enum_vo_changes or field_changes:
                            rag_llm_comparison.append({
                                "option_index": i,
                                "aggregate_changes": aggregate_changes,
                                "enum_vo_changes": enum_vo_changes,
                                "field_changes": field_changes
                            })
                    
                    if rag_llm_comparison:
                        summary["rag_llm_comparison"] = rag_llm_comparison
                        
                        # ìœ ì‚¬ë„ ì±„íƒìœ¼ë¡œ ë°”ë€ ê²°ê³¼ (ê°„ë‹¨í•œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹)
                        summary["rag_transformations"] = []
                        for comp in rag_llm_comparison:
                            option_index = comp.get("option_index", 0)
                            for change in comp.get("aggregate_changes", []):
                                summary["rag_transformations"].append({
                                    "option_index": option_index,
                                    "type": "aggregate",
                                    "alias": change.get("alias", ""),
                                    "before": change.get("before", ""),
                                    "after": change.get("after", ""),
                                    "method": change.get("method", "rag_llm")
                                })
                            for change in comp.get("enum_vo_changes", []):
                                summary["rag_transformations"].append({
                                    "option_index": option_index,
                                    "type": change.get("type", "enum"),
                                    "alias": change.get("alias", ""),
                                    "before": change.get("before", ""),
                                    "after": change.get("after", ""),
                                    "method": change.get("method", "rag_llm")
                                })
                            for change in comp.get("field_changes", []):
                                summary["rag_transformations"].append({
                                    "option_index": option_index,
                                    "type": "field",
                                    "alias": change.get("aggregate_alias", ""),
                                    "before": change.get("before", ""),
                                    "after": change.get("after", ""),
                                    "method": change.get("method", "rag_llm")
                                })
            
            summary_file = result_dir / f'{bc_name_safe}_summary.json'
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            LoggingUtil.info("StandardTransformer", 
                          f"âœ… ë³€í™˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {result_dir}")
            
        except Exception as e:
            LoggingUtil.error("StandardTransformer", 
                            f"âŒ ë³€í™˜ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def _get_response_schema(self) -> Dict:
        """ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜ (aggregate_draft_generatorì™€ ë™ì¼í•œ íŒ¨í„´)"""
        return {
            "title": "StandardTransformationResponse",
            "description": "Response schema for standard transformation",
            "type": "object",
            "properties": {
                "inference": {
                    "type": "string",
                    "description": "Brief explanation of transformations applied"
                },
                "result": {
                    "type": "object",
                    "properties": {
                        "transformedOptions": {
                            "type": "array",
                            "description": "Transformed aggregate draft options",
                            "items": {
                                "type": "object",
                                "properties": {
                                    # í•­ìƒ ìˆëŠ” í•„ë“œ
                                    "structure": {
                                        "type": "array",
                                        "items": {
                                            "type": "object",
                                            "properties": {
                                                "aggregate": {
                                                    "type": "object",
                                                    "properties": {
                                                        "name": {"type": "string"},
                                                        "alias": {"type": "string"}
                                                    },
                                                    "required": ["name", "alias"],
                                                    "additionalProperties": True  # refs ë“± ì¶”ê°€ í•„ë“œ í—ˆìš©
                                                },
                                                "enumerations": {
                                                    "type": "array",
                                                    "items": {
                                                        "type": "object",
                                                        "properties": {
                                                            "name": {"type": "string"},
                                                            "alias": {"type": "string"}
                                                        },
                                                        "required": ["name", "alias"],
                                                        "additionalProperties": True  # refs ë“± ì¶”ê°€ í•„ë“œ í—ˆìš©
                                                    }
                                                },
                                                "valueObjects": {
                                                    "type": "array",
                                                    "items": {
                                                        "type": "object",
                                                        "properties": {
                                                            "name": {"type": "string"},
                                                            "alias": {"type": "string"},
                                                            "referencedAggregateName": {"type": "string"}
                                                        },
                                                        "required": ["name", "alias", "referencedAggregateName"],
                                                        "additionalProperties": True  # refs ë“± ì¶”ê°€ í•„ë“œ í—ˆìš©
                                                    }
                                                },
                                                "previewAttributes": {
                                                    "type": "array",
                                                    "items": {
                                                        "type": "object",
                                                        "properties": {
                                                            "fieldName": {"type": "string"},
                                                            "fieldAlias": {"type": "string"}
                                                        },
                                                        "required": ["fieldName", "fieldAlias"],
                                                        "additionalProperties": True
                                                    }
                                                },
                                                "ddlFields": {
                                                    "type": "array",
                                                    "items": {
                                                        "type": "object",
                                                        "properties": {
                                                            "fieldName": {"type": "string"},
                                                            "fieldAlias": {"type": "string"}
                                                        },
                                                        "required": ["fieldName", "fieldAlias"],
                                                        "additionalProperties": True
                                                    }
                                                }
                                            },
                                            "required": ["aggregate", "enumerations", "valueObjects", "previewAttributes", "ddlFields"],
                                            "additionalProperties": True  # ê¸°íƒ€ ì¶”ê°€ í•„ë“œ í—ˆìš©
                                        },
                                        "description": "Aggregate structure with transformed names (preserve all aggregates from original)"
                                    },
                                    "pros": {
                                        "type": "object",
                                        "properties": {
                                            "cohesion": {"type": "string"},
                                            "coupling": {"type": "string"},
                                            "consistency": {"type": "string"},
                                            "encapsulation": {"type": "string"},
                                            "complexity": {"type": "string"},
                                            "independence": {"type": "string"},
                                            "performance": {"type": "string"}
                                        },
                                        "required": ["cohesion", "coupling", "consistency", "encapsulation", "complexity", "independence", "performance"],
                                        "additionalProperties": False,
                                        "description": "Pros analysis (preserve from original)"
                                    },
                                    "cons": {
                                        "type": "object",
                                        "properties": {
                                            "cohesion": {"type": "string"},
                                            "coupling": {"type": "string"},
                                            "consistency": {"type": "string"},
                                            "encapsulation": {"type": "string"},
                                            "complexity": {"type": "string"},
                                            "independence": {"type": "string"},
                                            "performance": {"type": "string"}
                                        },
                                        "required": ["cohesion", "coupling", "consistency", "encapsulation", "complexity", "independence", "performance"],
                                        "additionalProperties": False,
                                        "description": "Cons analysis (preserve from original)"
                                    }
                                },
                                # requiredì—ëŠ” í•­ìƒ ìˆëŠ” í•„ë“œë§Œ í¬í•¨ (aggregate_draft_generatorì™€ ë™ì¼)
                                "required": ["structure", "pros", "cons"],
                                "additionalProperties": True  # boundedContext, description ë“± ì›ë³¸ì˜ ë‹¤ë¥¸ í•„ë“œë“¤ë„ í—ˆìš©
                            }
                        }
                    },
                    "required": ["transformedOptions"],
                    "additionalProperties": False
                }
            },
            "required": ["inference", "result"],
            "additionalProperties": False
        }
    
    def _estimate_prompt_tokens(self, prompt: str) -> int:
        """
        í”„ë¡¬í”„íŠ¸ì˜ ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •
        (ëŒ€ëµì ìœ¼ë¡œ ë¬¸ì ìˆ˜ / 4ë¥¼ ì‚¬ìš©, ë” ì •í™•í•œ ì¶”ì •ì´ í•„ìš”í•˜ë©´ tiktoken ì‚¬ìš© ê°€ëŠ¥)
        
        Args:
            prompt: í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
            
        Returns:
            ì˜ˆìƒ í† í° ìˆ˜
        """
        # ëŒ€ëµì ì¸ ì¶”ì •: ì˜ì–´ëŠ” í‰ê·  4ìë‹¹ 1í† í°, í•œê¸€ì€ í‰ê·  1.5ìë‹¹ 1í† í°
        # í˜¼í•© í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•˜ì—¬ í‰ê· ì ìœ¼ë¡œ ë¬¸ì ìˆ˜ / 3.5 ì‚¬ìš©
        return len(prompt) // 3
    
    def _chunk_preview_attributes(self, preview_attrs: List[Dict], chunk_size: int = 20) -> List[List[Dict]]:
        """
        previewAttributesë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
        
        Args:
            preview_attrs: previewAttributes ë¦¬ìŠ¤íŠ¸
            chunk_size: ì²­í¬ë‹¹ í•„ë“œ ìˆ˜
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        for i in range(0, len(preview_attrs), chunk_size):
            chunks.append(preview_attrs[i:i + chunk_size])
        return chunks
    
    def _chunk_ddl_fields(self, ddl_fields: List[Dict], chunk_size: int = 20) -> List[List[Dict]]:
        """
        ddlFieldsë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
        
        Args:
            ddl_fields: ddlFields ë¦¬ìŠ¤íŠ¸
            chunk_size: ì²­í¬ë‹¹ í•„ë“œ ìˆ˜
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        for i in range(0, len(ddl_fields), chunk_size):
            chunks.append(ddl_fields[i:i + chunk_size])
        return chunks
    
    def _chunk_enumerations(self, enumerations: List[Dict], chunk_size: int = 20) -> List[List[Dict]]:
        """
        enumerationsë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
        
        Args:
            enumerations: enumerations ë¦¬ìŠ¤íŠ¸
            chunk_size: ì²­í¬ë‹¹ í•­ëª© ìˆ˜
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        for i in range(0, len(enumerations), chunk_size):
            chunks.append(enumerations[i:i + chunk_size])
        return chunks
    
    def _chunk_value_objects(self, value_objects: List[Dict], chunk_size: int = 20) -> List[List[Dict]]:
        """
        valueObjectsë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
        
        Args:
            value_objects: valueObjects ë¦¬ìŠ¤íŠ¸
            chunk_size: ì²­í¬ë‹¹ í•­ëª© ìˆ˜
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        for i in range(0, len(value_objects), chunk_size):
            chunks.append(value_objects[i:i + chunk_size])
        return chunks
    
    def _transform_structure_with_chunking(
        self,
        structure_item: Dict,
        bounded_context: Dict,
        relevant_standards: List[Dict],
        query_search_results: List[Dict],
        original_structure_item: Optional[Dict] = None,
        mapping_context: Optional[StandardMappingContext] = None,
        update_progress_callback: Optional[callable] = None,
        estimated_prompt_tokens: int = 0,
        bc_name: Optional[str] = None,
        agg_name: Optional[str] = None,
        original_option_bounded_context: Optional[Dict] = None  # ì›ë³¸ optionì˜ boundedContext ì •ë³´ (ì²­í‚¹ ì²˜ë¦¬ìš©)
    ) -> Dict:
        """
        ì²­í‚¹ ì²˜ë¦¬ë¥¼ í†µí•œ structure ë³€í™˜ (ëŒ€ëŸ‰ í•„ë“œ ì²˜ë¦¬ìš©)
        
        ì „ëµ:
        1. Aggregate, Enum, VOëŠ” ì²« ë²ˆì§¸ í˜¸ì¶œì—ì„œ ì²˜ë¦¬
        2. previewAttributesì™€ ddlFieldsëŠ” ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ê°ê° ì²˜ë¦¬
        3. ëª¨ë“  ì‘ë‹µì„ ë³‘í•©
        
        Args:
            structure_item: ë³€í™˜í•  ë‹¨ì¼ structure í•­ëª©
            bounded_context: Bounded Context ì •ë³´
            relevant_standards: ê²€ìƒ‰ëœ í‘œì¤€ ì²­í¬ë“¤
            query_search_results: ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ (top-k=3)
            original_structure_item: ì›ë³¸ structure í•­ëª© (ì„ ì²˜ë¦¬ ì „)
            mapping_context: ì„ ì²˜ë¦¬ ë§¤í•‘ ì»¨í…ìŠ¤íŠ¸
            update_progress_callback: ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì½œë°±
            estimated_prompt_tokens: ì˜ˆìƒ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜
            
        Returns:
            ë³€í™˜ëœ structure í•­ëª©
        """
        import copy
        # bc_nameê³¼ agg_nameì´ ì—†ìœ¼ë©´ structure_itemì—ì„œ ì¶”ì¶œ (í´ë¡œì €ë¥¼ ìœ„í•´ ë¡œì»¬ ë³€ìˆ˜ë¡œ ì €ì¥)
        if not bc_name:
            bc_name = bounded_context.get("name", bounded_context.get("boundedContext", "Unknown"))
        if not agg_name:
            agg_name = structure_item.get("aggregate", {}).get("name", "Unknown")
        
        bc_name_val = bc_name
        agg_name_val = agg_name
        
        enumerations = structure_item.get("enumerations", [])
        value_objects = structure_item.get("valueObjects", [])
        preview_attrs = structure_item.get("previewAttributes", [])
        ddl_fields = structure_item.get("ddlFields", [])
        
        # ì²­í¬ í¬ê¸° ê²°ì •: í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ì— ë”°ë¼ ì¡°ì •
        # ì•ˆì „í•œ ë²”ìœ„: í”„ë¡¬í”„íŠ¸ 15000 í† í° + ì‘ë‹µ 10000 í† í° = ì´ 25000 í† í° (ì—¬ìœ  ìˆê²Œ)
        # ê° ì•„ì´í…œ(enum/vo/field)ë‹¹ ì•½ 100 í† í°, ê²€ìƒ‰ ê²°ê³¼ 3ê°œë‹¹ ì•½ 500 í† í°, í”„ë¡¬í”„íŠ¸ ê¸°ë³¸ ì•½ 2000 í† í°
        # ì˜ˆ: 10ê°œ ì•„ì´í…œ = 1000 + 500 + 2000 = 3500 í† í° (í”„ë¡¬í”„íŠ¸), ì‘ë‹µ 1000 í† í° = ì´ 4500 í† í°
        base_chunk_size = 10  # ë” ì•ˆì „í•œ ê¸°ë³¸ê°’
        if estimated_prompt_tokens > 20000:
            chunk_size = 5  # ë§¤ìš° ì‘ì€ ì²­í¬
        elif estimated_prompt_tokens > 15000:
            chunk_size = 8
        elif estimated_prompt_tokens > 10000:
            chunk_size = 10
        else:
            chunk_size = 12  # í† í°ì´ ì ìœ¼ë©´ ì¡°ê¸ˆ ë” í° ì²­í¬ ê°€ëŠ¥
        
        LoggingUtil.info("StandardTransformer", 
                       f"      ğŸ“¦ [ì²­í‚¹] ì²­í¬ í¬ê¸°: {chunk_size}ê°œ (ì˜ˆìƒ í”„ë¡¬í”„íŠ¸ í† í°: {estimated_prompt_tokens})")
        
        # 1ë‹¨ê³„: Aggregateë§Œ ë³€í™˜ (enum, vo, í•„ë“œ ì—†ì´)
        LoggingUtil.info("StandardTransformer", 
                       f"      ğŸ“¦ [ì²­í‚¹] 1ë‹¨ê³„: Aggregateë§Œ ë³€í™˜ ì‹œì‘")
        
        structure_item_agg_only = {
            "aggregate": structure_item.get("aggregate", {}),
            "enumerations": [],
            "valueObjects": [],
            "previewAttributes": [],
            "ddlFields": []
        }
        
        # Aggregate ê´€ë ¨ ì¿¼ë¦¬ë§Œ í•„í„°ë§
        agg_name_val = agg_name or structure_item.get("aggregate", {}).get("name", "")
        agg_alias = structure_item.get("aggregate", {}).get("alias", "")
        agg_related_queries = set()
        
        if agg_name_val:
            for qr in query_search_results:
                query = qr.get("query", "")
                query_lower = query.lower()
                # Aggregate ì´ë¦„ì´ë‚˜ aliasê°€ ì¿¼ë¦¬ì— í¬í•¨ë˜ì–´ ìˆê±°ë‚˜, ì¿¼ë¦¬ê°€ Aggregate ì´ë¦„ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                if (agg_name_val.lower() in query_lower or 
                    query_lower in agg_name_val.lower() or
                    (agg_alias and (agg_alias in query or query in agg_alias))):
                    agg_related_queries.add(query)
        
        # Aggregate ê´€ë ¨ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì¿¼ë¦¬ì˜ ì¼ë¶€ë§Œ ì‚¬ìš© (ìµœëŒ€ 3ê°œ ì¿¼ë¦¬, ê° ì¿¼ë¦¬ë³„ top-3)
        if not agg_related_queries and query_search_results:
            # Aggregate ê´€ë ¨ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ì¿¼ë¦¬ ì¤‘ ìµœëŒ€ 3ê°œë§Œ ì‚¬ìš©
            unique_queries = list(set([qr.get("query", "") for qr in query_search_results]))[:3]
            agg_query_results = []
            for query in unique_queries:
                query_results = [qr for qr in query_search_results if qr.get("query", "") == query]
                if query_results:
                    qr_item = query_results[0].copy()  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ê²°ê³¼ ì‚¬ìš©
                    # ğŸ”’ CRITICAL: results ë¦¬ìŠ¤íŠ¸ëŠ” top-3 ìœ ì§€ (k_per_query=3ì´ë¯€ë¡œ ìµœëŒ€ 3ê°œ)
                    if "results" in qr_item and isinstance(qr_item["results"], list) and len(qr_item["results"]) > 3:
                        qr_item["results"] = qr_item["results"][:3]  # top-3ë§Œ ìœ ì§€
                    agg_query_results.append(qr_item)
            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“‹ Aggregate ê´€ë ¨ ì¿¼ë¦¬ ì—†ìŒ, ì¼ë°˜ ì¿¼ë¦¬ {len(agg_query_results)}ê°œ ì‚¬ìš© (ìµœëŒ€ 3ê°œ ì¿¼ë¦¬, ê° top-3)")
        else:
            # ê´€ë ¨ ì¿¼ë¦¬ ì¤‘ ìµœëŒ€ 3ê°œë§Œ ì„ íƒ
            agg_related_queries_list = list(agg_related_queries)[:3]
            # ê° ì¿¼ë¦¬ë³„ë¡œ top-3 ê²°ê³¼ ì‚¬ìš© (k_per_query=3ì´ë¯€ë¡œ ìµœëŒ€ 3ê°œ)
            agg_query_results = []
            for query in agg_related_queries_list:
                query_results = [qr for qr in query_search_results if qr.get("query", "") == query]
                if query_results:
                    qr_item = query_results[0].copy()  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ê²°ê³¼ ì‚¬ìš©
                    # ğŸ”’ CRITICAL: results ë¦¬ìŠ¤íŠ¸ëŠ” top-3 ìœ ì§€ (k_per_query=3ì´ë¯€ë¡œ ìµœëŒ€ 3ê°œ)
                    if "results" in qr_item and isinstance(qr_item["results"], list) and len(qr_item["results"]) > 3:
                        qr_item["results"] = qr_item["results"][:3]  # top-3ë§Œ ìœ ì§€
                    agg_query_results.append(qr_item)
            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“‹ Aggregate ê´€ë ¨ ì¿¼ë¦¬ {len(agg_query_results)}ê°œ í•„í„°ë§ë¨ (ìµœëŒ€ 3ê°œ ì¿¼ë¦¬, ê° top-3, ì „ì²´ {len(query_search_results)}ê°œ ì¤‘)")
        
        # bc_nameê³¼ agg_nameì„ í¬í•¨í•œ ì½œë°± ë˜í¼ ìƒì„±
        def _update_progress_with_chunking_context(progress: int, stage: str, 
                                                   property_type: Optional[str] = None,
                                                   chunk_info: Optional[str] = None,
                                                   status: str = "processing",
                                                   error_message: Optional[str] = None,
                                                   bc_name: Optional[str] = None,
                                                   agg_name: Optional[str] = None):
            bc_name_val = bc_name or bounded_context.get("name", bounded_context.get("boundedContext", "Unknown"))
            agg_name_val = agg_name or structure_item.get("aggregate", {}).get("name", "Unknown")
            
            if update_progress_callback:
                try:
                    update_progress_callback(progress, stage,
                                            bc_name=bc_name or bc_name_val,
                                            agg_name=agg_name or agg_name_val,
                                            property_type=property_type,
                                            chunk_info=chunk_info,
                                            status=status,
                                            error_message=error_message)
                except Exception as e:
                    LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        
        base_result = self._transform_single_structure_with_llm(
            structure_item=structure_item_agg_only,
            bounded_context=bounded_context,
            relevant_standards=relevant_standards,
            query_search_results=agg_query_results,  # Aggregate ê´€ë ¨ ê²€ìƒ‰ ê²°ê³¼ë§Œ
            original_structure_item=original_structure_item,
            mapping_context=mapping_context,
            update_progress_callback=_update_progress_with_chunking_context,
            skip_chunking=True,  # ì²­í‚¹ ë‚´ë¶€ í˜¸ì¶œì´ë¯€ë¡œ ë¬´í•œ ì¬ê·€ ë°©ì§€
            bc_name=bc_name,
            agg_name=agg_name,
            original_option_bounded_context=original_option_bounded_context
        )
        
        # 2ë‹¨ê³„: enumerations ì²­í¬ë³„ ë³€í™˜
        enum_chunks = self._chunk_enumerations(enumerations, chunk_size)
        transformed_enums = []
        
        if enum_chunks:
            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“¦ [ì²­í‚¹] 2ë‹¨ê³„: enumerations {len(enumerations)}ê°œë¥¼ {len(enum_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
            
            for chunk_idx, chunk in enumerate(enum_chunks):
                if update_progress_callback:
                    try:
                        update_progress_callback(0, f"LLM ë³€í™˜ ì¤‘: {agg_name_val} (Enum ì²­í¬ {chunk_idx + 1}/{len(enum_chunks)})",
                                                bc_name=bc_name_val,
                                                agg_name=agg_name_val,
                                                property_type="enum",
                                                chunk_info=f"ì²­í¬ {chunk_idx + 1}/{len(enum_chunks)}",
                                                status="processing")
                    except Exception as e:
                        LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                LoggingUtil.info("StandardTransformer", 
                               f"      ğŸ“¦ [ì²­í‚¹] enumerations ì²­í¬ {chunk_idx + 1}/{len(enum_chunks)} ì²˜ë¦¬ ì¤‘ ({len(chunk)}ê°œ)")
                
                # Enumë§Œ í¬í•¨í•œ ìµœì†Œ structure ìƒì„±
                chunk_structure = {
                    "aggregate": {
                        "name": base_result.get("aggregate", {}).get("name", ""),
                        "alias": base_result.get("aggregate", {}).get("alias", "")
                    },
                    "enumerations": chunk,
                    "valueObjects": [],
                    "previewAttributes": [],
                    "ddlFields": []
                }
                
                # ì²­í¬ì— ê´€ë ¨ëœ ê²€ìƒ‰ ê²°ê³¼ë§Œ í•„í„°ë§ (ê° ì¿¼ë¦¬ë³„ top-1ë§Œ ì‚¬ìš©, ìµœëŒ€ 3ê°œ ì¿¼ë¦¬)
                chunk_related_queries = set()
                for enum in chunk:
                    if isinstance(enum, dict):
                        enum_name = enum.get("name", "")
                        if enum_name:
                            for qr in query_search_results:
                                query = qr.get("query", "")
                                if enum_name.lower() in query.lower() or query.lower() in enum_name.lower():
                                    chunk_related_queries.add(query)
                
                # ê´€ë ¨ ì¿¼ë¦¬ ì¤‘ ìµœëŒ€ 3ê°œë§Œ ì„ íƒ
                chunk_related_queries_list = list(chunk_related_queries)[:3]
                # ê° ì¿¼ë¦¬ë³„ë¡œ top-3 ê²°ê³¼ ì‚¬ìš© (k_per_query=3ì´ë¯€ë¡œ ìµœëŒ€ 3ê°œ)
                chunk_query_results = []
                for query in chunk_related_queries_list:
                    query_results = [qr for qr in query_search_results if qr.get("query", "") == query]
                    if query_results:
                        qr_item = query_results[0].copy()  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ê²°ê³¼ ì‚¬ìš©
                        # ğŸ”’ CRITICAL: results ë¦¬ìŠ¤íŠ¸ëŠ” top-3 ìœ ì§€ (k_per_query=3ì´ë¯€ë¡œ ìµœëŒ€ 3ê°œ)
                        if "results" in qr_item and isinstance(qr_item["results"], list) and len(qr_item["results"]) > 3:
                            qr_item["results"] = qr_item["results"][:3]  # top-3ë§Œ ìœ ì§€
                        chunk_query_results.append(qr_item)
                
                # Enum ì „ìš© ë³€í™˜ (bc_name, agg_name í¬í•¨í•œ ì½œë°± ë˜í¼)
                def _enum_update_callback(progress: int, stage: str, 
                                          property_type: Optional[str] = None,
                                          chunk_info: Optional[str] = None,
                                          status: str = "processing",
                                          error_message: Optional[str] = None):
                    if update_progress_callback:
                        try:
                            update_progress_callback(progress, stage,
                                                    property_type=property_type or "enum",
                                                    chunk_info=chunk_info or f"ì²­í¬ {chunk_idx + 1}/{len(enum_chunks)}",
                                                    status=status,
                                                    error_message=error_message)
                        except Exception as e:
                            LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                chunk_result = self._transform_enums_vos_only_with_llm(
                    structure_item=chunk_structure,
                    bounded_context=bounded_context,
                    relevant_standards=relevant_standards,
                    query_search_results=chunk_query_results,
                    item_type="enumerations",
                    update_progress_callback=_enum_update_callback
                )
                
                # ë³€í™˜ëœ enumerations ì¶”ì¶œ ë° ë³‘í•©
                chunk_transformed_enums = chunk_result.get("enumerations", [])
                transformed_enums.extend(chunk_transformed_enums)
        
        # 3ë‹¨ê³„: valueObjects ì²­í¬ë³„ ë³€í™˜
        vo_chunks = self._chunk_value_objects(value_objects, chunk_size)
        transformed_vos = []
        
        if vo_chunks:
            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“¦ [ì²­í‚¹] 3ë‹¨ê³„: valueObjects {len(value_objects)}ê°œë¥¼ {len(vo_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
            
            for chunk_idx, chunk in enumerate(vo_chunks):
                if update_progress_callback:
                    try:
                        update_progress_callback(0, f"LLM ë³€í™˜ ì¤‘: {agg_name} (VO ì²­í¬ {chunk_idx + 1}/{len(vo_chunks)})",
                                                property_type="vo",
                                                chunk_info=f"ì²­í¬ {chunk_idx + 1}/{len(vo_chunks)}",
                                                status="processing")
                    except Exception as e:
                        LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                LoggingUtil.info("StandardTransformer", 
                               f"      ğŸ“¦ [ì²­í‚¹] valueObjects ì²­í¬ {chunk_idx + 1}/{len(vo_chunks)} ì²˜ë¦¬ ì¤‘ ({len(chunk)}ê°œ)")
                
                # VOë§Œ í¬í•¨í•œ ìµœì†Œ structure ìƒì„±
                chunk_structure = {
                    "aggregate": {
                        "name": base_result.get("aggregate", {}).get("name", ""),
                        "alias": base_result.get("aggregate", {}).get("alias", "")
                    },
                    "enumerations": [],
                    "valueObjects": chunk,
                    "previewAttributes": [],
                    "ddlFields": []
                }
                
                # ì²­í¬ì— ê´€ë ¨ëœ ê²€ìƒ‰ ê²°ê³¼ë§Œ í•„í„°ë§ (ê° ì¿¼ë¦¬ë³„ top-1ë§Œ ì‚¬ìš©, ìµœëŒ€ 3ê°œ ì¿¼ë¦¬)
                chunk_related_queries = set()
                for vo in chunk:
                    if isinstance(vo, dict):
                        vo_name = vo.get("name", "")
                        if vo_name:
                            for qr in query_search_results:
                                query = qr.get("query", "")
                                if vo_name.lower() in query.lower() or query.lower() in vo_name.lower():
                                    chunk_related_queries.add(query)
                
                # ê´€ë ¨ ì¿¼ë¦¬ ì¤‘ ìµœëŒ€ 3ê°œë§Œ ì„ íƒ
                chunk_related_queries_list = list(chunk_related_queries)[:3]
                # ê° ì¿¼ë¦¬ë³„ë¡œ top-3 ê²°ê³¼ ì‚¬ìš© (k_per_query=3ì´ë¯€ë¡œ ìµœëŒ€ 3ê°œ)
                chunk_query_results = []
                for query in chunk_related_queries_list:
                    query_results = [qr for qr in query_search_results if qr.get("query", "") == query]
                    if query_results:
                        qr_item = query_results[0].copy()  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ê²°ê³¼ ì‚¬ìš©
                        # ğŸ”’ CRITICAL: results ë¦¬ìŠ¤íŠ¸ëŠ” top-3 ìœ ì§€ (k_per_query=3ì´ë¯€ë¡œ ìµœëŒ€ 3ê°œ)
                        if "results" in qr_item and isinstance(qr_item["results"], list) and len(qr_item["results"]) > 3:
                            qr_item["results"] = qr_item["results"][:3]  # top-3ë§Œ ìœ ì§€
                        chunk_query_results.append(qr_item)
                
                # VO ì „ìš© ë³€í™˜ (bc_name, agg_name í¬í•¨í•œ ì½œë°± ë˜í¼)
                def _vo_update_callback(progress: int, stage: str, 
                                        property_type: Optional[str] = None,
                                        chunk_info: Optional[str] = None,
                                        status: str = "processing",
                                        error_message: Optional[str] = None):
                    if update_progress_callback:
                        try:
                            update_progress_callback(progress, stage,
                                                    property_type=property_type or "vo",
                                                    chunk_info=chunk_info or f"ì²­í¬ {chunk_idx + 1}/{len(vo_chunks)}",
                                                    status=status,
                                                    error_message=error_message)
                        except Exception as e:
                            LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                chunk_result = self._transform_enums_vos_only_with_llm(
                    structure_item=chunk_structure,
                    bounded_context=bounded_context,
                    relevant_standards=relevant_standards,
                    query_search_results=chunk_query_results,
                    item_type="valueObjects",
                    update_progress_callback=_vo_update_callback
                )
                
                # ë³€í™˜ëœ valueObjects ì¶”ì¶œ ë° ë³‘í•©
                chunk_transformed_vos = chunk_result.get("valueObjects", [])
                transformed_vos.extend(chunk_transformed_vos)
        
        # 4ë‹¨ê³„: previewAttributes ì²­í¬ë³„ ë³€í™˜
        preview_attr_chunks = self._chunk_preview_attributes(preview_attrs, chunk_size)
        transformed_preview_attrs = []
        
        if preview_attr_chunks:
            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“¦ [ì²­í‚¹] 2ë‹¨ê³„: previewAttributes {len(preview_attrs)}ê°œë¥¼ {len(preview_attr_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
            
            for chunk_idx, chunk in enumerate(preview_attr_chunks):
                if update_progress_callback:
                    try:
                        update_progress_callback(0, f"LLM ë³€í™˜ ì¤‘: {agg_name_val} (í•„ë“œ ì²­í¬ {chunk_idx + 1}/{len(preview_attr_chunks)})",
                                                bc_name=bc_name_val,
                                                agg_name=agg_name_val,
                                                property_type="field",
                                                chunk_info=f"ì²­í¬ {chunk_idx + 1}/{len(preview_attr_chunks)}",
                                                status="processing")
                    except Exception as e:
                        LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                LoggingUtil.info("StandardTransformer", 
                               f"      ğŸ“¦ [ì²­í‚¹] previewAttributes ì²­í¬ {chunk_idx + 1}/{len(preview_attr_chunks)} ì²˜ë¦¬ ì¤‘ ({len(chunk)}ê°œ í•„ë“œ)")
                
                # í•„ë“œë§Œ í¬í•¨í•œ ìµœì†Œ structure ìƒì„± (Agg ì •ë³´ëŠ” ìµœì†Œí•œë§Œ í¬í•¨)
                chunk_structure = {
                    "aggregate": {
                        "name": structure_item.get("aggregate", {}).get("name", ""),
                        "alias": structure_item.get("aggregate", {}).get("alias", "")
                    },
                    "previewAttributes": chunk,
                    "enumerations": [],  # í•„ë“œ ë³€í™˜ì—ëŠ” ë¶ˆí•„ìš”
                    "valueObjects": [],  # í•„ë“œ ë³€í™˜ì—ëŠ” ë¶ˆí•„ìš”
                    "ddlFields": []  # previewAttributesë§Œ ì²˜ë¦¬
                }
                
                # ì²­í¬ì— ê´€ë ¨ëœ ê²€ìƒ‰ ê²°ê³¼ë§Œ í•„í„°ë§ (ê° ì¿¼ë¦¬ë³„ top-1ë§Œ ì‚¬ìš©, ìµœëŒ€ 3ê°œ ì¿¼ë¦¬)
                chunk_related_queries = set()
                for attr in chunk:
                    if isinstance(attr, dict):
                        field_name = attr.get("fieldName", "")
                        if field_name:
                            # í•„ë“œëª…ê³¼ ê´€ë ¨ëœ ì¿¼ë¦¬ ì°¾ê¸°
                            for qr in query_search_results:
                                query = qr.get("query", "")
                                if field_name.lower() in query.lower() or query.lower() in field_name.lower():
                                    chunk_related_queries.add(query)
                
                # ê´€ë ¨ ì¿¼ë¦¬ ì¤‘ ìµœëŒ€ 3ê°œë§Œ ì„ íƒ
                chunk_related_queries_list = list(chunk_related_queries)[:3]
                # ê° ì¿¼ë¦¬ë³„ë¡œ top-3 ê²°ê³¼ ì‚¬ìš© (k_per_query=3ì´ë¯€ë¡œ ìµœëŒ€ 3ê°œ)
                chunk_query_results = []
                for query in chunk_related_queries_list:
                    query_results = [qr for qr in query_search_results if qr.get("query", "") == query]
                    if query_results:
                        qr_item = query_results[0].copy()  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ê²°ê³¼ ì‚¬ìš©
                        # ğŸ”’ CRITICAL: results ë¦¬ìŠ¤íŠ¸ëŠ” top-3 ìœ ì§€ (k_per_query=3ì´ë¯€ë¡œ ìµœëŒ€ 3ê°œ)
                        if "results" in qr_item and isinstance(qr_item["results"], list) and len(qr_item["results"]) > 3:
                            qr_item["results"] = qr_item["results"][:3]  # top-3ë§Œ ìœ ì§€
                        chunk_query_results.append(qr_item)
                
                # í•„ë“œ ì „ìš© ë³€í™˜ (bc_name, agg_name í¬í•¨í•œ ì½œë°± ë˜í¼)
                def _field_update_callback(progress: int, stage: str, 
                                          property_type: Optional[str] = None,
                                          chunk_info: Optional[str] = None,
                                          status: str = "processing",
                                          error_message: Optional[str] = None):
                    if update_progress_callback:
                        try:
                            update_progress_callback(progress, stage,
                                                    property_type=property_type or "field",
                                                    chunk_info=chunk_info or f"ì²­í¬ {chunk_idx + 1}/{len(preview_attr_chunks)}",
                                                    status=status,
                                                    error_message=error_message)
                        except Exception as e:
                            LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                chunk_result = self._transform_fields_only_with_llm(
                    structure_item=chunk_structure,
                    bounded_context=bounded_context,
                    relevant_standards=relevant_standards,
                    query_search_results=chunk_query_results,
                    field_type="previewAttributes",
                    update_progress_callback=_field_update_callback
                )
                
                # ë³€í™˜ëœ previewAttributes ì¶”ì¶œ ë° ë³‘í•©
                chunk_transformed_attrs = chunk_result.get("previewAttributes", [])
                transformed_preview_attrs.extend(chunk_transformed_attrs)
        
        # 5ë‹¨ê³„: ddlFields ì²­í¬ë³„ ë³€í™˜
        ddl_field_chunks = self._chunk_ddl_fields(ddl_fields, chunk_size)
        transformed_ddl_fields = []
        
        if ddl_field_chunks:
            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“¦ [ì²­í‚¹] 5ë‹¨ê³„: ddlFields {len(ddl_fields)}ê°œë¥¼ {len(ddl_field_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
            
            for chunk_idx, chunk in enumerate(ddl_field_chunks):
                if update_progress_callback:
                    try:
                        update_progress_callback(0, f"LLM ë³€í™˜ ì¤‘: {agg_name_val} (DDL í•„ë“œ ì²­í¬ {chunk_idx + 1}/{len(ddl_field_chunks)})",
                                                bc_name=bc_name_val,
                                                agg_name=agg_name_val,
                                                property_type="field",
                                                chunk_info=f"DDL ì²­í¬ {chunk_idx + 1}/{len(ddl_field_chunks)}",
                                                status="processing")
                    except Exception as e:
                        LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                LoggingUtil.info("StandardTransformer", 
                               f"      ğŸ“¦ [ì²­í‚¹] ddlFields ì²­í¬ {chunk_idx + 1}/{len(ddl_field_chunks)} ì²˜ë¦¬ ì¤‘ ({len(chunk)}ê°œ í•„ë“œ)")
                
                # í•„ë“œë§Œ í¬í•¨í•œ ìµœì†Œ structure ìƒì„± (Agg ì •ë³´ëŠ” ìµœì†Œí•œë§Œ í¬í•¨)
                chunk_structure = {
                    "aggregate": {
                        "name": structure_item.get("aggregate", {}).get("name", ""),
                        "alias": structure_item.get("aggregate", {}).get("alias", "")
                    },
                    "ddlFields": chunk,
                    "enumerations": [],  # í•„ë“œ ë³€í™˜ì—ëŠ” ë¶ˆí•„ìš”
                    "valueObjects": [],  # í•„ë“œ ë³€í™˜ì—ëŠ” ë¶ˆí•„ìš”
                    "previewAttributes": []  # ddlFieldsë§Œ ì²˜ë¦¬
                }
                
                # ì²­í¬ì— ê´€ë ¨ëœ ê²€ìƒ‰ ê²°ê³¼ë§Œ í•„í„°ë§ (ê° ì¿¼ë¦¬ë³„ top-1ë§Œ ì‚¬ìš©, ìµœëŒ€ 3ê°œ ì¿¼ë¦¬)
                chunk_related_queries = set()
                for field in chunk:
                    if isinstance(field, dict):
                        field_name = field.get("fieldName", "")
                        if field_name:
                            # í•„ë“œëª…ê³¼ ê´€ë ¨ëœ ì¿¼ë¦¬ ì°¾ê¸°
                            for qr in query_search_results:
                                query = qr.get("query", "")
                                if field_name.lower() in query.lower() or query.lower() in field_name.lower():
                                    chunk_related_queries.add(query)
                
                # ê´€ë ¨ ì¿¼ë¦¬ ì¤‘ ìµœëŒ€ 3ê°œë§Œ ì„ íƒ
                chunk_related_queries_list = list(chunk_related_queries)[:3]
                # ê° ì¿¼ë¦¬ë³„ë¡œ top-3 ê²°ê³¼ ì‚¬ìš© (k_per_query=3ì´ë¯€ë¡œ ìµœëŒ€ 3ê°œ)
                chunk_query_results = []
                for query in chunk_related_queries_list:
                    query_results = [qr for qr in query_search_results if qr.get("query", "") == query]
                    if query_results:
                        qr_item = query_results[0].copy()  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ê²°ê³¼ ì‚¬ìš©
                        # ğŸ”’ CRITICAL: results ë¦¬ìŠ¤íŠ¸ëŠ” top-3 ìœ ì§€ (k_per_query=3ì´ë¯€ë¡œ ìµœëŒ€ 3ê°œ)
                        if "results" in qr_item and isinstance(qr_item["results"], list) and len(qr_item["results"]) > 3:
                            qr_item["results"] = qr_item["results"][:3]  # top-3ë§Œ ìœ ì§€
                        chunk_query_results.append(qr_item)
                
                # DDL í•„ë“œ ì „ìš© ë³€í™˜ (bc_name, agg_name í¬í•¨í•œ ì½œë°± ë˜í¼)
                def _ddl_field_update_callback(progress: int, stage: str, 
                                               property_type: Optional[str] = None,
                                               chunk_info: Optional[str] = None,
                                               status: str = "processing",
                                               error_message: Optional[str] = None):
                    if update_progress_callback:
                        try:
                            update_progress_callback(progress, stage,
                                                    property_type=property_type or "field",
                                                    chunk_info=chunk_info or f"DDL ì²­í¬ {chunk_idx + 1}/{len(ddl_field_chunks)}",
                                                    status=status,
                                                    error_message=error_message)
                        except Exception as e:
                            LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                chunk_result = self._transform_fields_only_with_llm(
                    structure_item=chunk_structure,
                    bounded_context=bounded_context,
                    relevant_standards=relevant_standards,
                    query_search_results=chunk_query_results,
                    field_type="ddlFields",
                    update_progress_callback=_ddl_field_update_callback
                )
                
                # ë³€í™˜ëœ ddlFields ì¶”ì¶œ ë° ë³‘í•©
                chunk_transformed_ddl = chunk_result.get("ddlFields", [])
                transformed_ddl_fields.extend(chunk_transformed_ddl)
        
        # 6ë‹¨ê³„: ëª¨ë“  ê²°ê³¼ ë³‘í•©
        LoggingUtil.info("StandardTransformer", 
                       f"      ğŸ“¦ [ì²­í‚¹] 4ë‹¨ê³„: ëª¨ë“  ì²­í¬ ê²°ê³¼ ë³‘í•© ì¤‘")
        
        # ğŸ”’ CRITICAL: original_structure_itemì—ì„œ ì›ë³¸ êµ¬ì¡° ê°€ì ¸ì˜¤ê¸° (refs í¬í•¨)
        if original_structure_item:
            merged_result = copy.deepcopy(original_structure_item)
            # ë³€í™˜ëœ aggregate ì´ë¦„ë§Œ ì—…ë°ì´íŠ¸
            if "aggregate" in base_result and "aggregate" in merged_result:
                merged_result["aggregate"]["name"] = base_result["aggregate"].get("name", merged_result["aggregate"].get("name"))
                merged_result["aggregate"]["alias"] = base_result["aggregate"].get("alias", merged_result["aggregate"].get("alias"))
        else:
            merged_result = copy.deepcopy(base_result)
        
        # ğŸ”’ CRITICAL: original_structure_itemì—ì„œ ì›ë³¸ enumerations, valueObjects, previewAttributes, ddlFields ê°€ì ¸ì˜¤ê¸°
        original_enumerations = []
        original_value_objects = []
        original_preview_attrs = []
        original_ddl_fields = []
        if original_structure_item:
            original_enumerations = original_structure_item.get("enumerations", [])
            original_value_objects = original_structure_item.get("valueObjects", [])
            original_preview_attrs = original_structure_item.get("previewAttributes", [])
            original_ddl_fields = original_structure_item.get("ddlFields", [])
        
        # enumerations ë³‘í•© (refs ë³µì›)
        if transformed_enums and len(transformed_enums) == len(enumerations):
            merged_enums = []
            for i, trans_enum in enumerate(transformed_enums):
                if i < len(original_enumerations):
                    # original_structure_itemì—ì„œ ì›ë³¸ enum ê°€ì ¸ì˜¤ê¸° (refs í¬í•¨)
                    merged_enum = copy.deepcopy(original_enumerations[i])
                    # ë³€í™˜ëœ ì´ë¦„ë§Œ ì—…ë°ì´íŠ¸
                    if "name" in trans_enum:
                        merged_enum["name"] = trans_enum["name"]
                    if "alias" in trans_enum:
                        merged_enum["alias"] = trans_enum["alias"]
                    merged_enums.append(merged_enum)
                else:
                    merged_enums.append(trans_enum)
            merged_result["enumerations"] = merged_enums
        else:
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œ original_structure_itemì—ì„œ ë³µì›
            if original_structure_item:
                merged_result["enumerations"] = copy.deepcopy(original_enumerations)
            else:
                merged_result["enumerations"] = enumerations
            if transformed_enums:
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  [ì²­í‚¹] enumerations ë³‘í•© ì‹¤íŒ¨: ì›ë³¸ {len(enumerations)}ê°œ, ë³€í™˜ {len(transformed_enums)}ê°œ")
        
        # valueObjects ë³‘í•© (refs ë³µì›)
        if transformed_vos and len(transformed_vos) == len(value_objects):
            merged_vos = []
            for i, trans_vo in enumerate(transformed_vos):
                if i < len(original_value_objects):
                    # original_structure_itemì—ì„œ ì›ë³¸ VO ê°€ì ¸ì˜¤ê¸° (refs í¬í•¨)
                    merged_vo = copy.deepcopy(original_value_objects[i])
                    # ë³€í™˜ëœ ì´ë¦„ë§Œ ì—…ë°ì´íŠ¸
                    if "name" in trans_vo:
                        merged_vo["name"] = trans_vo["name"]
                    if "alias" in trans_vo:
                        merged_vo["alias"] = trans_vo["alias"]
                    if "referencedAggregateName" in trans_vo:
                        merged_vo["referencedAggregateName"] = trans_vo["referencedAggregateName"]
                    merged_vos.append(merged_vo)
                else:
                    merged_vos.append(trans_vo)
            merged_result["valueObjects"] = merged_vos
        else:
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œ original_structure_itemì—ì„œ ë³µì›
            if original_structure_item:
                merged_result["valueObjects"] = copy.deepcopy(original_value_objects)
            else:
                merged_result["valueObjects"] = value_objects
            if transformed_vos:
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  [ì²­í‚¹] valueObjects ë³‘í•© ì‹¤íŒ¨: ì›ë³¸ {len(value_objects)}ê°œ, ë³€í™˜ {len(transformed_vos)}ê°œ")
        
        # previewAttributes ë³‘í•© (fieldAlias ê¸°ë°˜ ë§¤ì¹­ - ì¸ë±ìŠ¤ ê¸°ë°˜ì€ LLMì´ ìˆœì„œë¥¼ ë°”ê¿€ ìˆ˜ ìˆì–´ ë¶ˆì•ˆì •)
        if transformed_preview_attrs and len(transformed_preview_attrs) == len(preview_attrs):
            # ì›ë³¸ í•„ë“œë¥¼ fieldAliasë¡œ ì¸ë±ì‹± (fieldAliasëŠ” ë³€í™˜ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì•ˆì „í•œ í‚¤)
            original_attrs_by_alias = {}  # {fieldAlias: original_attr}
            for orig_attr in original_preview_attrs:
                if isinstance(orig_attr, dict):
                    field_alias = orig_attr.get("fieldAlias")
                    if field_alias:
                        original_attrs_by_alias[field_alias] = orig_attr
            
            # ë³€í™˜ëœ í•„ë“œë¥¼ fieldAliasë¡œ ë§¤ì¹­í•˜ì—¬ ë³‘í•©
            merged_preview_attrs = []
            for transformed_attr in transformed_preview_attrs:
                if isinstance(transformed_attr, dict):
                    trans_field_name = transformed_attr.get("fieldName")
                    trans_field_alias = transformed_attr.get("fieldAlias")
                    
                    # fieldAliasë¡œ ë§¤ì¹­ ì‹œë„ (ê°€ì¥ ì•ˆì „ - ë³€í™˜ë˜ì§€ ì•ŠìŒ)
                    matched_original = None
                    if trans_field_alias and trans_field_alias in original_attrs_by_alias:
                        matched_original = original_attrs_by_alias[trans_field_alias]
                    elif len(merged_preview_attrs) < len(original_preview_attrs):
                        # fieldAlias ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì¸ë±ìŠ¤ ê¸°ë°˜ fallback (ìˆœì„œê°€ ê°™ë‹¤ê³  ê°€ì •)
                        idx = len(merged_preview_attrs)
                        if idx < len(original_preview_attrs):
                            candidate = original_preview_attrs[idx]
                            if isinstance(candidate, dict):
                                # ì›ë³¸ì˜ fieldAliasê°€ transformed_attrì˜ fieldAliasì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                                orig_field_alias = candidate.get("fieldAlias")
                                if not trans_field_alias or orig_field_alias == trans_field_alias:
                                    matched_original = candidate
                    
                    # ë§¤ì¹­ëœ ì›ë³¸ì´ ìˆìœ¼ë©´ ë³µì‚¬í•˜ê³  fieldNameë§Œ ì—…ë°ì´íŠ¸
                    if matched_original:
                        merged_attr = copy.deepcopy(matched_original)
                        merged_attr["fieldName"] = trans_field_name
                        merged_preview_attrs.append(merged_attr)
                    else:
                        # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ transformed_attr ê·¸ëŒ€ë¡œ ì‚¬ìš© (refsëŠ” ì›ë³¸ì—ì„œ ë³µì› ë¶ˆê°€)
                        merged_preview_attrs.append(transformed_attr)
                else:
                    merged_preview_attrs.append(transformed_attr)
            merged_result["previewAttributes"] = merged_preview_attrs
        else:
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œ original_structure_itemì—ì„œ ë³µì›
            if original_structure_item:
                merged_result["previewAttributes"] = copy.deepcopy(original_preview_attrs)
            else:
                merged_result["previewAttributes"] = preview_attrs
            if transformed_preview_attrs:
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  [ì²­í‚¹] previewAttributes ë³‘í•© ì‹¤íŒ¨: ì›ë³¸ {len(preview_attrs)}ê°œ, ë³€í™˜ {len(transformed_preview_attrs)}ê°œ")
        
        # ddlFields ë³‘í•© (fieldAlias ê¸°ë°˜ ë§¤ì¹­ - ì¸ë±ìŠ¤ ê¸°ë°˜ì€ LLMì´ ìˆœì„œë¥¼ ë°”ê¿€ ìˆ˜ ìˆì–´ ë¶ˆì•ˆì •)
        if transformed_ddl_fields and len(transformed_ddl_fields) == len(ddl_fields):
            # ì›ë³¸ í•„ë“œë¥¼ fieldAliasë¡œ ì¸ë±ì‹± (fieldAliasëŠ” ë³€í™˜ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì•ˆì „í•œ í‚¤)
            original_ddl_by_alias = {}  # {fieldAlias: original_field}
            for orig_field in original_ddl_fields:
                if isinstance(orig_field, dict):
                    field_alias = orig_field.get("fieldAlias")
                    if field_alias:
                        original_ddl_by_alias[field_alias] = orig_field
            
            # ë³€í™˜ëœ í•„ë“œë¥¼ fieldAliasë¡œ ë§¤ì¹­í•˜ì—¬ ë³‘í•©
            merged_ddl_fields = []
            for transformed_field in transformed_ddl_fields:
                if isinstance(transformed_field, dict):
                    trans_field_name = transformed_field.get("fieldName")
                    trans_field_alias = transformed_field.get("fieldAlias")
                    
                    # fieldAliasë¡œ ë§¤ì¹­ ì‹œë„ (ê°€ì¥ ì•ˆì „ - ë³€í™˜ë˜ì§€ ì•ŠìŒ)
                    matched_original = None
                    if trans_field_alias and trans_field_alias in original_ddl_by_alias:
                        matched_original = original_ddl_by_alias[trans_field_alias]
                    elif len(merged_ddl_fields) < len(original_ddl_fields):
                        # fieldAlias ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì¸ë±ìŠ¤ ê¸°ë°˜ fallback (ìˆœì„œê°€ ê°™ë‹¤ê³  ê°€ì •)
                        idx = len(merged_ddl_fields)
                        if idx < len(original_ddl_fields):
                            candidate = original_ddl_fields[idx]
                            if isinstance(candidate, dict):
                                # ì›ë³¸ì˜ fieldAliasê°€ transformed_fieldì˜ fieldAliasì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                                orig_field_alias = candidate.get("fieldAlias")
                                if not trans_field_alias or orig_field_alias == trans_field_alias:
                                    matched_original = candidate
                    
                    # ë§¤ì¹­ëœ ì›ë³¸ì´ ìˆìœ¼ë©´ ë³µì‚¬í•˜ê³  fieldNameë§Œ ì—…ë°ì´íŠ¸
                    if matched_original:
                        merged_field = copy.deepcopy(matched_original)
                        merged_field["fieldName"] = trans_field_name
                        merged_ddl_fields.append(merged_field)
                    else:
                        # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ transformed_field ê·¸ëŒ€ë¡œ ì‚¬ìš© (refsëŠ” ì›ë³¸ì—ì„œ ë³µì› ë¶ˆê°€)
                        merged_ddl_fields.append(transformed_field)
                else:
                    merged_ddl_fields.append(transformed_field)
            merged_result["ddlFields"] = merged_ddl_fields
        else:
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œ original_structure_itemì—ì„œ ë³µì›
            if original_structure_item:
                merged_result["ddlFields"] = copy.deepcopy(original_ddl_fields)
            else:
                merged_result["ddlFields"] = ddl_fields
            if transformed_ddl_fields:
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  [ì²­í‚¹] ddlFields ë³‘í•© ì‹¤íŒ¨: ì›ë³¸ {len(ddl_fields)}ê°œ, ë³€í™˜ {len(transformed_ddl_fields)}ê°œ")
        
        LoggingUtil.info("StandardTransformer", 
                       f"      âœ… [ì²­í‚¹] ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ: previewAttributes={len(merged_result.get('previewAttributes', []))}ê°œ, ddlFields={len(merged_result.get('ddlFields', []))}ê°œ")
        
        # ì²­í‚¹ ì™„ë£Œ ì•Œë¦¼
        if update_progress_callback:
            try:
                update_progress_callback(100, f"ì²­í‚¹ ì²˜ë¦¬ ì™„ë£Œ: {agg_name_val}",
                                        bc_name=bc_name_val,
                                        agg_name=agg_name_val,
                                        property_type="aggregate",
                                        status="completed")
            except Exception as e:
                LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        
        return merged_result
    
    def _transform_fields_only_with_llm(
        self,
        structure_item: Dict,
        bounded_context: Dict,
        relevant_standards: List[Dict],
        query_search_results: List[Dict],
        field_type: str,  # "previewAttributes" or "ddlFields"
        update_progress_callback: Optional[callable] = None
    ) -> Dict:
        """
        í•„ë“œë§Œ ë³€í™˜í•˜ëŠ” ì „ìš© ë©”ì„œë“œ (Agg ì •ë³´ëŠ” ì´ë¯¸ ë³€í™˜ë¨, í•„ë“œì™€ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì²­í‚¹ ì²˜ë¦¬)
        
        Args:
            structure_item: í•„ë“œë§Œ í¬í•¨í•œ ìµœì†Œ structure (aggregate.name, aggregate.alias, í•„ë“œ ë°°ì—´ë§Œ)
            bounded_context: Bounded Context ì •ë³´
            relevant_standards: ê²€ìƒ‰ëœ í‘œì¤€ ì²­í¬ë“¤
            query_search_results: í•„í„°ë§ëœ ê²€ìƒ‰ ê²°ê³¼ (ì²­í¬ ê´€ë ¨ ì¿¼ë¦¬ë§Œ)
            field_type: "previewAttributes" or "ddlFields"
            update_progress_callback: ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì½œë°±
            
        Returns:
            ë³€í™˜ëœ í•„ë“œ ë°°ì—´ì„ í¬í•¨í•œ structure
        """
        # í•„ë“œ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_field_transformation_prompt(
            structure_item=structure_item,
            bounded_context=bounded_context,
            relevant_standards=relevant_standards,
            query_search_results=query_search_results,
            field_type=field_type
        )
        
        # í•„ë“œ ì „ìš© ìŠ¤í‚¤ë§ˆ ìƒì„±
        field_schema = self._get_field_response_schema(field_type)
        
        # LLM í˜¸ì¶œ
        try:
            agg_name = structure_item.get("aggregate", {}).get("name", "Unknown")
            field_count = len(structure_item.get(field_type, []))
            
            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“¤ [í•„ë“œ ì „ìš©] LLM API í˜¸ì¶œ ì‹œì‘: {field_type} {field_count}ê°œ")
            
            # í•„ë“œ ì „ìš© structured output ìƒì„±
            llm_structured = self.llm.with_structured_output(field_schema)
            
            max_retries = 2
            retry_count = 0
            response = None
            
            while retry_count <= max_retries:
                try:
                    response = llm_structured.invoke(prompt)
                    break
                except Exception as e:
                    retry_count += 1
                    error_msg = str(e)
                    
                    LoggingUtil.warning("StandardTransformer", 
                                      f"      âš ï¸  [í•„ë“œ ì „ìš©] LLM API í˜¸ì¶œ ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries}): {e}")
                    
                    if retry_count > max_retries:
                        if update_progress_callback:
                            try:
                                update_progress_callback(0, f"âŒ í•„ë“œ ë³€í™˜ ì‹¤íŒ¨: {field_type} (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼)",
                                                        property_type="field",
                                                        status="error",
                                                        error_message=f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {error_msg}")
                            except Exception as update_e:
                                LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_e}")
                        raise
                    
                    import time
                    time.sleep(2)
            
            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“¥ [í•„ë“œ ì „ìš©] LLM API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
            
            result = response.get("result", {})
            transformed_fields = result.get(field_type, [])
            
            if not transformed_fields:
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  [í•„ë“œ ì „ìš©] LLM ì‘ë‹µì— {field_type}ê°€ ì—†ìŒ, ì›ë³¸ ë°˜í™˜")
                return structure_item
            
            # ë³€í™˜ëœ í•„ë“œë§Œ ë°˜í™˜
            import copy
            result_structure = copy.deepcopy(structure_item)
            result_structure[field_type] = transformed_fields
            
            return result_structure
            
        except Exception as e:
            LoggingUtil.error("StandardTransformer", 
                            f"âŒ [í•„ë“œ ì „ìš©] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            LoggingUtil.error("StandardTransformer", traceback.format_exc())
            if update_progress_callback:
                try:
                    update_progress_callback(0, f"âŒ í•„ë“œ ë³€í™˜ ì‹¤íŒ¨: {field_type}",
                                            property_type="field",
                                            status="error",
                                            error_message=str(e))
                except Exception as update_e:
                    LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_e}")
            return structure_item
    
    def _build_field_transformation_prompt(
        self,
        structure_item: Dict,
        bounded_context: Dict,
        relevant_standards: List[Dict],
        query_search_results: List[Dict],
        field_type: str
    ) -> str:
        """
        í•„ë“œ ì „ìš© ë³€í™˜ í”„ë¡¬í”„íŠ¸ ìƒì„± (Agg ì •ë³´ëŠ” ìµœì†Œí•œë§Œ í¬í•¨, í•„ë“œì™€ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì§‘ì¤‘)
        """
        # í‘œì¤€ ë¬¸ì„œ í¬ë§·íŒ… (ê¸°ì¡´ê³¼ ë™ì¼)
        standards_text = ""
        
        # ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ ë³€í™˜
        transformed_query_results = {}
        if query_search_results:
            for qr in query_search_results:
                query = qr.get("query", "")
                if not query:
                    continue
                
                if "results" in qr:
                    results_list = qr["results"]
                    query_results = []
                    for result_item in results_list:
                        query_results.append(result_item.get("result", {}))
                    transformed_query_results[query] = query_results
        
        if transformed_query_results:
            standards_text += "\n## Standard Transformation Reference (Field-Related Queries Only):\n"
            standards_text += json.dumps(transformed_query_results, ensure_ascii=False, indent=2)
        
        # Aggregate ì •ë³´ (ìµœì†Œí•œë§Œ)
        agg_name = structure_item.get("aggregate", {}).get("name", "")
        agg_alias = structure_item.get("aggregate", {}).get("alias", "")
        
        # í•„ë“œ ì •ë³´
        fields = structure_item.get(field_type, [])
        fields_json = json.dumps(fields, ensure_ascii=False, indent=2)
        
        bc_name = bounded_context.get("name", "")
        
        prompt = f"""You are a Standard Naming Transformer specialized in field name transformation.

## Task:
Transform ONLY the `fieldName` values in the `{field_type}` array based on the "Standard Transformation Reference" below.
**Aggregate information is already transformed - DO NOT modify aggregate.name or aggregate.alias.**

## Context:
- **Bounded Context**: {bc_name}
- **Aggregate**: {agg_name} ({agg_alias})

## Input Structure:
```json
{{
  "aggregate": {{
    "name": "{agg_name}",
    "alias": "{agg_alias}"
  }},
  "{field_type}": {fields_json}
}}
```

{standards_text}

## Instructions:
1. **ONLY transform `{field_type}[].fieldName`** - Check EVERY field in the array
2. **DO NOT modify** aggregate.name, aggregate.alias, or any other fields
3. **Preserve ALL fields** - Every field in input must exist in output
4. **Match and Transform**: For each `fieldName`, check if it matches a key in "Standard Transformation Reference"
   - If matched, use the most appropriate `í‘œì¤€ëª…` from the candidate list
   - If no match, keep original unchanged
5. **Preserve fieldAlias**: Keep all `fieldAlias` values unchanged (they are Korean text for matching)

## Output Format:
Return JSON with the EXACT same structure as input, ONLY changing `fieldName` values that match the reference.

## CRITICAL:
- Return ALL fields from input (same count, same order)
- Only `fieldName` values may change
- All other fields (fieldAlias, className, type, etc.) must remain unchanged
"""
        
        return prompt
    
    def _get_field_response_schema(self, field_type: str) -> Dict:
        """
        í•„ë“œ ì „ìš© ì‘ë‹µ ìŠ¤í‚¤ë§ˆ (í•„ë“œ ë°°ì—´ë§Œ í¬í•¨)
        """
        field_properties = {
            "fieldName": {"type": "string"},
            "fieldAlias": {"type": "string"}
        }
        
        if field_type == "ddlFields":
            field_properties["className"] = {"type": "string"}
            field_properties["type"] = {"type": "string"}
        
        return {
            "title": "FieldTransformationResponse",
            "description": "Response schema for field-only transformation",
            "type": "object",
            "properties": {
                "inference": {
                    "type": "string",
                    "description": "Brief explanation of transformation process"
                },
                "result": {
                    "type": "object",
                    "properties": {
                        "aggregate": {
                            "type": "object",
                            "properties": {
                                "name": {"type": "string"},
                                "alias": {"type": "string"}
                            },
                            "required": ["name", "alias"],
                            "additionalProperties": True
                        },
                        field_type: {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": field_properties,
                                "required": ["fieldName"],
                                "additionalProperties": True
                            }
                        }
                    },
                    "required": ["aggregate", field_type],
                    "additionalProperties": True
                }
            },
            "required": ["inference", "result"],
            "additionalProperties": False
        }
    
    def _transform_enums_vos_only_with_llm(
        self,
        structure_item: Dict,
        bounded_context: Dict,
        relevant_standards: List[Dict],
        query_search_results: List[Dict],
        item_type: str,  # "enumerations" or "valueObjects"
        update_progress_callback: Optional[callable] = None
    ) -> Dict:
        """
        Enum/VOë§Œ ë³€í™˜í•˜ëŠ” ì „ìš© ë©”ì„œë“œ (Agg ì •ë³´ëŠ” ì´ë¯¸ ë³€í™˜ë¨, Enum/VOì™€ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì²­í‚¹ ì²˜ë¦¬)
        
        Args:
            structure_item: Enum/VOë§Œ í¬í•¨í•œ ìµœì†Œ structure (aggregate.name, aggregate.alias, enum/vo ë°°ì—´ë§Œ)
            bounded_context: Bounded Context ì •ë³´
            relevant_standards: ê²€ìƒ‰ëœ í‘œì¤€ ì²­í¬ë“¤
            query_search_results: í•„í„°ë§ëœ ê²€ìƒ‰ ê²°ê³¼ (ì²­í¬ ê´€ë ¨ ì¿¼ë¦¬ë§Œ)
            item_type: "enumerations" or "valueObjects"
            update_progress_callback: ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì½œë°±
            
        Returns:
            ë³€í™˜ëœ Enum/VO ë°°ì—´ì„ í¬í•¨í•œ structure
        """
        # Enum/VO ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_enum_vo_transformation_prompt(
            structure_item=structure_item,
            bounded_context=bounded_context,
            relevant_standards=relevant_standards,
            query_search_results=query_search_results,
            item_type=item_type
        )
        
        # Enum/VO ì „ìš© ìŠ¤í‚¤ë§ˆ ìƒì„±
        enum_vo_schema = self._get_enum_vo_response_schema(item_type)
        
        # LLM í˜¸ì¶œ
        try:
            agg_name = structure_item.get("aggregate", {}).get("name", "Unknown")
            item_count = len(structure_item.get(item_type, []))
            
            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“¤ [Enum/VO ì „ìš©] LLM API í˜¸ì¶œ ì‹œì‘: {item_type} {item_count}ê°œ")
            
            # Enum/VO ì „ìš© structured output ìƒì„±
            llm_structured = self.llm.with_structured_output(enum_vo_schema)
            
            max_retries = 2
            retry_count = 0
            response = None
            
            while retry_count <= max_retries:
                try:
                    response = llm_structured.invoke(prompt)
                    break
                except Exception as e:
                    retry_count += 1
                    error_msg = str(e)
                    
                    LoggingUtil.warning("StandardTransformer", 
                                      f"      âš ï¸  [Enum/VO ì „ìš©] LLM API í˜¸ì¶œ ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries}): {e}")
                    
                    if retry_count > max_retries:
                        if update_progress_callback:
                            try:
                                item_label = "Enum" if item_type == "enumerations" else "ValueObject"
                                update_progress_callback(0, f"âŒ {item_label} ë³€í™˜ ì‹¤íŒ¨ (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼)",
                                                        property_type="enum" if item_type == "enumerations" else "vo",
                                                        status="error",
                                                        error_message=f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {error_msg}")
                            except Exception as update_e:
                                LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_e}")
                        raise
                    
                    import time
                    time.sleep(2)

            LoggingUtil.info("StandardTransformer", 
                           f"      ğŸ“¥ [Enum/VO ì „ìš©] LLM API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
            
            result = response.get("result", {})
            transformed_items = result.get(item_type, [])
            
            if not transformed_items:
                LoggingUtil.warning("StandardTransformer", 
                                  f"âš ï¸  [Enum/VO ì „ìš©] LLM ì‘ë‹µì— {item_type}ê°€ ì—†ìŒ, ì›ë³¸ ë°˜í™˜")
                return structure_item
            
            # ë³€í™˜ëœ Enum/VOë§Œ ë°˜í™˜
            import copy
            result_structure = copy.deepcopy(structure_item)
            result_structure[item_type] = transformed_items
            
            return result_structure
            
        except Exception as e:
            LoggingUtil.error("StandardTransformer", 
                            f"âŒ [Enum/VO ì „ìš©] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            LoggingUtil.error("StandardTransformer", traceback.format_exc())
            if update_progress_callback:
                try:
                    item_label = "Enum" if item_type == "enumerations" else "ValueObject"
                    update_progress_callback(0, f"âŒ {item_label} ë³€í™˜ ì‹¤íŒ¨",
                                            property_type="enum" if item_type == "enumerations" else "vo",
                                            status="error",
                                            error_message=str(e))
                except Exception as update_e:
                    LoggingUtil.warning("StandardTransformer", f"ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_e}")
            return structure_item
    
    def _build_enum_vo_transformation_prompt(
        self,
        structure_item: Dict,
        bounded_context: Dict,
        relevant_standards: List[Dict],
        query_search_results: List[Dict],
        item_type: str
    ) -> str:
        """
        Enum/VO ì „ìš© ë³€í™˜ í”„ë¡¬í”„íŠ¸ ìƒì„± (Agg ì •ë³´ëŠ” ìµœì†Œí•œë§Œ í¬í•¨, Enum/VOì™€ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì§‘ì¤‘)
        """
        # í‘œì¤€ ë¬¸ì„œ í¬ë§·íŒ…
        standards_text = ""
        
        # ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ ë³€í™˜
        transformed_query_results = {}
        if query_search_results:
            for qr in query_search_results:
                query = qr.get("query", "")
                if not query:
                    continue
                
                if "results" in qr:
                    results_list = qr["results"]
                    query_results = []
                    for result_item in results_list:
                        query_results.append(result_item.get("result", {}))
                    transformed_query_results[query] = query_results
        
        if transformed_query_results:
            standards_text += "\n## Standard Transformation Reference (Related Queries Only):\n"
            standards_text += json.dumps(transformed_query_results, ensure_ascii=False, indent=2)
        
        # Aggregate ì •ë³´ (ìµœì†Œí•œë§Œ)
        agg_name = structure_item.get("aggregate", {}).get("name", "")
        agg_alias = structure_item.get("aggregate", {}).get("alias", "")
        
        # Enum/VO ì •ë³´
        items = structure_item.get(item_type, [])
        items_json = json.dumps(items, ensure_ascii=False, indent=2)
        
        bc_name = bounded_context.get("name", "")
        item_label = "enumerations" if item_type == "enumerations" else "value objects"
        
        prompt = f"""You are a Standard Naming Transformer specialized in {item_label} name transformation.

## Task:
Transform ONLY the `name` values in the `{item_type}` array based on the "Standard Transformation Reference" below.
**Aggregate information is already transformed - DO NOT modify aggregate.name or aggregate.alias.**

## Context:
- **Bounded Context**: {bc_name}
- **Aggregate**: {agg_name} ({agg_alias})

## Input Structure:
```json
{{
  "aggregate": {{
    "name": "{agg_name}",
    "alias": "{agg_alias}"
  }},
  "{item_type}": {items_json}
}}
```

{standards_text}

## Instructions:
1. **ONLY transform `{item_type}[].name`** - Check EVERY item in the array
2. **DO NOT modify** aggregate.name, aggregate.alias, or any other fields
3. **Preserve ALL items** - Every item in input must exist in output
4. **Match and Transform**: For each `name`, check if it matches a key in "Standard Transformation Reference"
   - If matched, use the most appropriate `í‘œì¤€ëª…` from the candidate list
   - If no match, keep original unchanged
5. **Preserve alias**: Keep all `alias` values unchanged (they are Korean text for matching)
6. **Preserve referencedAggregateName**: For valueObjects, keep `referencedAggregateName` unchanged

## Output Format:
Return JSON with the EXACT same structure as input, ONLY changing `name` values that match the reference.

## CRITICAL:
- Return ALL items from input (same count, same order)
- Only `name` values may change
- All other fields (alias, referencedAggregateName, etc.) must remain unchanged
"""
        
        return prompt
    
    def _get_enum_vo_response_schema(self, item_type: str) -> Dict:
        """
        Enum/VO ì „ìš© ì‘ë‹µ ìŠ¤í‚¤ë§ˆ (Enum/VO ë°°ì—´ë§Œ í¬í•¨)
        """
        item_properties = {
            "name": {"type": "string"},
            "alias": {"type": "string"}
        }
        
        if item_type == "valueObjects":
            item_properties["referencedAggregateName"] = {"type": "string"}
        
        return {
            "title": "EnumVOTransformationResponse",
            "description": "Response schema for enum/vo-only transformation",
            "type": "object",
            "properties": {
                "inference": {
                    "type": "string",
                    "description": "Brief explanation of transformation process"
                },
                "result": {
                    "type": "object",
                    "properties": {
                        "aggregate": {
                            "type": "object",
                            "properties": {
                                "name": {"type": "string"},
                                "alias": {"type": "string"}
                            },
                            "required": ["name", "alias"],
                            "additionalProperties": True
                        },
                        item_type: {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": item_properties,
                                "required": ["name", "alias"] if item_type == "enumerations" else ["name", "alias", "referencedAggregateName"],
                                "additionalProperties": True
                            }
                        }
                    },
                    "required": ["aggregate", item_type],
                    "additionalProperties": True
                }
            },
            "required": ["inference", "result"],
            "additionalProperties": False
        }

