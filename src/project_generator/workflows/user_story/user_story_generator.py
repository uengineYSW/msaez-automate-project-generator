"""
UserStoryGenerator - LangGraph ì›Œí¬í”Œë¡œìš°
RAGë¥¼ í™œìš©í•œ User Story ìë™ ìƒì„±
"""
from typing import TypedDict, List, Dict, Annotated
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
import json
from datetime import datetime
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.project_generator.workflows.common.rag_retriever import RAGRetriever
from src.project_generator.config import Config
from src.project_generator.utils.logging_util import LoggingUtil


class UserStoryState(TypedDict):
    """UserStory ìƒì„± ìƒíƒœ (camelCase for Frontend compatibility)"""
    # Inputs
    requirements: str                    # ì…ë ¥ ìš”êµ¬ì‚¬í•­
    bounded_contexts: List[Dict]         # Bounded Context ì •ë³´ (ì„ íƒ)
    
    # RAG Context
    rag_context: Dict                   # RAG ê²€ìƒ‰ ê²°ê³¼
    
    # Outputs
    userStories: List[Dict]             # ìƒì„±ëœ User Stories (camelCase)
    actors: List[Dict]                  # Actors
    businessRules: List[Dict]           # Business Rules
    boundedContexts: List[Dict]         # Bounded Contexts
    textResponse: str                   # í…ìŠ¤íŠ¸ ëª¨ë“œ ì‘ë‹µ (ì„ íƒ)
    
    # Metadata
    progress: int                       # ì§„í–‰ë¥  (0-100)
    logs: Annotated[List[Dict], "append"]  # ë¡œê·¸ (append only)
    isCompleted: bool                   # ì™„ë£Œ ì—¬ë¶€ (camelCase)
    error: str                         # ì—ëŸ¬ ë©”ì‹œì§€


class UserStoryWorkflow:
    """
    UserStory ìƒì„± ì›Œí¬í”Œë¡œìš°
    
    Workflow:
    1. retrieve_rag: RAGë¡œ ìœ ì‚¬ í”„ë¡œì íŠ¸ ë° íŒ¨í„´ ê²€ìƒ‰
    2. generate_user_stories: User Story ìƒì„±
    3. validate_user_stories: ê²€ì¦
    4. finalize: ìµœì¢… ê²°ê³¼ ì •ë¦¬
    """
    
    def __init__(self):
        self.rag_retriever = RAGRetriever()
        # âœ… Frontendì™€ ì™„ì „íˆ ë™ì¼í•œ ì„¤ì • (model_kwargs ëŒ€ì‹  ì§ì ‘ íŒŒë¼ë¯¸í„°ë¡œ)
        self.llm = ChatOpenAI(
            model=Config.DEFAULT_LLM_MODEL,  # gpt-4.1-2025-04-14
            temperature=Config.DEFAULT_LLM_TEMPERATURE,  # 0.2
            top_p=1.0,  # model_kwargs â†’ ì§ì ‘ íŒŒë¼ë¯¸í„° âœ…
            frequency_penalty=0.0,  # model_kwargs â†’ ì§ì ‘ íŒŒë¼ë¯¸í„° âœ…
            presence_penalty=0.0,  # model_kwargs â†’ ì§ì ‘ íŒŒë¼ë¯¸í„° âœ…
            # max_tokens ì„¤ì • ì•ˆ í•¨ (Frontendë„ ì—†ìŒ)
            verbose=True  # ğŸ” ë””ë²„ê¹…: API ìš”ì²­ ë¡œê¹…
        )
        self.workflow = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±"""
        workflow = StateGraph(UserStoryState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("retrieve_rag", self.retrieve_rag_context)
        workflow.add_node("generate_user_stories", self.generate_user_stories)
        workflow.add_node("validate_user_stories", self.validate_user_stories)
        workflow.add_node("finalize", self.finalize_result)
        
        # ì—£ì§€ ì„¤ì •
        workflow.set_entry_point("retrieve_rag")
        workflow.add_edge("retrieve_rag", "generate_user_stories")
        workflow.add_edge("generate_user_stories", "validate_user_stories")
        workflow.add_edge("validate_user_stories", "finalize")
        workflow.add_edge("finalize", END)
        
        return workflow.compile()
    
    def retrieve_rag_context(self, state: UserStoryState) -> Dict:
        """
        Step 1: RAGë¡œ ìœ ì‚¬ í”„ë¡œì íŠ¸ ë° User Story íŒ¨í„´ ê²€ìƒ‰
        """
        requirements = state["requirements"]
        
        # User Story íŒ¨í„´ ê²€ìƒ‰
        user_story_patterns = self.rag_retriever.search_ddd_patterns(
            f"User Story patterns for: {requirements}",
            k=10
        )
        
        # ìœ ì‚¬ í”„ë¡œì íŠ¸ ê²€ìƒ‰
        similar_projects = self.rag_retriever.search_project_templates(
            requirements,
            k=5
        )
        
        # ë„ë©”ì¸ ìš©ì–´ ê²€ìƒ‰
        vocabulary = self.rag_retriever.search_vocabulary(requirements, k=20)
        
        return {
            "rag_context": {
                "user_story_patterns": user_story_patterns,
                "similar_projects": similar_projects,
                "vocabulary": vocabulary
            },
            "progress": 20,
            "logs": [{
                "timestamp": datetime.now().isoformat(),
                "level": "info",
                "message": f"RAG context retrieved: {len(user_story_patterns)} patterns, {len(similar_projects)} projects, {len(vocabulary)} terms"
            }]
        }
    
    def generate_user_stories(self, state: UserStoryState) -> Dict:
        """
        Step 2: RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ User Story ìƒì„±
        """
        
        requirements = state["requirements"]
        rag_context = state.get("rag_context", {})
        bounded_contexts = state.get("bounded_contexts", [])
        
        # âœ… ê¸°ì¡´ ì²­í¬ì˜ ëˆ„ì  ê²°ê³¼ (Recursive ëª¨ë“œ)
        existing_actors = state.get("existingActors", [])
        existing_user_stories = state.get("existingUserStories", [])
        existing_business_rules = state.get("existingBusinessRules", [])
        existing_bounded_contexts = state.get("existingBoundedContexts", [])
        
        # RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·
        rag_context_str = self._format_rag_context(rag_context)
        
        # ìš”êµ¬ì‚¬í•­ ì–¸ì–´ ê°ì§€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        has_korean = any('\uac00' <= c <= '\ud7a3' for c in requirements[:500])
        language = "Korean" if has_korean else "English"
        
        # âœ… ê¸°ì¡´ ë°ì´í„° í”„ë¡¬í”„íŠ¸ ìƒì„± (ì¤‘ë³µ ë°©ì§€)
        existing_data_prompt = self._format_existing_data(
            existing_actors, existing_user_stories, 
            existing_business_rules, existing_bounded_contexts
        )
        
        # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì²« ë²ˆì§¸ ì²­í¬ì¸ì§€ íŒë³„)
        has_existing_data = bool(existing_data_prompt.strip())
        
        # ê°€ìƒ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë“œ ê°ì§€ (ìš”êµ¬ì‚¬í•­ì´ ì—†ì„ ë•Œ)
        is_virtual_scenario = self._is_virtual_scenario(requirements)
        
        # ìš”êµ¬ì‚¬í•­ í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        user_story_section = ""
        if is_virtual_scenario:
            # ê°€ìƒ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë“œ: requirements ìì²´ê°€ í”„ë¡¬í”„íŠ¸
            user_story_section = requirements
        elif requirements and len(requirements) >= 100:
            user_story_section = f"""
The user story is: {requirements}

Please generate user stories and scenarios based on the above content, staying within the scope and context provided.
"""
        else:
            # userStoryê°€ 100ì ë¯¸ë§Œì´ë©´ ë¹ˆ ë¬¸ìì—´ (ê¸°ì¡´ ìƒì„±ê¸°ì™€ ë™ì¼)
            user_story_section = ""
        
        # ê°€ìƒ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë“œ ë˜ëŠ” userStoryê°€ 100ì ë¯¸ë§Œì¼ ë•ŒëŠ” í…ìŠ¤íŠ¸ í˜•ì‹ ì‚¬ìš©
        is_text_mode = is_virtual_scenario or not user_story_section
        
        if is_text_mode:
            # ê°€ìƒ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë“œ: Frontendì—ì„œ ì´ë¯¸ ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if is_virtual_scenario:
                # requirements ìì²´ê°€ ì´ë¯¸ ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ (Personas, Business Model í¬í•¨)
                prompt = f"""{requirements}

{existing_data_prompt}

LANGUAGE REQUIREMENT:
Please generate the response in {language} while ensuring that all code elements (e.g., variable names, function names) remain in English.

The response must:
- Ensure complete traceability between actors, stories
- Avoid any missing connections between components
- Provide a clear, well-structured text response
"""
            else:
                # ì œëª©ë§Œ ìˆëŠ” ê²½ìš°: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                prompt = f"""Please generate a comprehensive analysis for the service with the following requests:

{existing_data_prompt}

1. Actors:
   - List all actors (users, external systems, etc.) that interact with the system
   - Describe each actor's role and responsibilities
   
2. User Stories (in detailed usecase spec):
   - Create detailed user stories for each actor
   - Each story should include "As a", "I want to", "So that" format
   - Include acceptance criteria for each story
   
3. Business Rules:
   - Define core business rules and constraints
   - Include validation rules and business logic
   - Specify any regulatory or compliance requirements

LANGUAGE REQUIREMENT:
Please generate the response in {language} while ensuring that all code elements (e.g., variable names, function names) remain in English.

The response must:
- Ensure complete traceability between actors, stories
- Avoid any missing connections between components
- Provide a clear, well-structured text response
"""
        else:
            prompt = f"""Please analyze the following requirements and extract ONLY the actors, user stories, and business rules that are EXPLICITLY mentioned or directly implied.

{user_story_section}

{existing_data_prompt}

REQUIREMENTS ANALYSIS RULES:
1. Actors:
   - Extract ONLY actors that are explicitly mentioned in the requirements
   - Do NOT create fictional or hypothetical actors
   - Focus on actual users, systems, or external entities mentioned in the text

2. User Stories:
   - Create user stories ONLY for features and functionality explicitly described in the requirements
   - Do NOT invent additional features or expand beyond what is specified
   - Each story must be directly traceable to specific requirements in the text
   - Use the format: "As a [role], I want [specific feature], so that [stated benefit]"

3. Business Rules:
   - Extract ONLY business rules that are explicitly stated or clearly implied in the requirements
   - Do NOT create generic or hypothetical business rules
   - Focus on actual constraints, validations, or processes mentioned in the text

OUTPUT FORMAT (JSON only, no markdown):
{{
  "title": "ì„œë¹„ìŠ¤ ì œëª©",
  "actors": [
    {{
      "title": "ì•¡í„° ì œëª©",
      "description": "ì•¡í„° ì„¤ëª…",
      "role": "ì•¡í„° ì—­í• "
    }}
  ],
  "userStories": [
    {{
      "title": "ìœ ì €ìŠ¤í† ë¦¬ ì œëª©",
      "as": "As a [role]",
      "iWant": "I want [action]",
      "soThat": "so that [benefit]",
      "description": "ìƒì„¸ ì„¤ëª…"
    }}
  ],
  "businessRules": [
    {{
      "title": "ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì œëª©",
      "description": "ê·œì¹™ ì„¤ëª…"
    }}
  ],
  "boundedContexts": [
    {{
      "name": "ì»¨í…ìŠ¤íŠ¸ ì´ë¦„",
      "description": "ì»¨í…ìŠ¤íŠ¸ ì„¤ëª…"
    }}
  ]
}}

CRITICAL INSTRUCTIONS:
- Generate content ONLY from the provided requirements text
- Do NOT create fictional scenarios or hypothetical features
- Do NOT expand beyond what is explicitly stated or directly implied
- Extract ALL features, actors, and business rules mentioned in the requirements
- Be thorough and comprehensive in your extraction{" - The following topics are ALREADY COVERED in previous chunks. Do NOT duplicate them." if has_existing_data else ""}

The response must:
- Be directly traceable to the provided requirements text
- Avoid any fictional or hypothetical content
- Return ONLY the JSON object, no additional text

LANGUAGE REQUIREMENT:
Please generate the response in {language} while ensuring that all code elements (e.g., variable names, function names) remain in English.

FORMAT REQUIREMENT:
Please generate the json in valid json format and if there's a property its value is null, don't contain the property. Also, please return only the json without any natural language.
"""
        
        try:
            # LLM í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš© - ë” ì™„ì „í•œ ì‘ë‹µ ìƒì„±)
            from langchain_core.messages import HumanMessage
            
            messages = [HumanMessage(content=prompt)]
            
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°›ì•„ì„œ ì™„ì „í•œ ì‘ë‹µ í™•ë³´
            response_chunks = []
            for chunk in self.llm.stream(messages):
                if chunk.content:
                    response_chunks.append(chunk.content)
            
            response = "".join(response_chunks)
            
            # í…ìŠ¤íŠ¸ ëª¨ë“œì¼ ë•ŒëŠ” JSON íŒŒì‹± ê±´ë„ˆë›°ê¸°
            if is_text_mode:
                return {
                    "userStories": [],
                    "actors": [],
                    "businessRules": [],
                    "boundedContexts": [],
                    "textResponse": response,  # í…ìŠ¤íŠ¸ ì‘ë‹µ ì €ì¥
                    "progress": 70,
                    "logs": [{
                        "timestamp": datetime.now().isoformat(),
                        "level": "info",
                        "message": "Generated text response (no JSON parsing)"
                    }]
                }
            
            # JSON íŒŒì‹±
            response_clean = self._extract_json(response)
            
            result_data = json.loads(response_clean)
            
            # camelCaseë¡œ ì‘ë‹µ ë°›ìŒ
            user_stories = result_data.get("userStories", [])
            actors = result_data.get("actors", [])
            business_rules = result_data.get("businessRules", [])
            bounded_contexts = result_data.get("boundedContexts", [])
            
            return {
                "userStories": user_stories,
                "actors": actors,
                "businessRules": business_rules,
                "boundedContexts": bounded_contexts,
                "progress": 70,
                "logs": [{
                    "timestamp": datetime.now().isoformat(),
                    "level": "info",
                    "message": f"Generated {len(user_stories)} user stories"
                }]
            }
            
        except Exception as e:
            error_msg = f"Failed to generate user stories: {str(e)}"
            print(f"âŒ {error_msg}")
            
            return {
                "userStories": [],
                "actors": [],
                "businessRules": [],
                "boundedContexts": [],
                "error": error_msg,
                "progress": 70,
                "logs": [{
                    "timestamp": datetime.now().isoformat(),
                    "level": "error",
                    "message": error_msg
                }]
            }
    
    def validate_user_stories(self, state: UserStoryState) -> Dict:
        """
        Step 3: ìƒì„±ëœ User Story ê²€ì¦
        """
        
        user_stories = state.get("userStories", [])
        text_response = state.get("textResponse", None)
        
        # textResponseê°€ ìˆìœ¼ë©´ ê²€ì¦ ê±´ë„ˆë›°ê³  ê·¸ëŒ€ë¡œ ì „ë‹¬
        if text_response:
            return {
                "textResponse": text_response,
                "userStories": [],
                "progress": 90,
                "logs": [{
                    "timestamp": datetime.now().isoformat(),
                    "level": "info",
                    "message": "Text mode response, skipping validation"
                }]
            }
        
        if not user_stories:
            return {
                "userStories": [],  # ë¹ˆ ë°°ì—´ì´ë¼ë„ í•„ë“œë¥¼ ë°˜í™˜í•´ì•¼ í•¨
                "progress": 90,
                "logs": [{
                    "timestamp": datetime.now().isoformat(),
                    "level": "warning",
                    "message": "No user stories to validate"
                }]
            }
        
        # ê²€ì¦ ë¡œì§
        validated_stories = []
        issues = []
        
        for story in user_stories:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸ (camelCase)
            required_fields = ["as", "iWant", "soThat"]
            missing_fields = [f for f in required_fields if f not in story or not story[f]]
            
            if missing_fields:
                issues.append({
                    "story_id": story.get("id", "unknown"),
                    "issue": f"Missing required fields: {', '.join(missing_fields)}"
                })
            else:
                validated_stories.append(story)
        
        logs = [{
            "timestamp": datetime.now().isoformat(),
            "level": "info",
            "message": f"Validated {len(validated_stories)}/{len(user_stories)} user stories"
        }]
        
        if issues:
            logs.append({
                "timestamp": datetime.now().isoformat(),
                "level": "warning",
                "message": f"Found {len(issues)} validation issues: {json.dumps(issues)}"
            })
        
        return {
            "userStories": validated_stories,
            "progress": 90,
            "logs": logs
        }
    
    def finalize_result(self, state: UserStoryState) -> Dict:
        """
        Step 4: ìµœì¢… ê²°ê³¼ ì •ë¦¬
        """
        
        # Stateì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì™„ë£Œ ìƒíƒœ ì¶”ê°€ (camelCase)
        result = {
            "userStories": state.get("userStories", []),
            "actors": state.get("actors", []),
            "businessRules": state.get("businessRules", []),
            "boundedContexts": state.get("boundedContexts", []),
            "ragContext": state.get("rag_context", {}),
            "requirements": state.get("requirements", ""),
            "isCompleted": True,
            "progress": 100,
            "logs": state.get("logs", []) + [{
                "timestamp": datetime.now().isoformat(),
                "level": "info",
                "message": f"User story generation completed successfully. Total: {len(state.get('userStories', []))} stories"
            }]
        }
        
        # textResponseê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if state.get("textResponse"):
            result["textResponse"] = state.get("textResponse")
        
        return result
    
    def _is_virtual_scenario(self, requirements: str) -> bool:
        """
        ê°€ìƒ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë“œ ê°ì§€
        Frontendì—ì„œ ìƒì„±í•œ ê°€ìƒ ì‹œë‚˜ë¦¬ì˜¤ í”„ë¡¬í”„íŠ¸ì¸ì§€ í™•ì¸
        """
        if not requirements:
            return False
        
        # ê°€ìƒ ì‹œë‚˜ë¦¬ì˜¤ í”„ë¡¬í”„íŠ¸ì˜ íŠ¹ì§•ì ì¸ ì‹œì‘ ë¶€ë¶„ í™•ì¸
        virtual_scenario_indicators = [
            "Please generate a comprehensive analysis",
            "Create pain points and possible solutions",
            "Persona definition and he(or her)'s painpoints"
        ]
        
        return any(indicator in requirements for indicator in virtual_scenario_indicators)
    
    def _format_existing_data(self, existing_actors, existing_user_stories, 
                              existing_business_rules, existing_bounded_contexts) -> str:
        """
        ê¸°ì¡´ ì²­í¬ì˜ ëˆ„ì  ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ (ì¤‘ë³µ ë°©ì§€)
        RecursiveUserStoryGeneratorì˜ createExistingDataPromptì™€ ë™ì¼
        """
        prompt = ""
        
        # ê¸°ì¡´ ì•¡í„° (titleë§Œ)
        if existing_actors and len(existing_actors) > 0:
            actor_titles = [actor.get("title", "") for actor in existing_actors if actor.get("title")]
            if actor_titles:
                prompt += "Already Generated Actors: " + ", ".join(actor_titles) + "\n"
                prompt += "Create new actors if needed, but avoid duplicating existing ones.\n\n"
        
        # ê¸°ì¡´ ìœ ì € ìŠ¤í† ë¦¬ (titleë§Œ)
        if existing_user_stories and len(existing_user_stories) > 0:
            story_titles = [story.get("title", "") for story in existing_user_stories if story.get("title")]
            if story_titles:
                prompt += "Already Generated Topics (do not create related content):\n"
                prompt += ", ".join(story_titles) + "\n\n"
                prompt += "Focus on generating new, unrelated user stories and features.\n\n"
        
        # ê¸°ì¡´ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ (titleë§Œ)
        if existing_business_rules and len(existing_business_rules) > 0:
            rule_titles = [rule.get("title", "") for rule in existing_business_rules if rule.get("title")]
            if rule_titles:
                prompt += "Already Generated Business Rules: " + ", ".join(rule_titles) + "\n"
                prompt += "Create new rules if needed, but avoid duplicating existing ones.\n\n"
        
        # ê¸°ì¡´ ë°”ìš´ë””ë“œ ì»¨í…ìŠ¤íŠ¸ (nameë§Œ)
        if existing_bounded_contexts and len(existing_bounded_contexts) > 0:
            bc_names = [bc.get("name", "") for bc in existing_bounded_contexts if bc.get("name")]
            if bc_names:
                prompt += "Already Generated Bounded Contexts: " + ", ".join(bc_names) + "\n"
                prompt += "Create new contexts if needed, but avoid duplicating existing ones.\n\n"
        
        if prompt:
            prompt += "IMPORTANT: When generating new content, ensure it complements and builds upon the existing data rather than duplicating it.\n\n"
        
        return prompt
    
    def _format_rag_context(self, rag_context: Dict) -> str:
        """RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ í¬ë§·"""
        if not rag_context:
            return "NO RAG CONTEXT AVAILABLE (Knowledge base not initialized)"
        
        sections = []
        
        # User Story íŒ¨í„´
        patterns = rag_context.get("user_story_patterns", [])
        if patterns:
            sections.append("USER STORY PATTERNS (from knowledge base):")
            for i, pattern in enumerate(patterns[:5], 1):  # ìµœëŒ€ 5ê°œ
                sections.append(f"{i}. {pattern.get('content', '')[:200]}")
        
        # ìœ ì‚¬ í”„ë¡œì íŠ¸
        projects = rag_context.get("similar_projects", [])
        if projects:
            sections.append("\nSIMILAR PROJECT EXAMPLES:")
            for i, project in enumerate(projects[:3], 1):  # ìµœëŒ€ 3ê°œ
                sections.append(f"{i}. {project.get('content', '')[:300]}")
        
        # ë„ë©”ì¸ ìš©ì–´
        vocab = rag_context.get("vocabulary", [])
        if vocab:
            sections.append("\nDOMAIN VOCABULARY:")
            for i, term in enumerate(vocab[:10], 1):  # ìµœëŒ€ 10ê°œ
                sections.append(f"{i}. {term.get('content', '')[:100]}")
        
        return "\n".join(sections) if sections else "NO RAG CONTEXT AVAILABLE"
    
    def _extract_json(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì œê±°)"""
        # ```json ... ``` ì œê±°
        text = text.strip()
        if text.startswith("```json"):
            text = text[7:]
        if text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        return text.strip()
    
    def run(self, inputs: Dict) -> Dict:
        """
        ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        
        Args:
            inputs: {
                "requirements": str,
                "bounded_contexts": List[Dict] (optional)
            }
            
        Returns:
            {
                "user_stories": List[Dict],
                "progress": int,
                "logs": List[Dict],
                "is_completed": bool,
                "error": str
            }
        """
        initial_state: UserStoryState = {
            "requirements": inputs.get("requirements", ""),
            "bounded_contexts": inputs.get("bounded_contexts", []),
            "rag_context": {},
            "userStories": [],
            "actors": [],
            "businessRules": [],
            "boundedContexts": [],
            "progress": 0,
            "logs": [],
            "isCompleted": False,
            "error": ""
        }
        
        try:
            result = self.workflow.invoke(initial_state)
            
            # ê²°ê³¼ëŠ” ì´ë¯¸ camelCase í˜•ì‹
            return result
        except Exception as e:
            error_msg = f"Workflow execution failed: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                "userStories": [],
                "boundedContexts": [],
                "ragContext": {},
                "progress": 0,
                "logs": [{
                    "timestamp": datetime.now().isoformat(),
                    "level": "error",
                    "message": error_msg
                }],
                "isCompleted": False,
                "isFailed": True,
                "error": error_msg,
                "requirements": inputs.get("requirements", "")
            }

